"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[893],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>m});var l=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);a&&(l=l.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,l)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,l,n=function(e,a){if(null==e)return{};var t,l,n={},r=Object.keys(e);for(l=0;l<r.length;l++)t=r[l],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(l=0;l<r.length;l++)t=r[l],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var u=l.createContext({}),i=function(e){var a=l.useContext(u),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=i(e.components);return l.createElement(u.Provider,{value:a},e.children)},c="mdxType",k={inlineCode:"code",wrapper:function(e){var a=e.children;return l.createElement(l.Fragment,{},a)}},d=l.forwardRef((function(e,a){var t=e.components,n=e.mdxType,r=e.originalType,u=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=i(t),d=n,m=c["".concat(u,".").concat(d)]||c[d]||k[d]||r;return t?l.createElement(m,o(o({ref:a},p),{},{components:t})):l.createElement(m,o({ref:a},p))}));function m(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var r=t.length,o=new Array(r);o[0]=d;var s={};for(var u in a)hasOwnProperty.call(a,u)&&(s[u]=a[u]);s.originalType=e,s[c]="string"==typeof e?e:n,o[1]=s;for(var i=2;i<r;i++)o[i]=t[i];return l.createElement.apply(null,o)}return l.createElement.apply(null,t)}d.displayName="MDXCreateElement"},5162:(e,a,t)=>{t.d(a,{Z:()=>o});var l=t(7294),n=t(6010);const r={tabItem:"tabItem_Ymn6"};function o(e){let{children:a,hidden:t,className:o}=e;return l.createElement("div",{role:"tabpanel",className:(0,n.Z)(r.tabItem,o),hidden:t},a)}},4866:(e,a,t)=>{t.d(a,{Z:()=>v});var l=t(7462),n=t(7294),r=t(6010),o=t(2466),s=t(6550),u=t(1980),i=t(7392),p=t(12);function c(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:a,label:t,attributes:l,default:n}}=e;return{value:a,label:t,attributes:l,default:n}}))}function k(e){const{values:a,children:t}=e;return(0,n.useMemo)((()=>{const e=a??c(t);return function(e){const a=(0,i.l)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,t])}function d(e){let{value:a,tabValues:t}=e;return t.some((e=>e.value===a))}function m(e){let{queryString:a=!1,groupId:t}=e;const l=(0,s.k6)(),r=function(e){let{queryString:a=!1,groupId:t}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:a,groupId:t});return[(0,u._X)(r),(0,n.useCallback)((e=>{if(!r)return;const a=new URLSearchParams(l.location.search);a.set(r,e),l.replace({...l.location,search:a.toString()})}),[r,l])]}function S(e){const{defaultValue:a,queryString:t=!1,groupId:l}=e,r=k(e),[o,s]=(0,n.useState)((()=>function(e){let{defaultValue:a,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!d({value:a,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const l=t.find((e=>e.default))??t[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:a,tabValues:r}))),[u,i]=m({queryString:t,groupId:l}),[c,S]=function(e){let{groupId:a}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(a),[l,r]=(0,p.Nk)(t);return[l,(0,n.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:l}),h=(()=>{const e=u??c;return d({value:e,tabValues:r})?e:null})();(0,n.useLayoutEffect)((()=>{h&&s(h)}),[h]);return{selectedValue:o,selectValue:(0,n.useCallback)((e=>{if(!d({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);s(e),i(e),S(e)}),[i,S,r]),tabValues:r}}var h=t(2389);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function g(e){let{className:a,block:t,selectedValue:s,selectValue:u,tabValues:i}=e;const p=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.o5)(),k=e=>{const a=e.currentTarget,t=p.indexOf(a),l=i[t].value;l!==s&&(c(a),u(l))},d=e=>{let a=null;switch(e.key){case"Enter":k(e);break;case"ArrowRight":{const t=p.indexOf(e.currentTarget)+1;a=p[t]??p[0];break}case"ArrowLeft":{const t=p.indexOf(e.currentTarget)-1;a=p[t]??p[p.length-1];break}}a?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":t},a)},i.map((e=>{let{value:a,label:t,attributes:o}=e;return n.createElement("li",(0,l.Z)({role:"tab",tabIndex:s===a?0:-1,"aria-selected":s===a,key:a,ref:e=>p.push(e),onKeyDown:d,onClick:k},o,{className:(0,r.Z)("tabs__item",b.tabItem,o?.className,{"tabs__item--active":s===a})}),t??a)})))}function f(e){let{lazy:a,children:t,selectedValue:l}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(a){const e=r.find((e=>e.props.value===l));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},r.map(((e,a)=>(0,n.cloneElement)(e,{key:a,hidden:e.props.value!==l}))))}function T(e){const a=S(e);return n.createElement("div",{className:(0,r.Z)("tabs-container",b.tabList)},n.createElement(g,(0,l.Z)({},e,a)),n.createElement(f,(0,l.Z)({},e,a)))}function v(e){const a=(0,h.Z)();return n.createElement(T,(0,l.Z)({key:String(a)},e))}},904:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>u,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var l=t(7462),n=(t(7294),t(3905)),r=t(4866),o=t(5162);const s={},u="Spark \u5feb\u901f\u5f00\u59cb",i={unversionedId:"Getting Started/spark-guide",id:"Getting Started/spark-guide",title:"Spark \u5feb\u901f\u5f00\u59cb",description:"\x3c!--",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/01-Getting Started/02-spark-guide.mdx",sourceDirName:"01-Getting Started",slug:"/Getting Started/spark-guide",permalink:"/zh-Hans/docs/Getting Started/spark-guide",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/02-spark-guide.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\u5feb\u901f\u642d\u5efa\u8fd0\u884c\u73af\u5883",permalink:"/zh-Hans/docs/Getting Started/setup-local-env"},next:{title:"Flink \u5feb\u901f\u5f00\u59cb",permalink:"/zh-Hans/docs/Getting Started/Flink-Guide"}},p={},c=[{value:"\u914d\u7f6e",id:"\u914d\u7f6e",level:2},{value:"Spark 3 Support Matrix",id:"spark-3-support-matrix",level:3},{value:"Spark Shell/SQL",id:"spark-shellsql",level:3},{value:"Maven \u9879\u76ee\u4f9d\u8d56\u914d\u7f6e",id:"maven-\u9879\u76ee\u4f9d\u8d56\u914d\u7f6e",level:3},{value:"\u521b\u5efa\u547d\u540d\u7a7a\u95f4",id:"\u521b\u5efa\u547d\u540d\u7a7a\u95f4",level:2},{value:"\u521b\u5efa\u8868",id:"\u521b\u5efa\u8868",level:2},{value:"\u4e3b\u952e\u8868",id:"\u4e3b\u952e\u8868",level:3},{value:"\u4e3b\u952e CDC \u8868",id:"\u4e3b\u952e-cdc-\u8868",level:3},{value:"\u6570\u636e\u63d2\u5165/\u5408\u5e76",id:"\u6570\u636e\u63d2\u5165\u5408\u5e76",level:2},{value:"\u6570\u636e\u66f4\u65b0",id:"\u6570\u636e\u66f4\u65b0",level:2},{value:"\u6570\u636e\u5220\u9664",id:"\u6570\u636e\u5220\u9664",level:2},{value:"\u6570\u636e\u67e5\u8be2",id:"\u6570\u636e\u67e5\u8be2",level:2},{value:"Time Travel\u67e5\u8be2",id:"time-travel\u67e5\u8be2",level:2},{value:"\u5168\u91cf\u67e5\u8be2",id:"\u5168\u91cf\u67e5\u8be2",level:3},{value:"\u5feb\u7167\u67e5\u8be2",id:"\u5feb\u7167\u67e5\u8be2",level:3},{value:"\u589e\u91cf\u67e5\u8be2",id:"\u589e\u91cf\u67e5\u8be2",level:3},{value:"\u66f4\u591a\u6848\u4f8b",id:"\u66f4\u591a\u6848\u4f8b",level:2}],k={toc:c},d="wrapper";function m(e){let{components:a,...t}=e;return(0,n.kt)(d,(0,l.Z)({},k,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"spark-\u5feb\u901f\u5f00\u59cb"},"Spark \u5feb\u901f\u5f00\u59cb"),(0,n.kt)("h2",{id:"\u914d\u7f6e"},"\u914d\u7f6e"),(0,n.kt)("p",null,"\u8981\u5728Spark\u4e2d\u4f7f\u7528LakeSoul\uff0c\u8bf7\u9996\u5148\u914d\u7f6e",(0,n.kt)("a",{parentName:"p",href:"/zh-Hans/docs/Getting%20Started/setup-local-env"},"Spark Catalog"),"\u3002LakeSoul\u4f7f\u7528Apache Spark\u7684DataSourceV2 API\u6765\u5b9e\u73b0\u6570\u636e\u6e90\u548c\u76ee\u5f55\u3002\u6b64\u5916\uff0cLakeSoul\u8fd8\u63d0\u4f9b\u4e86 Scala \u7684\u8868API\uff0c\u4ee5\u6269\u5c55LakeSoul\u6570\u636e\u8868\u7684\u529f\u80fd\u3002"),(0,n.kt)("h3",{id:"spark-3-support-matrix"},"Spark 3 Support Matrix"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"LakeSoul"),(0,n.kt)("th",{parentName:"tr",align:null},"Spark Version"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.2.x-2.4.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.3.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.0.x-2.1.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.1.x")))),(0,n.kt)("h3",{id:"spark-shellsql"},"Spark Shell/SQL"),(0,n.kt)("p",null,"\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulSparkSessionExtension")," sql\u6269\u5c55\u6765\u8fd0\u884cspark-shell/spark-sql\u3002"),(0,n.kt)(r.Z,{defaultValue:"SQL",values:[{label:"Scala",value:"Scala"},{label:"SQL",value:"SQL"}],mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"SQL",label:"SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-sql --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul  --jars lakesoul-spark-2.5.0-spark-3.3.jar\n"))),(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul  --jars lakesoul-spark-2.5.0-spark-3.3.jar\n")))),(0,n.kt)("h3",{id:"maven-\u9879\u76ee\u4f9d\u8d56\u914d\u7f6e"},"Maven \u9879\u76ee\u4f9d\u8d56\u914d\u7f6e"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>com.dmetasoul</groupId>\n    <artifactId>lakesoul</artifactId>\n    <version>2.5.0-spark-3.3</version>\n</dependency>\n")),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nimport org.apache.spark.sql.SparkSession\nimport spark.implicits._\nimport com.dmetasoul.lakesoul.tables.LakeSoulTable\n\nval spark = SparkSession.builder()\n    .master("local")\n    .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog")\n    .config("spark.sql.defaultCatalog", "lakesoul")\n    .getOrCreate()\n    \n')))),(0,n.kt)("h2",{id:"\u521b\u5efa\u547d\u540d\u7a7a\u95f4"},"\u521b\u5efa\u547d\u540d\u7a7a\u95f4"),(0,n.kt)("p",null,"\u9996\u5148\uff0c\u4e3aLakeSoul\u8868\u521b\u5efa\u4e00\u4e2anamespace\uff0c\u5982\u679c\u4e0d\u521b\u5efa\u5c06\u4f7f\u7528\u9ed8\u8ba4\u7684namespace\uff0cLakeSoul Catalog\u7684\u9ed8\u8ba4namespace\u662f",(0,n.kt)("inlineCode",{parentName:"p"},"default"),"\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nCREATE NAMESPACE lakesoul_namespace;\nUSE lakesoul_namespace\n")),(0,n.kt)("h2",{id:"\u521b\u5efa\u8868"},"\u521b\u5efa\u8868"),(0,n.kt)("p",null,"\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"USING lakesoul"),"\u7684\u5b50\u53e5\u521b\u5efa\u4e00\u4e2a\u5206\u533a\u7684LakeSoul\u8868"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nCREATE TABLE lakesoul_table (id BIGINT, date STRING, data STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table'\n\n"))),(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// scala\nval tablePath= "s3://lakesoul-test-bucket/test_table"\nval df = Seq(("2021-01-01",1,"rice"),("2021-01-01",2,"bread")).toDF("date","id","name")\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .save(tablePath)\n')))),(0,n.kt)("h3",{id:"\u4e3b\u952e\u8868"},"\u4e3b\u952e\u8868"),(0,n.kt)("p",null,"\u5728LakeSoul\u4e2d\uff0c\u5e26\u6709\u4e3b\u952e\u7684\u8868\u88ab\u5b9a\u4e49\u4e3a\u54c8\u5e0c\u5206\u533a\u8868\u3002\u4f7f\u7528USING lakesoul\u5b50\u53e5\uff0c\u5e76\u7ed3\u5408TBLPROPERTIES\u8bbe\u7f6e\uff08\u5176\u4e2d'hashPartitions'\u6307\u5b9a\u4ee5\u9017\u53f7\u5206\u9694\u7684\u4e3b\u952e\u5217\u8868\uff0c'hashBucketNum'\u6307\u5b9a\u54c8\u5e0c\u6876\u7684\u5927\u5c0f\uff09\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u54c8\u5e0c\u5206\u533a\u7684LakeSoul\u8868\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nCREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, date STRING, name STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' \nTBLPROPERTIES ( \n  'hashPartitions'='id',\n  'hashBucketNum'='2')\n")),(0,n.kt)("h3",{id:"\u4e3b\u952e-cdc-\u8868"},"\u4e3b\u952e CDC \u8868"),(0,n.kt)("p",null,"\u54c8\u5e0c\u5206\u533a\u7684LakeSoul\u8868\u5177\u6709\u53ef\u9009\u7684\u6570\u636e\u53d8\u66f4\u6355\u83b7\uff08CDC\uff09\u529f\u80fd\uff0c\u80fd\u591f\u8bb0\u5f55\u6570\u636e\u7684\u53d8\u5316\u3002\u8981\u521b\u5efa\u652f\u6301CDC\u7684LakeSoul\u8868\uff0c\u53ef\u4ee5\u5728\u54c8\u5e0c\u5206\u533a\u8868\u7684DDL\u8bed\u53e5\u4e2d\u6dfb\u52a0\u989d\u5916\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"TBLPROPERTIES"),"\u8bbe\u7f6e\uff0c\u6307\u5b9a",(0,n.kt)("inlineCode",{parentName:"p"},"'lakesoul_cdc_change_column'"),"\u5c5e\u6027\u3002\u8fd9\u4e2a\u5c5e\u6027\u5b9a\u4e49\u4e86\u4e00\u4e2a\u9690\u5f0f\u5217\uff0c\u5e2e\u52a9\u8868\u6709\u6548\u5730\u5904\u7406CDC\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u6570\u636e\u53d8\u66f4\u7684\u7cbe\u786e\u8ffd\u8e2a\u548c\u7ba1\u7406\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nCREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, date STRING, name STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' \nTBLPROPERTIES( \n  'hashPartitions'='id',\n  'hashBucketNum'='2',\n  'lakesoul_cdc_change_column' = 'op'\n)\n")),(0,n.kt)("h2",{id:"\u6570\u636e\u63d2\u5165\u5408\u5e76"},"\u6570\u636e\u63d2\u5165/\u5408\u5e76"),(0,n.kt)("p",null,"\u8981\u4f7f\u7528Spark SQL\u5411\u975e\u54c8\u5e0c\u5206\u533a\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"INSERT INTO"),"\u8bed\u53e5\u3002"),(0,n.kt)("p",null,"\u8981\u4f7f\u7528DataFrame\u5411\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"DataFrameWriterV2")," API\u3002\u5982\u679c\u8fd9\u662f\u5bf9\u8be5\u8868\u7684\u7b2c\u4e00\u6b21\u5199\u5165\uff0c\u5b83\u8fd8\u5c06\u81ea\u52a8\u521b\u5efa\u76f8\u5e94\u7684LakeSoul\u8868\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nINSERT INTO TABLE lakesoul_table VALUES (1, '2024-01-01', 'Alice'), (2, '2024-01-01', 'Bob'), (1, \"2024-01-02\", \"Cathy\")\n"))),(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval data: DataFrame = Seq((1, "2024-01-01", "Alice"), (2, "2024-01-01", "Bob"), (1, "2024-01-02", "Cathy"))\n              .toDF("id", "date", "name")\ndata.write.format("lakesoul").insertIno("lakesoul_table")\n')))),(0,n.kt)("p",null,"\u8981\u4f7f\u7528Spark SQL\u5411\u54c8\u5e0c\u5206\u533a\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"Merge INTO"),"\u8bed\u53e5\u3002"),(0,n.kt)("p",null,"\u8981\u4f7f\u7528DataFrame\u5411\u54c8\u5e0c\u5206\u533a\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable"),"\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"upsert")," API\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\n// Create source_view\nCREATE OR REPLACE VIEW spark_catalog.default.source_view (id , date, data)\nAS SELECT (1 as `id`, '2024-01-01' as `date`, 'data' as `data`)\n\n// Merge source_view Into lakesoul_hash_table\n\nMERGE INTO lakesoul_hash_table AS t \nUSING spark_catalog.default.source_view AS s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n"))),(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_hash_table"\n\n// Init hash table with first dataframe\nval df = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4))\n        .toDF("range", "hash", "value")\nval writer = df.write.format("lakesoul").mode("overwrite")\n\nwriter\n    .option("rangePartitions", rangePartition.mkString(","))\n    .option("hashPartitions", hashPartition.mkString(","))\n    .option("hashBucketNum", hashBucketNum)\n    .save(tablePath)\n\n// merge the second dataframe into hash table using LakeSoulTable upsert API\nval dfUpsert = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4))\n        .toDF("range", "hash", "value")\nLakeSoulTable.forPath(tablePath).upsert(dfUpsert)\n\n')))),(0,n.kt)("h2",{id:"\u6570\u636e\u66f4\u65b0"},"\u6570\u636e\u66f4\u65b0"),(0,n.kt)("p",null,"LakeSoul\u8868\u53ef\u4ee5\u901a\u8fc7DataFrame\u6216\u4f7f\u7528\u6807\u51c6\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"UPDATE"),"\u8bed\u53e5\u8fdb\u884c\u66f4\u65b0\u3002\u8981\u4f7f\u7528DataFrame\u66f4\u65b0\u8868\u4e2d\u7684\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable"),"\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"updateExpr")," API\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},'// Spark SQL\nUPDATE table_namespace.table_name SET name = "David" WHERE id = 2\n'))),(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).updateExpr("id = 2", Seq("name"->"David").toMap)\n')))),(0,n.kt)("h2",{id:"\u6570\u636e\u5220\u9664"},"\u6570\u636e\u5220\u9664"),(0,n.kt)("p",null,"LakeSoul\u8868\u53ef\u4ee5\u901a\u8fc7DataFrame\u6216\u4f7f\u7528\u6807\u51c6\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"DELETE"),"\u8bed\u53e5\u6765\u5220\u9664\u8bb0\u5f55\u3002\u8981\u4f7f\u7528DataFrame\u4ece\u8868\u4e2d\u5220\u9664\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable"),"\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"delete")," API\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nDELETE FROM lakesoul_table WHERE id =1\n"))),(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).delete("id = 1 or id =2")\n')))),(0,n.kt)("h2",{id:"\u6570\u636e\u67e5\u8be2"},"\u6570\u636e\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u8868\u53ef\u4ee5\u4f7f\u7528DataFrame\u6216Spark SQL\u8fdb\u884c\u67e5\u8be2\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nSELECT * FROM lakesoul_table\n"))),(0,n.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\n\n// query data with DataFrameReader API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nspark.read.format("lakesoul").load(tablePath)\n\n// query data with LakeSoulTable API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).toDF\n\nval tableName = "lakesoul_table"\nLakeSoulTable.forName(tableName).toDF\n')))),(0,n.kt)("h2",{id:"time-travel\u67e5\u8be2"},"Time Travel\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u652f\u6301Time Travel\u67e5\u8be2\uff0c\u53ef\u4ee5\u67e5\u8be2\u5386\u53f2\u4e0a\u4efb\u4f55\u65f6\u95f4\u70b9\u7684\u8868\u6216\u4e24\u4e2a\u63d0\u4ea4\u65f6\u95f4\u4e4b\u95f4\u7684\u66f4\u6539\u6570\u636e\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_cdc_table"\nSeq(("range1", "hash1", "insert"), ("range2", "hash2", "insert"), ("range3", "hash2", "insert"), ("range4", "hash2", "insert"), ("range4", "hash4", "insert"), ("range3", "hash3", "insert"))\n    .toDF("range", "hash", "op")\n    .write\n    .mode("append")\n    .format("lakesoul")\n    .option("rangePartitions", "range")\n    .option("hashPartitions", "hash")\n    .option("hashBucketNum", "2")\n    .option("shortTableName", "lakesoul_cdc_table")\n    .option("lakesoul_cdc_change_column", "op")\n    .save(tablePath)\n// record the version of 1st commit \nval versionA: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n\nval lakeTable = LakeSoulTable.forPath(tablePath)\nlakeTable.upsert(Seq(("range1", "hash1-1", "delete"), ("range2", "hash2-10", "delete"))\n.toDF("range", "hash", "op"))\n// record the version of 2nd commit \nval versionB: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\nlakeTable.upsert(Seq(("range1", "hash1-13", "insert"), ("range2", "hash2-13", "update"))\n.toDF("range", "hash", "op"))\nlakeTable.upsert(Seq(("range1", "hash1-15", "insert"), ("range2", "hash2-15", "update"))\n.toDF("range", "hash", "op"))\n// record the version of 3rd,4th commits \nval versionC: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n')),(0,n.kt)("h3",{id:"\u5168\u91cf\u67e5\u8be2"},"\u5168\u91cf\u67e5\u8be2"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.sql("SELECT * FROM lakesoul_cdc_table")\n')),(0,n.kt)("h3",{id:"\u5feb\u7167\u67e5\u8be2"},"\u5feb\u7167\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u652f\u6301\u5feb\u7167\u67e5\u8be2\uff0c\u53ef\u7528\u4e8e\u67e5\u8be2\u5386\u53f2\u4e0a\u67d0\u4e00\u65f6\u95f4\u70b9\u7684\u8868\u6570\u636e\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range2")\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, ReadType.SNAPSHOT_READ)\n    .load(tablePath)\n')),(0,n.kt)("h3",{id:"\u589e\u91cf\u67e5\u8be2"},"\u589e\u91cf\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u652f\u6301\u589e\u91cf\u67e5\u8be2\uff0c\u53ef\u83b7\u5f97\u5728\u8d77\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4\u4e4b\u95f4\u53d1\u751f\u66f4\u6539\u7684\u6570\u636e\u8bb0\u5f55\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range1")\n    .option(LakeSoulOptions.READ_START_TIME, versionA)\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, ReadType.INCREMENTAL_READ)\n    .load(tablePath)\n')),(0,n.kt)("h2",{id:"\u66f4\u591a\u6848\u4f8b"},"\u66f4\u591a\u6848\u4f8b"),(0,n.kt)("p",null,"\u63a5\u4e0b\u6765\uff0c\u60a8\u53ef\u4ee5\u5728",(0,n.kt)("a",{parentName:"p",href:"/zh-Hans/docs/Usage%20Docs/spark-api-docs"},"Spark API\u6587\u6863"),"\u4e2d\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5728Spark\u4e2d\u4f7f\u7528LakeSoul\u8868\u7684\u6848\u4f8b\u3002"))}m.isMDXComponent=!0}}]);