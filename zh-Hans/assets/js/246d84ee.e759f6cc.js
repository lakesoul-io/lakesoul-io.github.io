"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[893],{3905:(e,a,l)=>{l.d(a,{Zo:()=>k,kt:()=>d});var t=l(7294);function n(e,a,l){return a in e?Object.defineProperty(e,a,{value:l,enumerable:!0,configurable:!0,writable:!0}):e[a]=l,e}function r(e,a){var l=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),l.push.apply(l,t)}return l}function s(e){for(var a=1;a<arguments.length;a++){var l=null!=arguments[a]?arguments[a]:{};a%2?r(Object(l),!0).forEach((function(a){n(e,a,l[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(l)):r(Object(l)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(l,a))}))}return e}function o(e,a){if(null==e)return{};var l,t,n=function(e,a){if(null==e)return{};var l,t,n={},r=Object.keys(e);for(t=0;t<r.length;t++)l=r[t],a.indexOf(l)>=0||(n[l]=e[l]);return n}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)l=r[t],a.indexOf(l)>=0||Object.prototype.propertyIsEnumerable.call(e,l)&&(n[l]=e[l])}return n}var p=t.createContext({}),u=function(e){var a=t.useContext(p),l=a;return e&&(l="function"==typeof e?e(a):s(s({},a),e)),l},k=function(e){var a=u(e.components);return t.createElement(p.Provider,{value:a},e.children)},i="mdxType",c={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},m=t.forwardRef((function(e,a){var l=e.components,n=e.mdxType,r=e.originalType,p=e.parentName,k=o(e,["components","mdxType","originalType","parentName"]),i=u(l),m=n,d=i["".concat(p,".").concat(m)]||i[m]||c[m]||r;return l?t.createElement(d,s(s({ref:a},k),{},{components:l})):t.createElement(d,s({ref:a},k))}));function d(e,a){var l=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var r=l.length,s=new Array(r);s[0]=m;var o={};for(var p in a)hasOwnProperty.call(a,p)&&(o[p]=a[p]);o.originalType=e,o[i]="string"==typeof e?e:n,s[1]=o;for(var u=2;u<r;u++)s[u]=l[u];return t.createElement.apply(null,s)}return t.createElement.apply(null,l)}m.displayName="MDXCreateElement"},5162:(e,a,l)=>{l.d(a,{Z:()=>s});var t=l(7294),n=l(6010);const r={tabItem:"tabItem_Ymn6"};function s(e){let{children:a,hidden:l,className:s}=e;return t.createElement("div",{role:"tabpanel",className:(0,n.Z)(r.tabItem,s),hidden:l},a)}},4866:(e,a,l)=>{l.d(a,{Z:()=>f});var t=l(7462),n=l(7294),r=l(6010),s=l(2466),o=l(6550),p=l(1980),u=l(7392),k=l(12);function i(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:a,label:l,attributes:t,default:n}}=e;return{value:a,label:l,attributes:t,default:n}}))}function c(e){const{values:a,children:l}=e;return(0,n.useMemo)((()=>{const e=a??i(l);return function(e){const a=(0,u.l)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,l])}function m(e){let{value:a,tabValues:l}=e;return l.some((e=>e.value===a))}function d(e){let{queryString:a=!1,groupId:l}=e;const t=(0,o.k6)(),r=function(e){let{queryString:a=!1,groupId:l}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!l)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return l??null}({queryString:a,groupId:l});return[(0,p._X)(r),(0,n.useCallback)((e=>{if(!r)return;const a=new URLSearchParams(t.location.search);a.set(r,e),t.replace({...t.location,search:a.toString()})}),[r,t])]}function S(e){const{defaultValue:a,queryString:l=!1,groupId:t}=e,r=c(e),[s,o]=(0,n.useState)((()=>function(e){let{defaultValue:a,tabValues:l}=e;if(0===l.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!m({value:a,tabValues:l}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${l.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const t=l.find((e=>e.default))??l[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:a,tabValues:r}))),[p,u]=d({queryString:l,groupId:t}),[i,S]=function(e){let{groupId:a}=e;const l=function(e){return e?`docusaurus.tab.${e}`:null}(a),[t,r]=(0,k.Nk)(l);return[t,(0,n.useCallback)((e=>{l&&r.set(e)}),[l,r])]}({groupId:t}),h=(()=>{const e=p??i;return m({value:e,tabValues:r})?e:null})();(0,n.useLayoutEffect)((()=>{h&&o(h)}),[h]);return{selectedValue:s,selectValue:(0,n.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);o(e),u(e),S(e)}),[u,S,r]),tabValues:r}}var h=l(2389);const T={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:a,block:l,selectedValue:o,selectValue:p,tabValues:u}=e;const k=[],{blockElementScrollPositionUntilNextRender:i}=(0,s.o5)(),c=e=>{const a=e.currentTarget,l=k.indexOf(a),t=u[l].value;t!==o&&(i(a),p(t))},m=e=>{let a=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const l=k.indexOf(e.currentTarget)+1;a=k[l]??k[0];break}case"ArrowLeft":{const l=k.indexOf(e.currentTarget)-1;a=k[l]??k[k.length-1];break}}a?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":l},a)},u.map((e=>{let{value:a,label:l,attributes:s}=e;return n.createElement("li",(0,t.Z)({role:"tab",tabIndex:o===a?0:-1,"aria-selected":o===a,key:a,ref:e=>k.push(e),onKeyDown:m,onClick:c},s,{className:(0,r.Z)("tabs__item",T.tabItem,s?.className,{"tabs__item--active":o===a})}),l??a)})))}function g(e){let{lazy:a,children:l,selectedValue:t}=e;const r=(Array.isArray(l)?l:[l]).filter(Boolean);if(a){const e=r.find((e=>e.props.value===t));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},r.map(((e,a)=>(0,n.cloneElement)(e,{key:a,hidden:e.props.value!==t}))))}function N(e){const a=S(e);return n.createElement("div",{className:(0,r.Z)("tabs-container",T.tabList)},n.createElement(b,(0,t.Z)({},e,a)),n.createElement(g,(0,t.Z)({},e,a)))}function f(e){const a=(0,h.Z)();return n.createElement(N,(0,t.Z)({key:String(a)},e))}},904:(e,a,l)=>{l.r(a),l.d(a,{assets:()=>k,contentTitle:()=>p,default:()=>d,frontMatter:()=>o,metadata:()=>u,toc:()=>i});var t=l(7462),n=(l(7294),l(3905)),r=l(4866),s=l(5162);const o={},p="Spark \u5feb\u901f\u5f00\u59cb",u={unversionedId:"Getting Started/spark-guide",id:"Getting Started/spark-guide",title:"Spark \u5feb\u901f\u5f00\u59cb",description:"\x3c!--",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/01-Getting Started/02-spark-guide.mdx",sourceDirName:"01-Getting Started",slug:"/Getting Started/spark-guide",permalink:"/zh-Hans/docs/Getting Started/spark-guide",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/02-spark-guide.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\u5feb\u901f\u642d\u5efa\u8fd0\u884c\u73af\u5883",permalink:"/zh-Hans/docs/Getting Started/setup-local-env"},next:{title:"Flink \u5feb\u901f\u5f00\u59cb",permalink:"/zh-Hans/docs/Getting Started/Flink-Guide"}},k={},i=[{value:"\u914d\u7f6e",id:"\u914d\u7f6e",level:2},{value:"Spark 3 Support Matrix",id:"spark-3-support-matrix",level:3},{value:"Spark Shell/SQL/PySpark",id:"spark-shellsqlpyspark",level:3},{value:"Maven \u9879\u76ee\u4f9d\u8d56\u914d\u7f6e",id:"maven-\u9879\u76ee\u4f9d\u8d56\u914d\u7f6e",level:3},{value:"\u521b\u5efa\u547d\u540d\u7a7a\u95f4",id:"\u521b\u5efa\u547d\u540d\u7a7a\u95f4",level:2},{value:"\u521b\u5efa\u8868",id:"\u521b\u5efa\u8868",level:2},{value:"\u4e3b\u952e\u8868",id:"\u4e3b\u952e\u8868",level:3},{value:"\u4e3b\u952e CDC \u8868",id:"\u4e3b\u952e-cdc-\u8868",level:3},{value:"\u6570\u636e\u63d2\u5165/\u5408\u5e76",id:"\u6570\u636e\u63d2\u5165\u5408\u5e76",level:2},{value:"\u6570\u636e\u66f4\u65b0",id:"\u6570\u636e\u66f4\u65b0",level:2},{value:"\u6570\u636e\u5220\u9664",id:"\u6570\u636e\u5220\u9664",level:2},{value:"\u6570\u636e\u67e5\u8be2",id:"\u6570\u636e\u67e5\u8be2",level:2},{value:"Time Travel\u67e5\u8be2",id:"time-travel\u67e5\u8be2",level:2},{value:"\u5168\u91cf\u67e5\u8be2",id:"\u5168\u91cf\u67e5\u8be2",level:3},{value:"\u5feb\u7167\u67e5\u8be2",id:"\u5feb\u7167\u67e5\u8be2",level:3},{value:"\u589e\u91cf\u67e5\u8be2",id:"\u589e\u91cf\u67e5\u8be2",level:3},{value:"\u66f4\u591a\u6848\u4f8b",id:"\u66f4\u591a\u6848\u4f8b",level:2}],c={toc:i},m="wrapper";function d(e){let{components:a,...l}=e;return(0,n.kt)(m,(0,t.Z)({},c,l,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"spark-\u5feb\u901f\u5f00\u59cb"},"Spark \u5feb\u901f\u5f00\u59cb"),(0,n.kt)("h2",{id:"\u914d\u7f6e"},"\u914d\u7f6e"),(0,n.kt)("p",null,"\u8981\u5728Spark\u4e2d\u4f7f\u7528LakeSoul\uff0c\u8bf7\u9996\u5148\u914d\u7f6e",(0,n.kt)("a",{parentName:"p",href:"/zh-Hans/docs/Getting%20Started/setup-local-env"},"Spark Catalog"),"\u3002LakeSoul\u4f7f\u7528Apache Spark\u7684DataSourceV2 API\u6765\u5b9e\u73b0\u6570\u636e\u6e90\u548c\u76ee\u5f55\u3002\u6b64\u5916\uff0cLakeSoul\u8fd8\u63d0\u4f9b\u4e86 Scala \u7684\u8868API\uff0c\u4ee5\u6269\u5c55LakeSoul\u6570\u636e\u8868\u7684\u529f\u80fd\u3002"),(0,n.kt)("h3",{id:"spark-3-support-matrix"},"Spark 3 Support Matrix"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"LakeSoul"),(0,n.kt)("th",{parentName:"tr",align:null},"Spark Version"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.2.x-2.4.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.3.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.0.x-2.1.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.1.x")))),(0,n.kt)("h3",{id:"spark-shellsqlpyspark"},"Spark Shell/SQL/PySpark"),(0,n.kt)("p",null,"\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulSparkSessionExtension")," sql\u6269\u5c55\u6765\u8fd0\u884cspark-shell/spark-sql/pyspark\u3002"),(0,n.kt)(r.Z,{defaultValue:"Spark SQL",values:[{label:"Spark SQL",value:"Spark SQL"},{label:"Scala",value:"Scala"},{label:"PySpark",value:"PySpark"}],mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-sql --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul  --jars lakesoul-spark-2.5.0-spark-3.3.jar\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul  --jars lakesoul-spark-2.5.0-spark-3.3.jar\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://github.com/lakesoul-io/LakeSoul/tree/main/python/lakesoul/spark/tables.py\npyspark --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --jars lakesoul-spark-2.5.0-spark-3.3.jar --py-files tables.py\n")))),(0,n.kt)("h3",{id:"maven-\u9879\u76ee\u4f9d\u8d56\u914d\u7f6e"},"Maven \u9879\u76ee\u4f9d\u8d56\u914d\u7f6e"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>com.dmetasoul</groupId>\n    <artifactId>lakesoul</artifactId>\n    <version>2.5.0-spark-3.3</version>\n</dependency>\n")),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.lakesoul.LakeSoulOptions\nimport spark.implicits._\nimport com.dmetasoul.lakesoul.tables.LakeSoulTable\n\n\nval builder = SparkSession.builder()\n    .master("local")\n    .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog")\n    .config("spark.sql.defaultCatalog", "lakesoul")\nval spark = builder.getOrCreate()\n\n')))),(0,n.kt)("h2",{id:"\u521b\u5efa\u547d\u540d\u7a7a\u95f4"},"\u521b\u5efa\u547d\u540d\u7a7a\u95f4"),(0,n.kt)("p",null,"\u9996\u5148\uff0c\u4e3aLakeSoul\u8868\u521b\u5efa\u4e00\u4e2anamespace\uff0c\u5982\u679c\u4e0d\u521b\u5efa\u5c06\u4f7f\u7528\u9ed8\u8ba4\u7684namespace\uff0cLakeSoul Catalog\u7684\u9ed8\u8ba4namespace\u662f",(0,n.kt)("inlineCode",{parentName:"p"},"default"),"\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE NAMESPACE IF NOT EXISTS lakesoul_namespace;\nUSE lakesoul_namespace;\nSHOW TABLES;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.sql("CREATE NAMESPACE IF NOT EXISTS lakesoul_namespace")\nspark.sql("USE lakesoul_namespace")\nspark.sql("SHOW TABLES")\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'// python\nspark.sql("CREATE NAMESPACE IF NOT EXISTS lakesoul_namespace")\nspark.sql("USE lakesoul_namespace")\nspark.sql("SHOW TABLES")\n')))),(0,n.kt)("h2",{id:"\u521b\u5efa\u8868"},"\u521b\u5efa\u8868"),(0,n.kt)("p",null,"\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"USING lakesoul"),"\u7684\u5b50\u53e5\u521b\u5efa\u4e00\u4e2a\u5206\u533a\u7684LakeSoul\u8868\uff0c\u6216\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"DataFrameWriterV2")," API\uff0c\u7b2c\u4e00\u6b21\u5199\u5165\u65f6\u81ea\u52a8\u521b\u5efa\u76f8\u5e94\u7684LakeSoul\u8868\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE lakesoul_table (id BIGINT, name STRING, `date` STRING) \nUSING lakesoul \nPARTITIONED BY (`date`) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table';\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"// Scala\nspark.sql(\"CREATE TABLE lakesoul_table (id BIGINT, name STRING, `date` STRING) USING lakesoul PARTITIONED BY (`date`) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table'\")\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"// python\nspark.sql(\"CREATE TABLE lakesoul_table (id BIGINT, name STRING, `date` STRING) USING lakesoul PARTITIONED BY (`date`) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table'\")\n")))),(0,n.kt)("h3",{id:"\u4e3b\u952e\u8868"},"\u4e3b\u952e\u8868"),(0,n.kt)("p",null,"\u5728LakeSoul\u4e2d\uff0c\u5e26\u6709\u4e3b\u952e\u7684\u8868\u88ab\u5b9a\u4e49\u4e3a\u54c8\u5e0c\u5206\u533a\u8868\u3002\u4f7f\u7528USING lakesoul\u5b50\u53e5\uff0c\u5e76\u7ed3\u5408TBLPROPERTIES\u8bbe\u7f6e\uff08\u5176\u4e2d'hashPartitions'\u6307\u5b9a\u4ee5\u9017\u53f7\u5206\u9694\u7684\u4e3b\u952e\u5217\u8868\uff0c'hashBucketNum'\u6307\u5b9a\u54c8\u5e0c\u6876\u7684\u5927\u5c0f\uff09\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u54c8\u5e0c\u5206\u533a\u7684LakeSoul\u8868\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, name STRING, date STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' \nTBLPROPERTIES ( 'hashPartitions'='id', 'hashBucketNum'='2');\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"// Scala\nspark.sql(\"CREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' TBLPROPERTIES ( 'hashPartitions'='id', 'hashBucketNum'='2')\")\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"// python\nspark.sql(\"CREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' TBLPROPERTIES ( 'hashPartitions'='id', 'hashBucketNum'='2')\")\n")))),(0,n.kt)("h3",{id:"\u4e3b\u952e-cdc-\u8868"},"\u4e3b\u952e CDC \u8868"),(0,n.kt)("p",null,"\u54c8\u5e0c\u5206\u533a\u7684LakeSoul\u8868\u5177\u6709\u53ef\u9009\u7684\u6570\u636e\u53d8\u66f4\u6355\u83b7\uff08CDC\uff09\u529f\u80fd\uff0c\u80fd\u591f\u8bb0\u5f55\u6570\u636e\u7684\u53d8\u5316\u3002\u8981\u521b\u5efa\u652f\u6301CDC\u7684LakeSoul\u8868\uff0c\u53ef\u4ee5\u5728\u54c8\u5e0c\u5206\u533a\u8868\u7684DDL\u8bed\u53e5\u4e2d\u6dfb\u52a0\u989d\u5916\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"TBLPROPERTIES"),"\u8bbe\u7f6e\uff0c\u6307\u5b9a",(0,n.kt)("inlineCode",{parentName:"p"},"'lakesoul_cdc_change_column'"),"\u5c5e\u6027\u3002\u8fd9\u4e2a\u5c5e\u6027\u5b9a\u4e49\u4e86\u4e00\u4e2a\u9690\u5f0f\u5217\uff0c\u5e2e\u52a9\u8868\u6709\u6548\u5730\u5904\u7406CDC\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u6570\u636e\u53d8\u66f4\u7684\u7cbe\u786e\u8ffd\u8e2a\u548c\u7ba1\u7406\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, name STRING, date STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' \nTBLPROPERTIES('hashPartitions'='id', 'hashBucketNum'='2', 'lakesoul_cdc_change_column' = 'op');\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"// Scala\nspark.sql(\"CREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' TBLPROPERTIES('hashPartitions'='id', 'hashBucketNum'='2', 'lakesoul_cdc_change_column' = 'op')\")\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"// python\nspark.sql(\"CREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' TBLPROPERTIES('hashPartitions'='id', 'hashBucketNum'='2', 'lakesoul_cdc_change_column' = 'op')\")\n")))),(0,n.kt)("h2",{id:"\u6570\u636e\u63d2\u5165\u5408\u5e76"},"\u6570\u636e\u63d2\u5165/\u5408\u5e76"),(0,n.kt)("p",null,"\u8981\u4f7f\u7528Spark SQL\u5411\u975e\u54c8\u5e0c\u5206\u533a\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"INSERT INTO"),"\u8bed\u53e5\u3002"),(0,n.kt)("p",null,"\u8981\u4f7f\u7528DataFrame\u5411\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"DataFrameWriterV2")," API\u3002\u5982\u679c\u8fd9\u662f\u5bf9\u8be5\u8868\u7684\u7b2c\u4e00\u6b21\u5199\u5165\uff0c\u5b83\u8fd8\u5c06\u81ea\u52a8\u521b\u5efa\u76f8\u5e94\u7684LakeSoul\u8868\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO TABLE lakesoul_table VALUES (1, 'Alice', '2024-01-01'), (2, 'Bob', '2024-01-01'), (1, 'Cathy', '2024-01-02');\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval data = Seq(Row(1L, "Alice", "2024-01-01"), Row(2L, "Bob", "2024-01-01"), Row(1L, "Cathy", "2024-01-02"))\nval schema = StructType(Seq(StructField("id", LongType, false), StructField("name", StringType, true), StructField("date", StringType, false)))\nval df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\ndf.write.format("lakesoul").insertInto("lakesoul_table")\n\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom pyspark.sql.types import *\ndata = [(1,"Cathy","2024-01-02")]\nschema = StructType([StructField("id", LongType(), False), StructField("name", StringType(), True), StructField("date", StringType(), False)])\ndf = spark.createDataFrame(data,schema=schema)\ndf.write.format("lakesoul").insertInto("lakesoul_table")\n')))),(0,n.kt)("p",null,"\u8981\u4f7f\u7528Spark SQL\u5411\u54c8\u5e0c\u5206\u533a\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"Merge INTO"),"\u8bed\u53e5\u3002"),(0,n.kt)("p",null,"\u8981\u4f7f\u7528DataFrame\u5411\u54c8\u5e0c\u5206\u533a\u8868\u5199\u5165\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable"),"\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"upsert")," API\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE OR REPLACE VIEW spark_catalog.default.source_view (id , name, date)\nAS SELECT 1L as `id`, 'George' as `name`, '2024-01-01' as `date`;\n\n\nMERGE INTO lakesoul_hash_table AS t \nUSING spark_catalog.default.source_view AS s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nimport com.dmetasoul.lakesoul.tables.LakeSoulTable\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\n\n// Init hash table with first dataframe\nval df = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4)).toDF("range", "hash", "value")\nval writer = df.write.format("lakesoul").mode("overwrite")\n\nwriter\n    .option("rangePartitions", "range")\n    .option("hashPartitions", "hash")\n    .option("hashBucketNum", 2)\n    .save(tablePath)\n\n// merge the second dataframe into hash table using LakeSoulTable upsert API\nval dfUpsert = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4)).toDF("range", "hash", "value")\nLakeSoulTable.forPath(tablePath).upsert(dfUpsert)\n\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom pyspark.sql.types import *\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\ndf = spark.createDataFrame([(20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4)],schema=\'range string,hash string,value string\')\ndf.write.format("lakesoul").mode("overwrite").option("rangePartitions", "range").option("hashPartitions", "hash").option("hashBucketNum", 2).save(tablePath)\ndfUpsert = spark.createDataFrame([(20201111, 1, 1), (20201111, 2, 2), (20201111, 3, 3), (20201112, 4, 4)],schema=\'range string,hash string,value string\')\nLakeSoulTable.forPath(spark,tablePath).upsert(dfUpsert)\n')))),(0,n.kt)("h2",{id:"\u6570\u636e\u66f4\u65b0"},"\u6570\u636e\u66f4\u65b0"),(0,n.kt)("p",null,"LakeSoul\u8868\u53ef\u4ee5\u901a\u8fc7DataFrame\u6216\u4f7f\u7528\u6807\u51c6\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"UPDATE"),"\u8bed\u53e5\u8fdb\u884c\u66f4\u65b0\u3002\u8981\u4f7f\u7528DataFrame\u66f4\u65b0\u8868\u4e2d\u7684\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable"),"\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"updateExpr")," API\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"UPDATE lakesoul_table SET name = 'David' WHERE id = 2;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).updateExpr("id = 2", Seq(("name"->"\'David\'")).toMap)\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\nLakeSoulTable.forPath(spark,tablePath).update("hash = 4", { "value":"5"})\n')))),(0,n.kt)("h2",{id:"\u6570\u636e\u5220\u9664"},"\u6570\u636e\u5220\u9664"),(0,n.kt)("p",null,"LakeSoul\u8868\u53ef\u4ee5\u901a\u8fc7DataFrame\u6216\u4f7f\u7528\u6807\u51c6\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"DELETE"),"\u8bed\u53e5\u6765\u5220\u9664\u8bb0\u5f55\u3002\u8981\u4f7f\u7528DataFrame\u4ece\u8868\u4e2d\u5220\u9664\u6570\u636e\uff0c\u8bf7\u4f7f\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable"),"\u7684",(0,n.kt)("inlineCode",{parentName:"p"},"delete")," API\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"DELETE FROM lakesoul_table WHERE id =1;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).delete("id = 1 or id =2")\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\nLakeSoulTable.forPath(spark,tablePath).delete("hash = 4")\n')))),(0,n.kt)("h2",{id:"\u6570\u636e\u67e5\u8be2"},"\u6570\u636e\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u8868\u53ef\u4ee5\u4f7f\u7528DataFrame\u6216Spark SQL\u8fdb\u884c\u67e5\u8be2\u3002"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM lakesoul_table;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\n\n// query data with DataFrameReader API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nspark.read.format("lakesoul").load(tablePath)\n\n// query data with LakeSoulTable API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).toDF\n\nval tableName = "lakesoul_table"\nLakeSoulTable.forName(tableName).toDF\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\n\n// query data with LakeSoulTable API\nLakeSoulTable.forPath(spark,tablePath).toDF().show()\n\n// query data with DataFrameReader API\nspark.read.format("lakesoul").load(tablePath).show()\n')))),(0,n.kt)("h2",{id:"time-travel\u67e5\u8be2"},"Time Travel\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u652f\u6301Time Travel\u67e5\u8be2\uff0c\u53ef\u4ee5\u67e5\u8be2\u5386\u53f2\u4e0a\u4efb\u4f55\u65f6\u95f4\u70b9\u7684\u8868\u6216\u4e24\u4e2a\u63d0\u4ea4\u65f6\u95f4\u4e4b\u95f4\u7684\u66f4\u6539\u6570\u636e\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/cdc_table"\nSeq(("range1", "hash1", "insert"), ("range2", "hash2", "insert"), ("range3", "hash2", "insert"), ("range4", "hash2", "insert"), ("range4", "hash4", "insert"), ("range3", "hash3", "insert"))\n    .toDF("range", "hash", "op")\n    .write\n    .mode("append")\n    .format("lakesoul")\n    .option("rangePartitions", "range")\n    .option("hashPartitions", "hash")\n    .option("hashBucketNum", "2")\n    .option("shortTableName", "cdc_table")\n    .option("lakesoul_cdc_change_column", "op")\n    .save(tablePath)\n// record the version of 1st commit \nimport java.text.SimpleDateFormat\n\nval versionA: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n\nval lakeTable = LakeSoulTable.forPath(tablePath)\nlakeTable.upsert(Seq(("range1", "hash1-1", "delete"), ("range2", "hash2-10", "delete"))\n.toDF("range", "hash", "op"))\n// record the version of 2nd commit \nval versionB: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\nlakeTable.upsert(Seq(("range1", "hash1-13", "insert"), ("range2", "hash2-13", "update"))\n.toDF("range", "hash", "op"))\nlakeTable.upsert(Seq(("range1", "hash1-15", "insert"), ("range2", "hash2-15", "update"))\n.toDF("range", "hash", "op"))\n// record the version of 3rd,4th commits \nval versionC: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n\n')),(0,n.kt)("h3",{id:"\u5168\u91cf\u67e5\u8be2"},"\u5168\u91cf\u67e5\u8be2"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.sql("SELECT * FROM cdc_table")\n')),(0,n.kt)("h3",{id:"\u5feb\u7167\u67e5\u8be2"},"\u5feb\u7167\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u652f\u6301\u5feb\u7167\u67e5\u8be2\uff0c\u53ef\u7528\u4e8e\u67e5\u8be2\u5386\u53f2\u4e0a\u67d0\u4e00\u65f6\u95f4\u70b9\u7684\u8868\u6570\u636e\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range2")\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, LakeSoulOptions.ReadType.SNAPSHOT_READ)\n    .load(tablePath)\n')),(0,n.kt)("h3",{id:"\u589e\u91cf\u67e5\u8be2"},"\u589e\u91cf\u67e5\u8be2"),(0,n.kt)("p",null,"LakeSoul\u652f\u6301\u589e\u91cf\u67e5\u8be2\uff0c\u53ef\u83b7\u5f97\u5728\u8d77\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4\u4e4b\u95f4\u53d1\u751f\u66f4\u6539\u7684\u6570\u636e\u8bb0\u5f55\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range1")\n    .option(LakeSoulOptions.READ_START_TIME, versionA)\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, LakeSoulOptions.ReadType.INCREMENTAL_READ)\n    .load(tablePath)\n')),(0,n.kt)("h2",{id:"\u66f4\u591a\u6848\u4f8b"},"\u66f4\u591a\u6848\u4f8b"),(0,n.kt)("p",null,"\u63a5\u4e0b\u6765\uff0c\u60a8\u53ef\u4ee5\u5728",(0,n.kt)("a",{parentName:"p",href:"/zh-Hans/docs/Usage%20Docs/spark-api-docs"},"Spark API\u6587\u6863"),"\u4e2d\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5728Spark\u4e2d\u4f7f\u7528LakeSoul\u8868\u7684\u6848\u4f8b\u3002"))}d.isMDXComponent=!0}}]);