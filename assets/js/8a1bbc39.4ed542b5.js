"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[205],{3905:(e,a,t)=>{t.d(a,{Zo:()=>k,kt:()=>m});var o=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function l(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);a&&(o=o.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,o)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?l(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,o,n=function(e,a){if(null==e)return{};var t,o,n={},l=Object.keys(e);for(o=0;o<l.length;o++)t=l[o],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(o=0;o<l.length;o++)t=l[o],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var i=o.createContext({}),p=function(e){var a=o.useContext(i),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},k=function(e){var a=p(e.components);return o.createElement(i.Provider,{value:a},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var a=e.children;return o.createElement(o.Fragment,{},a)}},d=o.forwardRef((function(e,a){var t=e.components,n=e.mdxType,l=e.originalType,i=e.parentName,k=s(e,["components","mdxType","originalType","parentName"]),u=p(t),d=n,m=u["".concat(i,".").concat(d)]||u[d]||c[d]||l;return t?o.createElement(m,r(r({ref:a},k),{},{components:t})):o.createElement(m,r({ref:a},k))}));function m(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var l=t.length,r=new Array(l);r[0]=d;var s={};for(var i in a)hasOwnProperty.call(a,i)&&(s[i]=a[i]);s.originalType=e,s[u]="string"==typeof e?e:n,r[1]=s;for(var p=2;p<l;p++)r[p]=t[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}d.displayName="MDXCreateElement"},4213:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>i,contentTitle:()=>r,default:()=>c,frontMatter:()=>l,metadata:()=>s,toc:()=>p});var o=t(7462),n=(t(7294),t(3905));const l={},r="Quick Setup Environment",s={unversionedId:"Getting Started/setup-local-env",id:"Getting Started/setup-local-env",title:"Quick Setup Environment",description:"\x3c!--",source:"@site/docs/01-Getting Started/01-setup-local-env.md",sourceDirName:"01-Getting Started",slug:"/Getting Started/setup-local-env",permalink:"/docs/Getting Started/setup-local-env",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/01-setup-local-env.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LakeSoul General Concepts Introduction",permalink:"/docs/Getting Started/concepts"},next:{title:"Spark Getting Started Guide",permalink:"/docs/Getting Started/spark-guide"}},i={},p=[{value:"1. Set up a test environment in the Linux local file system",id:"1-set-up-a-test-environment-in-the-linux-local-file-system",level:2},{value:"1.1 Start A Local PostgreSQL DB",id:"11-start-a-local-postgresql-db",level:3},{value:"1.2 PG Database Initialization",id:"12-pg-database-initialization",level:3},{value:"1.3 Lakesoul PG Database Configuration Description:",id:"13-lakesoul-pg-database-configuration-description",level:3},{value:"1.4 Install an Apache Spark environment",id:"14-install-an-apache-spark-environment",level:3},{value:"1.4.1 Start spark-shell for testing LakeSoul",id:"141-start-spark-shell-for-testing-lakesoul",level:4},{value:"1.4.2 Write data to object storage service",id:"142-write-data-to-object-storage-service",level:4},{value:"LakeSoul Spark Conf Parameters",id:"lakesoul-spark-conf-parameters",level:4},{value:"1.5 Setup Flink environment",id:"15-setup-flink-environment",level:3},{value:"1.5.1 Start Flink SQL shell",id:"151-start-flink-sql-shell",level:4},{value:"1.5.2 Write data to object storage service",id:"152-write-data-to-object-storage-service",level:4},{value:"2. Start on Hadoop, Spark and Flink cluster environments",id:"2-start-on-hadoop-spark-and-flink-cluster-environments",level:2},{value:"2.1 Add the following information to the Spark configuration file spark-defaults.conf",id:"21-add-the-following-information-to-the-spark-configuration-file-spark-defaultsconf",level:3},{value:"2.2 Add the following information to the Flink configuration file flink-conf.yaml",id:"22-add-the-following-information-to-the-flink-configuration-file-flink-confyaml",level:3},{value:"2.3 Configuration global environment",id:"23-configuration-global-environment",level:3},{value:"3. Use Docker Compose to Start a Local Cluster",id:"3-use-docker-compose-to-start-a-local-cluster",level:2},{value:"3.1 Docker Compose Files",id:"31-docker-compose-files",level:3},{value:"3.2 Install Docker Compose",id:"32-install-docker-compose",level:3},{value:"3.3 Start docker compose",id:"33-start-docker-compose",level:3},{value:"3.4 Run LakeSoul Tests in Docker Compose Env",id:"34-run-lakesoul-tests-in-docker-compose-env",level:3},{value:"3.4.1 Prepare LakeSoul Properties File",id:"341-prepare-lakesoul-properties-file",level:4},{value:"3.4.2 Prepare Spark Image",id:"342-prepare-spark-image",level:4},{value:"3.4.3 Start Spark Shell",id:"343-start-spark-shell",level:4},{value:"3.4.4 Execute LakeSoul Scala APIs",id:"344-execute-lakesoul-scala-apis",level:4},{value:"3.4.5 Verify Data Written Successfully",id:"345-verify-data-written-successfully",level:4},{value:"3.5 Cleanup Meta Tables and MinIO Bucket",id:"35-cleanup-meta-tables-and-minio-bucket",level:3},{value:"3.6 Shutdown Docker Compose Env",id:"36-shutdown-docker-compose-env",level:3}],k={toc:p},u="wrapper";function c(e){let{components:a,...t}=e;return(0,n.kt)(u,(0,o.Z)({},k,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"quick-setup-environment"},"Quick Setup Environment"),(0,n.kt)("h2",{id:"1-set-up-a-test-environment-in-the-linux-local-file-system"},"1. Set up a test environment in the Linux local file system"),(0,n.kt)("p",null,"To store data on local disk, only a PostgreSQL database is required."),(0,n.kt)("h3",{id:"11-start-a-local-postgresql-db"},"1.1 Start A Local PostgreSQL DB"),(0,n.kt)("p",null,"The quickest way to start a pg DB is via docker container:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"docker run -d --name lakesoul-test-pg -p5432:5432 -e POSTGRES_USER=lakesoul_test -e POSTGRES_PASSWORD=lakesoul_test -e POSTGRES_DB=lakesoul_test -d postgres:14.5\n")),(0,n.kt)("h3",{id:"12-pg-database-initialization"},"1.2 PG Database Initialization"),(0,n.kt)("p",null,"Init PG database of LakeSoul using ",(0,n.kt)("inlineCode",{parentName:"p"},"script/meta_init.cql"),".\nExecute code blow in the LakeSoul base directory:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'docker cp script/meta_init.sql lakesoul-test-pg:/\n\ndocker exec -i lakesoul-test-pg sh -c "PGPASSWORD=lakesoul_test psql -h localhost -p 5432 -U lakesoul_test -f meta_init.sql"\n')),(0,n.kt)("h3",{id:"13-lakesoul-pg-database-configuration-description"},"1.3 Lakesoul PG Database Configuration Description:"),(0,n.kt)("p",null,"By default, the PG database is connected to the local database. The configuration information is as follows,"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-txt"},"lakesoul.pg.driver=com.lakesoul.shaded.org.postgresql.Driver\nlakesoul.pg.url=jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\nlakesoul.pg.username=lakesoul_test\nlakesoul.pg.password=lakesoul_test\n")),(0,n.kt)("p",null,"LakeSoul supports to customize PG database configuration information. Add an environment variable ",(0,n.kt)("inlineCode",{parentName:"p"},"lakesoul_home")," before starting the Spark program to include the configuration file information."),(0,n.kt)("p",null,"For example, the PG database configuration information file path name is: ",(0,n.kt)("inlineCode",{parentName:"p"},"/opt/soft/pg.property"),", you need to add this environment variable before the program starts:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"export lakesoul_home=/opt/soft/pg.property\n")),(0,n.kt)("p",null,"You can put customized database configuration information in this file."),(0,n.kt)("h3",{id:"14-install-an-apache-spark-environment"},"1.4 Install an Apache Spark environment"),(0,n.kt)("p",null,"You could download spark distribution from ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/downloads.html"},"https://spark.apache.org/downloads.html"),", and please choose spark version 3.3.0 or above. Note that the official package from Apache Spark does not include hadoop-cloud component. We provide a Spark package with Hadoop cloud dependencies, download it from ",(0,n.kt)("a",{parentName:"p",href:"https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop3.tgz"},"https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop3.tgz"),"."),(0,n.kt)("p",null,"After unpacking spark package, you could find LakeSoul distribution jar from ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/lakesoul-io/LakeSoul/releases"},"https://github.com/lakesoul-io/LakeSoul/releases"),". Download the jar file put it into ",(0,n.kt)("inlineCode",{parentName:"p"},"jars")," directory of your spark environment."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop-3.tgz\ntar xf spark-3.3.2-bin-hadoop-3.tgz\nexport SPARK_HOME=${PWD}/spark-3.3.2-bin-hadoop3\nwget https://github.com/lakesoul-io/LakeSoul/releases/download/v2.4.0/lakesoul-spark-2.4.0-spark-3.3.jar -P $SPARK_HOME/jars\n")),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"For production deployment on Hadoop, it's recommended to use spark release without bundled hadoop:"),(0,n.kt)("p",{parentName:"admonition"},(0,n.kt)("a",{parentName:"p",href:"https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-without-hadoop.tgz"},"https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-without-hadoop.tgz")),(0,n.kt)("p",{parentName:"admonition"},"Refer to ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/hadoop-provided.html"},"https://spark.apache.org/docs/latest/hadoop-provided.html")," on how to setup hadoop classpath.")),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"Since 2.1.0, LakeSoul package all its dependencies into one single jar via maven shade plugin. Before that all jars were packaged into one tar.gz file.")),(0,n.kt)("h4",{id:"141-start-spark-shell-for-testing-lakesoul"},"1.4.1 Start spark-shell for testing LakeSoul"),(0,n.kt)("p",null,"cd into the spark installation directory, and start an interactive spark-shell:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul\n")),(0,n.kt)("h4",{id:"142-write-data-to-object-storage-service"},"1.4.2 Write data to object storage service"),(0,n.kt)("p",null,"It is necessary to add information such as object storage access key, secret key and endpoint."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --conf spark.hadoop.fs.s3a.access.key=XXXXXX --conf spark.hadoop.fs.s3a.secret.key=XXXXXX --conf spark.hadoop.fs.s3a.endpoint=XXXXXX --conf spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n")),(0,n.kt)("h4",{id:"lakesoul-spark-conf-parameters"},"LakeSoul Spark Conf Parameters"),(0,n.kt)("p",null,"Before start to use Lakesoul, we should add some paramethers in ",(0,n.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf")," or ",(0,n.kt)("inlineCode",{parentName:"p"},"Spark Session Builder"),"\u3002"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Key"),(0,n.kt)("th",{parentName:"tr",align:null},"Value"),(0,n.kt)("th",{parentName:"tr",align:null},"Description"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"spark.sql.extensions"),(0,n.kt)("td",{parentName:"tr",align:null},"com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension"),(0,n.kt)("td",{parentName:"tr",align:null},"extention name for spark sql")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"spark.sql.catalog.lakesoul"),(0,n.kt)("td",{parentName:"tr",align:null},"org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog"),(0,n.kt)("td",{parentName:"tr",align:null},"plug in LakeSoul's catalog")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"spark.sql.defaultCatalog"),(0,n.kt)("td",{parentName:"tr",align:null},"lakesoul"),(0,n.kt)("td",{parentName:"tr",align:null},"set default catalog for spark")))),(0,n.kt)("h3",{id:"15-setup-flink-environment"},"1.5 Setup Flink environment"),(0,n.kt)("p",null,"Download LakeSoul Flink jars\uff1a",(0,n.kt)("a",{parentName:"p",href:"https://github.com/lakesoul-io/LakeSoul/releases/download/v2.4.1/lakesoul-flink-2.4.1-flink-1.17.jar"},"https://github.com/lakesoul-io/LakeSoul/releases/download/v2.4.1/lakesoul-flink-2.4.1-flink-1.17.jar"),"\nDownload Flink jars\uff1a",(0,n.kt)("a",{parentName:"p",href:"https://dlcdn.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz"},"https://dlcdn.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz")),(0,n.kt)("h4",{id:"151-start-flink-sql-shell"},"1.5.1 Start Flink SQL shell"),(0,n.kt)("p",null,"After creating the pg database and ",(0,n.kt)("inlineCode",{parentName:"p"},"lakesoul_home")," configuration file, place the LakeSoul Flink jars in the FLink directory.\nEnter the Flink installation directory and execute the following command:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"export lakesoul_home=/opt/soft/pg.property && ./bin/start-cluster.sh\n\nexport lakesoul_home=/opt/soft/pg.property && ./bin/sql-client.sh embedded -j lakesoul-flink-2.4.1-flink-1.17.jar\n")),(0,n.kt)("h4",{id:"152-write-data-to-object-storage-service"},"1.5.2 Write data to object storage service"),(0,n.kt)("p",null,"Access key, Secret key and Endpoint information need to be added to the Flink configuration file flink-conf.yaml"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"s3.access-key: XXXXXX\ns3.secret-key: XXXXXX\ns3.endpoint: XXXXXX\n")),(0,n.kt)("p",null,"Place flink-s3-fs-hadoop.jar and flink-shaded-hadoop-2-uber-2.6.5-10.0.jar under Flink/lib\nDownload flink-s3-fs-hadoop.jar: ",(0,n.kt)("a",{parentName:"p",href:"https://repo1.maven.org/maven2/org/apache/flink/flink-s3-fs-hadoop/1.17.2/flink-s3-fs-hadoop-1.17.2.jar"},"https://repo1.maven.org/maven2/org/apache/flink/flink-s3-fs-hadoop/1.17.2/flink-s3-fs-hadoop-1.17.2.jar"),"\nDownload flink-shaded-hadoop-2-uber-2.6.5-10.0.jar: ",(0,n.kt)("a",{parentName:"p",href:"https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-10.0/flink-shaded-hadoop-2-uber-2.6.5-10.0.jar"},"https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-10.0/flink-shaded-hadoop-2-uber-2.6.5-10.0.jar")),(0,n.kt)("h2",{id:"2-start-on-hadoop-spark-and-flink-cluster-environments"},"2. Start on Hadoop, Spark and Flink cluster environments"),(0,n.kt)("p",null,"Run LakeSoul tasks on Hadoop you only need to add the relevant configuration information to the environment variables and Spark and FLink cluster configurations. The specific operations are as follows:"),(0,n.kt)("h3",{id:"21-add-the-following-information-to-the-spark-configuration-file-spark-defaultsconf"},"2.1 Add the following information to the Spark configuration file spark-defaults.conf"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension\nspark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog\nspark.sql.defaultCatalog=lakesoul\n\nspark.yarn.appMasterEnv.LAKESOUL_PG_DRIVER=com.lakesoul.shaded.org.postgresql.Driver\nspark.yarn.appMasterEnv.LAKESOUL_PG_URL=jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\nspark.yarn.appMasterEnv.LAKESOUL_PG_USERNAME=lakesoul_test\nspark.yarn.appMasterEnv.LAKESOUL_PG_PASSWORD=lakesoul_test\n")),(0,n.kt)("h3",{id:"22-add-the-following-information-to-the-flink-configuration-file-flink-confyaml"},"2.2 Add the following information to the Flink configuration file flink-conf.yaml"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"containerized.master.env.LAKESOUL_PG_DRIVER: com.lakesoul.shaded.org.postgresql.Driver\ncontainerized.master.env.LAKESOUL_PG_USERNAME: postgres\ncontainerized.master.env.LAKESOUL_PG_PASSWORD: postgres123\ncontainerized.master.env.LAKESOUL_PG_URL: jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\ncontainerized.taskmanager.env.LAKESOUL_PG_DRIVER: com.lakesoul.shaded.org.postgresql.Driver\ncontainerized.taskmanager.env.LAKESOUL_PG_USERNAME: lakesoul_test\ncontainerized.taskmanager.env.LAKESOUL_PG_PASSWORD: lakesoul_test\ncontainerized.taskmanager.env.LAKESOUL_PG_URL: jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\n")),(0,n.kt)("h3",{id:"23-configuration-global-environment"},"2.3 Configuration global environment"),(0,n.kt)("p",null,"Configure global environment variable information on the client machine. Here you need to write the variable information into an env.sh file, the content is as follows:\nHere the Hadoop version is 3.1.4.0-315, the Spark version is spark-3.3.2, and the Flink version is flink-1.17.2"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nexport HADOOP_HOME="/usr/hdp/3.1.4.0-315/hadoop"\nexport HADOOP_HDFS_HOME="/usr/hdp/3.1.4.0-315/hadoop-hdfs"\nexport HADOOP_MAPRED_HOME="/usr/hdp/3.1.4.0-315/hadoop-mapreduce"\nexport HADOOP_YARN_HOME="/usr/hdp/3.1.4.0-315/hadoop-yarn"\nexport HADOOP_LIBEXEC_DIR="/usr/hdp/3.1.4.0-315/hadoop/libexec"\nexport HADOOP_CONF_DIR="/usr/hdp/3.1.4.0-315/hadoop/conf"\n\nexport SPARK_HOME=/usr/hdp/spark-3.3.2-bin-without-hadoop-ddf\nexport SPARK_CONF_DIR=/home/lakesoul/lakesoul_hadoop_ci/LakeSoul-main/LakeSoul/script/benchmark/hadoop/spark-conf\n\nexport FLINK_HOME=/opt/flink-1.17.2\nexport FLINK_CONF_DIR=/opt/flink-1.17.2/conf\nexport PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$FLINK_HOME/bin:$JAVA_HOME/bin:$PATH\nexport HADOOP_CLASSPATH=$(hadoop classpath)\nexport SPARK_DIST_CLASSPATH=$HADOOP_CLASSPATH\nexport LAKESOUL_PG_DRIVER=com.lakesoul.shaded.org.postgresql.Driver\nexport LAKESOUL_PG_URL=jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\nexport LAKESOUL_PG_USERNAME=lakesoul_test\nexport LAKESOUL_PG_PASSWORD=lakesoul_test\n')),(0,n.kt)("p",null,"After configuring the above information, execute the following command, and then you can submit the LakeSoul task to the yarn cluster for running on the client"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"source env.sh\n")),(0,n.kt)("h2",{id:"3-use-docker-compose-to-start-a-local-cluster"},"3. Use Docker Compose to Start a Local Cluster"),(0,n.kt)("h3",{id:"31-docker-compose-files"},"3.1 Docker Compose Files"),(0,n.kt)("p",null,"We provide a docker compose env to quickly start a local PostgreSQL service and a MinIO S3 Storage service. The docker compose env is located under ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/lakesoul-io/LakeSoul/tree/main/docker/lakesoul-docker-compose-env"},"lakesoul-docker-compose-env"),"."),(0,n.kt)("h3",{id:"32-install-docker-compose"},"3.2 Install Docker Compose"),(0,n.kt)("p",null,"To install docker compose, please refer to ",(0,n.kt)("a",{parentName:"p",href:"https://docs.docker.com/engine/install/"},"Install Docker Engine")),(0,n.kt)("h3",{id:"33-start-docker-compose"},"3.3 Start docker compose"),(0,n.kt)("p",null,"To start the docker compose env, cd into the docker compose env dir, and execute the command:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"cd docker/lakesoul-docker-compose-env/\ndocker compose up -d\n")),(0,n.kt)("p",null,"Then use ",(0,n.kt)("inlineCode",{parentName:"p"},"docker compose ps")," to check both services' statuses are ",(0,n.kt)("inlineCode",{parentName:"p"},"running(healthy)"),". The PostgreSQL service would automatically setup the database and tables required by LakeSoul Meta. And the MinIO service would setup a public bucket. You can change the user, password, database name and MinIO bucket name accordingly in the ",(0,n.kt)("inlineCode",{parentName:"p"},"docker-compose.yml")," file."),(0,n.kt)("h3",{id:"34-run-lakesoul-tests-in-docker-compose-env"},"3.4 Run LakeSoul Tests in Docker Compose Env"),(0,n.kt)("h4",{id:"341-prepare-lakesoul-properties-file"},"3.4.1 Prepare LakeSoul Properties File"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-ini",metastring:'title="lakesoul.properties"',title:'"lakesoul.properties"'},"lakesoul.pg.driver=com.lakesoul.shaded.org.postgresql.Driver\nlakesoul.pg.url=jdbc:postgresql://lakesoul-docker-compose-env-lakesoul-meta-db-1:5432/lakesoul_test?stringtype=unspecified\nlakesoul.pg.username=lakesoul_test\nlakesoul.pg.password=lakesoul_test\n")),(0,n.kt)("h4",{id:"342-prepare-spark-image"},"3.4.2 Prepare Spark Image"),(0,n.kt)("p",null,"You could use bitnami's Spark 3.3 docker image with packaged hadoop denendencies:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"docker pull bitnami/spark:3.3.1\n")),(0,n.kt)("h4",{id:"343-start-spark-shell"},"3.4.3 Start Spark Shell"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"docker run --net lakesoul-docker-compose-env_default --rm -ti \\\n    -v $(pwd)/lakesoul.properties:/opt/spark/work-dir/lakesoul.properties \\\n    --env lakesoul_home=/opt/spark/work-dir/lakesoul.properties bitnami/spark:3.3.1 \\\n    spark-shell \\\n    --packages com.dmetasoul:lakesoul-spark:2.4.0-spark-3.3 \\\n    --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension \\\n    --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog \\\n    --conf spark.sql.defaultCatalog=lakesoul \\\n    --conf spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n    --conf spark.hadoop.fs.s3a.buffer.dir=/opt/spark/work-dir/s3a \\\n    --conf spark.hadoop.fs.s3a.path.style.access=true \\\n    --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \\\n    --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\n")),(0,n.kt)("h4",{id:"344-execute-lakesoul-scala-apis"},"3.4.4 Execute LakeSoul Scala APIs"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'val tablePath= "s3://lakesoul-test-bucket/test_table"\nval df = Seq(("2021-01-01",1,"rice"),("2021-01-01",2,"bread")).toDF("date","id","name")\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id")\n  .option("hashBucketNum","2")\n  .save(tablePath)\n')),(0,n.kt)("h4",{id:"345-verify-data-written-successfully"},"3.4.5 Verify Data Written Successfully"),(0,n.kt)("p",null,"Open link ",(0,n.kt)("a",{parentName:"p",href:"http://127.0.0.1:9001/buckets/lakesoul-test-bucket/browse/"},"http://127.0.0.1:9001/buckets/lakesoul-test-bucket/browse/")," in your browser to verify that LakeSoul table has been written to MinIO successfully.\nUse minioadmin1:minioadmin1 to login into MinIO's console."),(0,n.kt)("h3",{id:"35-cleanup-meta-tables-and-minio-bucket"},"3.5 Cleanup Meta Tables and MinIO Bucket"),(0,n.kt)("p",null,"To cleanup all contents in LakeSoul meta tables, execute:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"docker exec -ti lakesoul-docker-compose-env-lakesoul-meta-db-1 psql -h localhost -U lakesoul_test -d lakesoul_test -f /meta_cleanup.sql\n")),(0,n.kt)("p",null,"To cleanup all contents in MinIO bucket, execute:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"docker run --net lakesoul-docker-compose-env_default --rm -t bitnami/spark:3.3.1 aws --no-sign-request --endpoint-url http://minio:9000 s3 rm --recursive s3://lakesoul-test-bucket/\n")),(0,n.kt)("h3",{id:"36-shutdown-docker-compose-env"},"3.6 Shutdown Docker Compose Env"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"cd docker/lakesoul-docker-compose-env/\ndocker compose stop\ndocker compose down\n")))}c.isMDXComponent=!0}}]);