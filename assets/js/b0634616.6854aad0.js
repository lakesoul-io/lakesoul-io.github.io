"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[844],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>k});var n=a(7294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var u=n.createContext({}),s=function(e){var t=n.useContext(u),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=s(e.components);return n.createElement(u.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,r=e.originalType,u=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),m=s(a),c=l,k=m["".concat(u,".").concat(c)]||m[c]||d[c]||r;return a?n.createElement(k,i(i({ref:t},p),{},{components:a})):n.createElement(k,i({ref:t},p))}));function k(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=a.length,i=new Array(r);i[0]=c;var o={};for(var u in t)hasOwnProperty.call(t,u)&&(o[u]=t[u]);o.originalType=e,o[m]="string"==typeof e?e:l,i[1]=o;for(var s=2;s<r;s++)i[s]=a[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},4823:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>s});var n=a(7462),l=(a(7294),a(3905));const r={},i="Multi Stream Merge to Build Wide Table Tutorial",o={unversionedId:"Tutorials/mutil-stream-merge",id:"Tutorials/mutil-stream-merge",title:"Multi Stream Merge to Build Wide Table Tutorial",description:"To build wide table, traditional data warehouse or ETL uses multi tables join according to the primary and foreign key. When there is a large amount of data or multiple joins are required, there will be problems such as low efficiency, large memory consumption, and even OOM. In addition, the Shuffle process takes up most of the data exchange time, and is inefficient. LakeSoul has supported upsert with merge operator, which can be used to implement multi stream merge in realtime, and avoid the above problems by eliminating join and shuffle. The following is a specific example of this scenario.",source:"@site/docs/02-Tutorials/06-mutil-stream-merge.md",sourceDirName:"02-Tutorials",slug:"/Tutorials/mutil-stream-merge",permalink:"/docs/Tutorials/mutil-stream-merge",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/02-Tutorials/06-mutil-stream-merge.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Upsert Data and Merge UDF Tutorial",permalink:"/docs/Tutorials/upsert-and-merge-udf"},next:{title:"Multiple Kafka Topics Data to LakeSoul Tutorial",permalink:"/docs/Tutorials/kafka-topics-data-to-lakesoul"}},u={},s=[],p={toc:s},m="wrapper";function d(e){let{components:t,...a}=e;return(0,l.kt)(m,(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"multi-stream-merge-to-build-wide-table-tutorial"},"Multi Stream Merge to Build Wide Table Tutorial"),(0,l.kt)("p",null,"To build wide table, traditional data warehouse or ETL uses multi tables join according to the primary and foreign key. When there is a large amount of data or multiple joins are required, there will be problems such as low efficiency, large memory consumption, and even OOM. In addition, the Shuffle process takes up most of the data exchange time, and is inefficient. LakeSoul has supported upsert with merge operator, which can be used to implement multi stream merge in realtime, and avoid the above problems by eliminating join and shuffle. The following is a specific example of this scenario."),(0,l.kt)("p",null,"Suppose there are data of the following streams, A, B, C and D. The data contents of each stream are as follows:"),(0,l.kt)("p",null,"A:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Ip"),(0,l.kt)("th",{parentName:"tr",align:null},"sy"),(0,l.kt)("th",{parentName:"tr",align:null},"us"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1.1.1.1"),(0,l.kt)("td",{parentName:"tr",align:null},"30"),(0,l.kt)("td",{parentName:"tr",align:null},"40")))),(0,l.kt)("p",null,"B:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Ip"),(0,l.kt)("th",{parentName:"tr",align:null},"free"),(0,l.kt)("th",{parentName:"tr",align:null},"cache"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1.1.1.1"),(0,l.kt)("td",{parentName:"tr",align:null},"1677"),(0,l.kt)("td",{parentName:"tr",align:null},"455")))),(0,l.kt)("p",null,"C:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Ip"),(0,l.kt)("th",{parentName:"tr",align:null},"level"),(0,l.kt)("th",{parentName:"tr",align:null},"des"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1.1.1.2"),(0,l.kt)("td",{parentName:"tr",align:null},"error"),(0,l.kt)("td",{parentName:"tr",align:null},"killed")))),(0,l.kt)("p",null,"D:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Ip"),(0,l.kt)("th",{parentName:"tr",align:null},"qps"),(0,l.kt)("th",{parentName:"tr",align:null},"tps"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1.1.1.1"),(0,l.kt)("td",{parentName:"tr",align:null},"30"),(0,l.kt)("td",{parentName:"tr",align:null},"40")))),(0,l.kt)("p",null,"Finally, a large wide table needs to be formed, and the four tables need to be consolidated and displayed as follows:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"IP"),(0,l.kt)("th",{parentName:"tr",align:null},"sy"),(0,l.kt)("th",{parentName:"tr",align:null},"us"),(0,l.kt)("th",{parentName:"tr",align:null},"free"),(0,l.kt)("th",{parentName:"tr",align:null},"cache"),(0,l.kt)("th",{parentName:"tr",align:null},"level"),(0,l.kt)("th",{parentName:"tr",align:null},"des"),(0,l.kt)("th",{parentName:"tr",align:null},"qps"),(0,l.kt)("th",{parentName:"tr",align:null},"tps"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1.1.1.1"),(0,l.kt)("td",{parentName:"tr",align:null},"30"),(0,l.kt)("td",{parentName:"tr",align:null},"40"),(0,l.kt)("td",{parentName:"tr",align:null},"1677"),(0,l.kt)("td",{parentName:"tr",align:null},"455"),(0,l.kt)("td",{parentName:"tr",align:null},"null"),(0,l.kt)("td",{parentName:"tr",align:null},"null"),(0,l.kt)("td",{parentName:"tr",align:null},"30"),(0,l.kt)("td",{parentName:"tr",align:null},"40")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"1.1.1.2"),(0,l.kt)("td",{parentName:"tr",align:null},"null"),(0,l.kt)("td",{parentName:"tr",align:null},"null"),(0,l.kt)("td",{parentName:"tr",align:null},"null"),(0,l.kt)("td",{parentName:"tr",align:null},"null"),(0,l.kt)("td",{parentName:"tr",align:null},"error"),(0,l.kt)("td",{parentName:"tr",align:null},"killed"),(0,l.kt)("td",{parentName:"tr",align:null},"null"),(0,l.kt)("td",{parentName:"tr",align:null},"null")))),(0,l.kt)("p",null,"Traditionally, to perform the above operations, four tables need to be joined three times according to the primary key (IP). The writing method is as follows:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"Select \n       A.IP as IP,  \n       A.sy as sy, \n       A.us as us, \n       B.free as free, \n       B.cache as cache, \n       C.level as level, \n       C.des as des, \n       D.qps as qps, \n       D.tps as tps \nfrom A join B on A.IP = B.IP \n    join C on C.IP = A.IP \n    join D on D.IP = A.IP.\n")),(0,l.kt)("p",null,"LakeSoul supports multi stream merge with different schemas (same primary keys should exist), and can automatically extend the schema of the table according to the primary key from multiple streams. If the newly written data field does not exist in the original table, it will automatically extend the table schema. The non-existent field is null by default. Therefore, the same resulting data can be achieved by writing each stream data to LakeSoul through upsert without table join. The above process code is implemented as follows:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .config("spark.dmetasoul.lakesoul.schema.autoMerge.enabled", "true")\n  .getOrCreate()\nimport spark.implicits._\n\nval df1 = Seq(("1.1.1.1", 30, 40)).toDF("IP", "sy", "us")\nval df2 = Seq(("1.1.1.1", 1677, 455)).toDF("IP", "free", "cache")\nval df3 = Seq(("1.1.1.2", "error", "killed")).toDF("IP", "level", "des")\nval df4 = Seq(("1.1.1.1", 30, 40)).toDF("IP", "qps", "tps")\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\n\ndf1.write\n  .mode("append")\n  .format("lakesoul")\n  .option("hashPartitions","IP")\n  .option("hashBucketNum","2")\n  .save(tablePath)\n\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\n\nlakeSoulTable.upsert(df2)\nlakeSoulTable.upsert(df3)\nlakeSoulTable.upsert(df4)\nlakeSoulTable.toDF.show()\n\n/**\n *  result\n *  |  IP   |  sy|  us|free|cache|level|   des| qps| tps|\n *  +-------+----+----+----+-----+-----+------+----+----+\n *  |1.1.1.2|null|null|null| null|error|killed|null|null|\n *  |1.1.1.1|  30|  40|1677|  455| null|  null|  30|  40|\n *  +-------+----+----+----+-----+-----+------+----+----+\n */\n\n')))}d.isMDXComponent=!0}}]);