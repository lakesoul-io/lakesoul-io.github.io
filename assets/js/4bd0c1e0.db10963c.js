"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[164],{3905:(t,e,a)=>{a.d(e,{Zo:()=>m,kt:()=>h});var n=a(7294);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function o(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function i(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?o(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function s(t,e){if(null==t)return{};var a,n,r=function(t,e){if(null==t)return{};var a,n,r={},o=Object.keys(t);for(n=0;n<o.length;n++)a=o[n],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(t);for(n=0;n<o.length;n++)a=o[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var l=n.createContext({}),c=function(t){var e=n.useContext(l),a=e;return t&&(a="function"==typeof t?t(e):i(i({},e),t)),a},m=function(t){var e=c(t.components);return n.createElement(l.Provider,{value:e},t.children)},p="mdxType",u={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},d=n.forwardRef((function(t,e){var a=t.components,r=t.mdxType,o=t.originalType,l=t.parentName,m=s(t,["components","mdxType","originalType","parentName"]),p=c(a),d=r,h=p["".concat(l,".").concat(d)]||p[d]||u[d]||o;return a?n.createElement(h,i(i({ref:e},m),{},{components:a})):n.createElement(h,i({ref:e},m))}));function h(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var o=a.length,i=new Array(o);i[0]=d;var s={};for(var l in e)hasOwnProperty.call(e,l)&&(s[l]=e[l]);s.originalType=t,s[p]="string"==typeof t?t:r,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},5430:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=a(7462),r=(a(7294),a(3905));const o={},i="LakeSoul Automatic Compaction Task Tutorial",s={unversionedId:"Tutorials/auto-compaction-task",id:"Tutorials/auto-compaction-task",title:"LakeSoul Automatic Compaction Task Tutorial",description:"Whether the data is written in batch or streaming tasks, the data is mostly written in merge mode, therefore there are some intermediate redundant data and a large number of small files. In order to reduce the waste of resources caused by such data and improve the efficiency of data reading, the data needs to be compressed.",source:"@site/docs/02-Tutorials/09-auto-compaction-task.md",sourceDirName:"02-Tutorials",slug:"/Tutorials/auto-compaction-task",permalink:"/docs/Tutorials/auto-compaction-task",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/02-Tutorials/09-auto-compaction-task.md",tags:[],version:"current",sidebarPosition:9,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Incremental query function tutorial",permalink:"/docs/Tutorials/incremental-query"},next:{title:"Setup LakeSoul Meta DB",permalink:"/docs/Usage Docs/setup-meta-env"}},l={},c=[{value:"Implementation mechanism",id:"implementation-mechanism",level:2},{value:"Compaction task startup tutorial",id:"compaction-task-startup-tutorial",level:2}],m={toc:c},p="wrapper";function u(t){let{components:e,...a}=t;return(0,r.kt)(p,(0,n.Z)({},m,a,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"lakesoul-automatic-compaction-task-tutorial"},"LakeSoul Automatic Compaction Task Tutorial"),(0,r.kt)("p",null,"Whether the data is written in batch or streaming tasks, the data is mostly written in merge mode, therefore there are some intermediate redundant data and a large number of small files. In order to reduce the waste of resources caused by such data and improve the efficiency of data reading, the data needs to be compressed."),(0,r.kt)("p",null,"If we perform compaction in a writing job (such as a stream job), the main receiver process may be blocked. If we compress each table in a separate job, it will be inconvenient to set it. Therefore, we added the global classification automatic compaction task, which can automatically compress the data according to the database and write partition data."),(0,r.kt)("h2",{id:"implementation-mechanism"},"Implementation mechanism"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Depending on PG's trigger notify listen mechanism, define a trigger function in PLSQL in PG: each time data is written, it can trigger the execution of a defined function, analyze and process the partitions that meet the compaction conditions in the function (for example, there are 10 submissions since the last compaction), and then publish the information;"),(0,r.kt)("li",{parentName:"ul"},"The backend starts a real-time listening task to listen to the information published by PG, and then starts the spark task to compress the data of partitions that meet the compaction conditions.")),(0,r.kt)("p",null,"Currently, compaction is only performed according to the version of the written partition, and the execution of the compaction task will be triggered every 10 submissions."),(0,r.kt)("h2",{id:"compaction-task-startup-tutorial"},"Compaction task startup tutorial"),(0,r.kt)("p",null,"The trigger and pg functions have been configured when the database is initialized, and the default compaction configuration will trigger a compaction signal every time a partition is inserted 10 times, so you only need to start the spark automatic compaction job."),(0,r.kt)("p",null,"Download or decompress the LakeSoul project code, and then put the jar package in the $SPARK_HOME/jars directory, or add the dependent jar package through --jars when submitting the task, and then start the spark automatic compaction task."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Pass ",(0,r.kt)("inlineCode",{parentName:"li"},"lakesoul_home")," environment variable to your job. For detailed documentation, please refer\nto ",(0,r.kt)("a",{parentName:"li",href:"/docs/Getting%20Started/setup-local-env"},"Setup a local test environment")),(0,r.kt)("li",{parentName:"ol"},"Submit task. The currently supported parameters are as follows:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,r.kt)("th",{parentName:"tr",align:null},"Meaning"),(0,r.kt)("th",{parentName:"tr",align:null},"required"),(0,r.kt)("th",{parentName:"tr",align:null},"default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"threadpool.size"),(0,r.kt)("td",{parentName:"tr",align:null},"the thread pools number of automatically compress task"),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null},"8")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"database"),(0,r.kt)("td",{parentName:"tr",align:null},"The database name to compress. If it is not filled, it means that all database partitions will compress that meet the conditions"),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null},'""')))),(0,r.kt)("p",null,"the demo of starting the automatic compaction task command locally:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'export lakesoul_home=pg.properties && ./bin/spark-submit \\\n--name auto_compaction_task \\\n--master local[4]  \\\n--executor-memory 2g \\\n--conf "spark.driver.extraJavaOptions=-XX:MaxDirectMemorySize=8G" \\\n--class com.dmetasoul.lakesoul.spark.compaction.CompactionTask  \\\njars/lakesoul-spark-2.2.0-spark-3.3-SNAPSHOT.jar  --threadpool.size=10 --database=test\n')),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"Because LakeSoul enables native IO by default and needs to rely on off-heap memory, the spark task needs to set the size of off-heap memory, otherwise it is prone to out-of-heap memory overflow.")))}u.isMDXComponent=!0}}]);