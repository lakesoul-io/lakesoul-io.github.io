"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[39],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var o=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},r=Object.keys(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var c=o.createContext({}),s=function(e){var t=o.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=s(e.components);return o.createElement(c.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=s(a),d=n,h=m["".concat(c,".").concat(d)]||m[d]||u[d]||r;return a?o.createElement(h,i(i({ref:t},p),{},{components:a})):o.createElement(h,i({ref:t},p))}));function h(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,i=new Array(r);i[0]=d;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l[m]="string"==typeof e?e:n,i[1]=l;for(var s=2;s<r;s++)i[s]=a[s];return o.createElement.apply(null,i)}return o.createElement.apply(null,a)}d.displayName="MDXCreateElement"},6324:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>s});var o=a(7462),n=(a(7294),a(3905));const r={},i="LakeSoul Global Automatic Compaction Service Usage",l={unversionedId:"Usage Docs/auto-compaction-task",id:"Usage Docs/auto-compaction-task",title:"LakeSoul Global Automatic Compaction Service Usage",description:"Since 2.3.0",source:"@site/docs/03-Usage Docs/08-auto-compaction-task.md",sourceDirName:"03-Usage Docs",slug:"/Usage Docs/auto-compaction-task",permalink:"/docs/Usage Docs/auto-compaction-task",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/03-Usage Docs/08-auto-compaction-task.md",tags:[],version:"current",sidebarPosition:8,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LakeSoul Flink Connector",permalink:"/docs/Usage Docs/flink-lakesoul-connector"}},c={},s=[{value:"Implementation Details",id:"implementation-details",level:2},{value:"Start Compaction Service",id:"start-compaction-service",level:2}],p={toc:s},m="wrapper";function u(e){let{components:t,...a}=e;return(0,n.kt)(m,(0,o.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"lakesoul-global-automatic-compaction-service-usage"},"LakeSoul Global Automatic Compaction Service Usage"),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"Since 2.3.0")),(0,n.kt)("p",null,"When the data is written in batch or streaming tasks, the data is mostly written in small batches, therefore there are some intermediate data and a large number of small files. In order to reduce the waste of resources caused by such data and improve the efficiency of data reading, compaction need to be executed periodically for all tables."),(0,n.kt)("p",null,"If we perform compaction from within a writing job (such as a stream job), the write process may be blocked and latency and throughput maybe impacted. If we start compaction task for each table in a separate job, it will be cumbersome to setup and deploy. Therefore, LakeSoul provides a global automatic compaction service, which can automatically compress the data according to the database and write partition data, and the compaction task can be automatically scaled."),(0,n.kt)("h2",{id:"implementation-details"},"Implementation Details"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Depending on PG's trigger notify listen mechanism, define a trigger function in PLSQL in PG: each time data is written, it can trigger the execution of a defined function, analyze and process the partitions that meet the compaction conditions in the function (for example, there are 10 submissions since the last compaction), and then publish the information;"),(0,n.kt)("li",{parentName:"ul"},"The backend starts a long running Spark job that listens to the event published by PG, and then starts the Spark job to compress the data of partitions that meet the compaction conditions. So that this one Spark job would be responsible for the compaction of all tables automatically.")),(0,n.kt)("p",null,"Currently, compaction is only performed according to the version of the written partition, and the execution of the compaction service will be triggered every 10 commits."),(0,n.kt)("h2",{id:"start-compaction-service"},"Start Compaction Service"),(0,n.kt)("p",null,"The trigger and PLSQL functions have been configured when the database is initialized, and the default compaction configuration will trigger a compaction signal every time a partition is inserted 10 times, so you only need to start the Spark automatic compaction job."),(0,n.kt)("p",null,"Download LakeSoul's Spark release jar file, add the dependent jar package through --jars when submitting the job, and then start the Spark automatic compaction service job."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Setup metadata connection for LakeSoul. For detailed documentation, please refer\nto ",(0,n.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/setup-spark"},"Setup Spark Job")),(0,n.kt)("li",{parentName:"ol"},"Submit the Spark job. The currently supported parameters are as follows:")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,n.kt)("th",{parentName:"tr",align:null},"Meaning"),(0,n.kt)("th",{parentName:"tr",align:null},"required"),(0,n.kt)("th",{parentName:"tr",align:null},"default"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"threadpool.size"),(0,n.kt)("td",{parentName:"tr",align:null},"the thread pools number of automatic compaction task"),(0,n.kt)("td",{parentName:"tr",align:null},"false"),(0,n.kt)("td",{parentName:"tr",align:null},"8")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"database"),(0,n.kt)("td",{parentName:"tr",align:null},"The database name to compress. If it is not filled, it means that all database partitions will compress that meet the conditions"),(0,n.kt)("td",{parentName:"tr",align:null},"false"),(0,n.kt)("td",{parentName:"tr",align:null},'""')))),(0,n.kt)("p",null,"The use the following command to start the compaction service job:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'./bin/spark-submit \\\n    --name auto_compaction_task \\\n    --master yarn  \\\n    --deploy-mode cluster \\\n    --executor-memory 3g \\\n    --executor-cores 1 \\\n    --num-executors 20 \\\n    --conf "spark.executor.extraJavaOptions=-XX:MaxDirectMemorySize=4G" \\\n    --conf "spark.executor.memoryOverhead=3g" \\\n    --class com.dmetasoul.lakesoul.spark.compaction.CompactionTask  \\\n    jars/lakesoul-spark-2.3.0-spark-3.3.jar \n    --threadpool.size=10\n    --database=test\n')),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"Because LakeSoul enables native IO by default and needs to rely on off-heap memory, the spark task needs to set the size of off-heap memory, otherwise it is prone to out-of-heap memory overflow.")),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"Your could enable Spark's dynamic allocation to get auto-scaling for the compaction service job. Refer to Spark's doc ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/3.3.1/job-scheduling.html#dynamic-resource-allocation"},"Dynamic Resource Allocation")," on how to config.")))}u.isMDXComponent=!0}}]);