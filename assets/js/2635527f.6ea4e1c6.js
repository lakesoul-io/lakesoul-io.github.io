"use strict";(self.webpackChunklakesoul_website=self.webpackChunklakesoul_website||[]).push([[8585],{4630:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"Usage Docs/new-async-clean-service","title":"LakeSoul Automatic Asynchronous Data Cleanup Service","description":"This feature is available in version 3.0.0 and above.","source":"@site/docs/03-Usage Docs/16-new-async-clean-service.md","sourceDirName":"03-Usage Docs","slug":"/Usage Docs/new-async-clean-service","permalink":"/docs/Usage Docs/new-async-clean-service","draft":false,"unlisted":false,"editUrl":"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/03-Usage Docs/16-new-async-clean-service.md","tags":[],"version":"current","sidebarPosition":16,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Read and write LakeSoul in Spark Gluten","permalink":"/docs/Usage Docs/spark-gluten/"},"next":{"title":"Deploy HA PostgreSQL Cluster","permalink":"/docs/Deployment/Postgres-Cluster"}}');var a=t(4848),r=t(8453);const i={},l="LakeSoul Automatic Asynchronous Data Cleanup Service",c={},o=[{value:"Run Flink Cleanup Job Locally",id:"run-flink-cleanup-job-locally",level:2},{value:"Parameter Configuration",id:"parameter-configuration",level:2}];function d(e){const n={admonition:"admonition",br:"br",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lakesoul-automatic-asynchronous-data-cleanup-service",children:"LakeSoul Automatic Asynchronous Data Cleanup Service"})}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsx)(n.p,{children:"This feature is available in version 3.0.0 and above."})}),"\n",(0,a.jsx)(n.p,{children:"In data warehouses, it is often necessary to define the lifecycle of table data in order to save storage space and reduce costs."}),"\n",(0,a.jsxs)(n.p,{children:["On the other hand, for real-time updating tables, redundant data may exist. Redundant data refers to the fact that every time a ",(0,a.jsx)(n.strong,{children:"compaction"})," operation is executed, a new compaction file is generated. The new compaction file contains all historical data, and at this point, all previous compaction files can be considered redundant."]}),"\n",(0,a.jsx)(n.p,{children:"Meanwhile, for a table that is continuously updated and compacted, if the user only cares about data changes within a recent time range, they can choose to clean up all data before a specific compaction. This way, one full snapshot of the data will be preserved, while still allowing users to perform incremental reads and snapshot queries from the recent time range onward."}),"\n",(0,a.jsxs)(n.p,{children:["Before version 3.0.0, the automatic cleanup service was scheduled to run daily, scanning all metadata to find expired files and then deleting them. This caused a high instantaneous load on the metadata service.",(0,a.jsx)(n.br,{}),"\n","Starting from version 3.0.0, the cleanup service has been completely reimplemented as an ",(0,a.jsx)(n.strong,{children:"asynchronous, real-time cleanup mechanism"}),". By consuming metadata change events through CDC combined with Flink\u2019s timer mechanism, the new design achieves asynchronous cleanup with higher efficiency and significantly reduces metadata service load."]}),"\n",(0,a.jsx)(n.h2,{id:"run-flink-cleanup-job-locally",children:"Run Flink Cleanup Job Locally"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'./bin/flink run \\\n-c org.apache.flink.lakesoul.entry.clean.NewCleanJob \\\nlakesoul-flink-1.20-3.0.0-SNAPSHOT.jar \\\n--source_db.dbName lakesoul_test \\\n--source_db.user lakesoul_test \\\n--source_db.host localhost \\\n--source_db.port 5432 \\\n--source_db.password lakesoul_test \\\n--slotName flink  \\\n--plugName pgoutput \\\n--url jdbc:postgresql://localhost:5432/lakesoul_test \\\n--ontimer_interval "1" \\\n--dataExpiredTime "5"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"parameter-configuration",children:"Parameter Configuration"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Parameter Name"}),(0,a.jsx)(n.th,{children:"Required"}),(0,a.jsx)(n.th,{children:"Description"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--source_db.dbName"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"PostgreSQL database name"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--source_db.user"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"PostgreSQL username"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--source_db.host"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"PostgreSQL host"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--source_db.port"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"Database port"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--source_db.password"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"Database password"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--slotName"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"Logical replication slot name"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--plugName"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"Logical replication plugin name"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--url"}),(0,a.jsx)(n.td,{children:"yes"}),(0,a.jsx)(n.td,{children:"JDBC URL of the PostgreSQL database"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--ontimer_interval"}),(0,a.jsx)(n.td,{children:"no"}),(0,a.jsx)(n.td,{children:"Timer trigger interval (in minutes), default is 5 minutes"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--dataExpiredTime"}),(0,a.jsx)(n.td,{children:"no"}),(0,a.jsx)(n.td,{children:"Data expiration time (in days), default is 3 days"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--source.parallelism"}),(0,a.jsx)(n.td,{children:"no"}),(0,a.jsx)(n.td,{children:"Source reading parallelism, default is 1"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"--targetTableName"}),(0,a.jsx)(n.td,{children:"no"}),(0,a.jsx)(n.td,{children:"Specify a table for cleanup, e.g., public.testTable. Multiple tables can be separated by commas. If not specified, all tables will be included in cleanup."})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var s=t(6540);const a={},r=s.createContext(a);function i(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);