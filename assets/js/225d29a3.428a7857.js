"use strict";(self.webpackChunklakesoul_website=self.webpackChunklakesoul_website||[]).push([[8947],{8453:(e,a,t)=>{t.d(a,{R:()=>o,x:()=>i});var n=t(6540);const r={},s=n.createContext(r);function o(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),n.createElement(s.Provider,{value:a},e.children)}},9506:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"Tutorials/upsert-and-merge-udf","title":"Upsert Data and Merge UDF Tutorial","description":"\x3c!--","source":"@site/docs/02-Tutorials/05-upsert-and-merge-udf.md","sourceDirName":"02-Tutorials","slug":"/Tutorials/upsert-and-merge-udf","permalink":"/docs/Tutorials/upsert-and-merge-udf","draft":false,"unlisted":false,"editUrl":"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/02-Tutorials/05-upsert-and-merge-udf.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Mount LakeSoul Data to Hive Meta","permalink":"/docs/Tutorials/data-mount-to-hive"},"next":{"title":"Multi Stream Merge to Build Wide Table Tutorial","permalink":"/docs/Tutorials/mutil-stream-merge"}}');var r=t(4848),s=t(8453);const o={},i="Upsert Data and Merge UDF Tutorial",d={},l=[{value:"Customize Merge Logic",id:"customize-merge-logic",level:2}];function c(e){const a={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"upsert-data-and-merge-udf-tutorial",children:"Upsert Data and Merge UDF Tutorial"})}),"\n",(0,r.jsx)(a.p,{children:"LakeSoul can support the function of updating some fields of the data that has entered the lake, without having to overwrite the entire data table, so as to avoid this heavy and resource wasting operation."}),"\n",(0,r.jsx)(a.p,{children:"For example, the data information of a table is as follows. The ID is the primary key (i.e. hashPartitions). At present, it is necessary to check the data of phone according to the primary key field_ Number to modify the field."}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"id"}),(0,r.jsx)(a.th,{children:"name"}),(0,r.jsx)(a.th,{children:"phone_number"}),(0,r.jsx)(a.th,{children:"address"}),(0,r.jsx)(a.th,{children:"job"}),(0,r.jsx)(a.th,{children:"company"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"1"}),(0,r.jsx)(a.td,{children:"Jake"}),(0,r.jsx)(a.td,{children:"13700001111"}),(0,r.jsx)(a.td,{children:"address_1"}),(0,r.jsx)(a.td,{children:"job_1"}),(0,r.jsx)(a.td,{children:"company_2"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"2"}),(0,r.jsx)(a.td,{children:"Make"}),(0,r.jsx)(a.td,{children:"13511110000"}),(0,r.jsx)(a.td,{children:"address_2"}),(0,r.jsx)(a.td,{children:"job_2"}),(0,r.jsx)(a.td,{children:"company_2"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Upsert can be used to update and overwrite existing fields. Upsert operation needs to include the primary key (e.g. id) and other fields (e.g. address) to be modified. Reading the address of the whole table data again can display the modified field information."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'import org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\nimport spark.implicits._\n\nval df = Seq(("1", "Jake", "13700001111", "address_1", "job_1", "company_1"),("2", "Make", "13511110000", "address_2", "job_2", "company_2"))\n  .toDF("id", "name", "phone_number", "address", "job", "company")\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\n\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("hashPartitions","id")\n  .option("hashBucketNum","2")\n  .save(tablePath)\n\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\nval extraDF = Seq(("1", "address_1_1")).toDF("id","address")\nlakeSoulTable.upsert(extraDF)\nlakeSoulTable.toDF.show()\n\n/**\n *  result:\n *  +---+----+------------+-----------+-----+---------+\n *  | id|name|phone_number|    address|  job|  company|\n *  +---+----+------------+-----------+-----+---------+\n *  |  1|Jake| 13700001111|address_1_1|job_1|company_1|\n *  |  2|Make| 13511110000|  address_2|job_2|company_2|\n *  +---+----+------------+-----------+-----+---------+\n */\n'})}),"\n",(0,r.jsx)(a.h2,{id:"customize-merge-logic",children:"Customize Merge Logic"}),"\n",(0,r.jsxs)(a.p,{children:["The essence of the field update supported by LakeSoul is to follow the default merge rule of LakeSoul, that is, after data is upserted, the latest record is taken as the changed field data (see ",(0,r.jsx)(a.code,{children:"org.apache.spark.sql.execution.datasources.v2.merge.request.batch.merge_operator.DefaultMergeOp"}),'). On this basis, LakeSoul has several built-int merge operators, including adding and merging Int/Long fields (MergeOpInt/MergeOpLong), updating no empty fields (MergeNonNullOp), and merging strings with ",".']}),"\n",(0,r.jsx)(a.p,{children:"The following is an example of updating no null fields (MergeNonNullOp), borrowing the above table data sample. When data is written, it is also updated and written in the upsert mode. When reading data, you need to register the merge logic and then read."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'import org.apache.spark.sql.execution.datasources.v2.merge.parquet.batch.merge_operator.MergeNonNullOp\nimport org.apache.spark.sql.functions.expr\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\nimport spark.implicits._\n\nval df = Seq(("1", "Jake", "13700001111", "address_1", "job_1", "company_1"),("2", "Make", "13511110000", "address_2", "job_2", "company_2"))\n  .toDF("id", "name", "phone_number", "address", "job", "company")\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\n\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("hashPartitions","id")\n  .option("hashBucketNum","2")\n  .save(tablePath)\n\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\nval extraDF = Seq(("1", "null", "13100001111", "address_1_1", "job_1_1", "company_1_1"),("2", "null", "13111110000", "address_2_2", "job_2_2", "company_2_2"))\n  .toDF("id", "name", "phone_number", "address", "job", "company")\n\nnew MergeNonNullOp().register(spark, "NotNullOp")\nlakeSoulTable.toDF.show()\nlakeSoulTable.upsert(extraDF)\nlakeSoulTable.toDF.withColumn("name", expr("NotNullOp(name)")).show()\n\n/**\n *  result\n *  +---+----+------------+-----------+-------+-----------+\n *  | id|name|phone_number|    address|    job|    company|\n *  +---+----+------------+-----------+-------+-----------+\n *  |  1|Jake| 13100001111|address_1_1|job_1_1|company_1_1|\n *  |  2|Make| 13111110000|address_2_2|job_2_2|company_2_2|\n *  +---+----+------------+-----------+-------+-----------+\n */\n'})}),"\n",(0,r.jsxs)(a.p,{children:["You could also define your own merge logic via implementing the trait ",(0,r.jsx)(a.code,{children:"org.apache.spark.sql.execution.datasources.v2.merge.parquet.batch.merge_operator.MergeOperator"})," to achieve efficient data update."]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);