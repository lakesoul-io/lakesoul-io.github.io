"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[430],{3905:(e,a,r)=>{r.d(a,{Zo:()=>u,kt:()=>m});var t=r(7294);function o(e,a,r){return a in e?Object.defineProperty(e,a,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[a]=r,e}function n(e,a){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),r.push.apply(r,t)}return r}function s(e){for(var a=1;a<arguments.length;a++){var r=null!=arguments[a]?arguments[a]:{};a%2?n(Object(r),!0).forEach((function(a){o(e,a,r[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):n(Object(r)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(r,a))}))}return e}function l(e,a){if(null==e)return{};var r,t,o=function(e,a){if(null==e)return{};var r,t,o={},n=Object.keys(e);for(t=0;t<n.length;t++)r=n[t],a.indexOf(r)>=0||(o[r]=e[r]);return o}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(t=0;t<n.length;t++)r=n[t],a.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var p=t.createContext({}),i=function(e){var a=t.useContext(p),r=a;return e&&(r="function"==typeof e?e(a):s(s({},a),e)),r},u=function(e){var a=i(e.components);return t.createElement(p.Provider,{value:a},e.children)},c="mdxType",k={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},d=t.forwardRef((function(e,a){var r=e.components,o=e.mdxType,n=e.originalType,p=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=i(r),d=o,m=c["".concat(p,".").concat(d)]||c[d]||k[d]||n;return r?t.createElement(m,s(s({ref:a},u),{},{components:r})):t.createElement(m,s({ref:a},u))}));function m(e,a){var r=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var n=r.length,s=new Array(n);s[0]=d;var l={};for(var p in a)hasOwnProperty.call(a,p)&&(l[p]=a[p]);l.originalType=e,l[c]="string"==typeof e?e:o,s[1]=l;for(var i=2;i<n;i++)s[i]=r[i];return t.createElement.apply(null,s)}return t.createElement.apply(null,r)}d.displayName="MDXCreateElement"},6262:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>p,contentTitle:()=>s,default:()=>k,frontMatter:()=>n,metadata:()=>l,toc:()=>i});var t=r(7462),o=(r(7294),r(3905));const n={},s="Setup Your Spark Project",l={unversionedId:"Usage Docs/setup-spark",id:"Usage Docs/setup-spark",title:"Setup Your Spark Project",description:"Required Spark Version",source:"@site/docs/03-Usage Docs/02-setup-spark.md",sourceDirName:"03-Usage Docs",slug:"/Usage Docs/setup-spark",permalink:"/docs/Usage Docs/setup-spark",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/03-Usage Docs/02-setup-spark.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Setup LakeSoul Meta DB",permalink:"/docs/Usage Docs/setup-meta-env"},next:{title:"Basic API Docs",permalink:"/docs/Usage Docs/api-docs"}},p={},i=[{value:"Required Spark Version",id:"required-spark-version",level:2},{value:"Setup (Py)Spark Shell or Spark SQL Shell",id:"setup-pyspark-shell-or-spark-sql-shell",level:2},{value:"Use Maven Coordinates via --packages",id:"use-maven-coordinates-via---packages",level:3},{value:"Use Local Packages",id:"use-local-packages",level:3},{value:"Setup Java/Scala Project",id:"setup-javascala-project",level:2},{value:"Pass <code>lakesoul_home</code> Environment Variable to Your Job",id:"pass-lakesoul_home-environment-variable-to-your-job",level:2}],u={toc:i},c="wrapper";function k(e){let{components:a,...r}=e;return(0,o.kt)(c,(0,t.Z)({},u,r,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"setup-your-spark-project"},"Setup Your Spark Project"),(0,o.kt)("h2",{id:"required-spark-version"},"Required Spark Version"),(0,o.kt)("p",null,"LakeSoul is currently available with Scala version 2.12 and Spark version 3.3."),(0,o.kt)("h2",{id:"setup-pyspark-shell-or-spark-sql-shell"},"Setup (Py)Spark Shell or Spark SQL Shell"),(0,o.kt)("p",null,"To use ",(0,o.kt)("inlineCode",{parentName:"p"},"spark-shell"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"pyspark")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"spark-sql")," shells, you should include LakeSoul's dependencies. There are two approaches to achieve this."),(0,o.kt)("h3",{id:"use-maven-coordinates-via---packages"},"Use Maven Coordinates via --packages"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"spark-shell --packages com.dmetasoul:lakesoul-spark:2.2.0-spark-3.3\n")),(0,o.kt)("h3",{id:"use-local-packages"},"Use Local Packages"),(0,o.kt)("p",null,"You can find the LakeSoul packages from our release page: ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/meta-soul/LakeSoul/releases"},"Releases"),".\nDownload the jar file and pass it to ",(0,o.kt)("inlineCode",{parentName:"p"},"spark-submit"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'spark-submit --jars "lakesoul-spark-2.2.0-spark-3.3.jar"\n')),(0,o.kt)("p",null,"Or you could directly put the jar into ",(0,o.kt)("inlineCode",{parentName:"p"},"$SPARK_HOME/jars")),(0,o.kt)("h2",{id:"setup-javascala-project"},"Setup Java/Scala Project"),(0,o.kt)("p",null,"Include maven dependencies in your project:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>com.dmetasoul</groupId>\n    <artifactId>lakesoul</artifactId>\n    <version>2.2.0-spark-3.3</version>\n</dependency>\n")),(0,o.kt)("h2",{id:"pass-lakesoul_home-environment-variable-to-your-job"},"Pass ",(0,o.kt)("inlineCode",{parentName:"h2"},"lakesoul_home")," Environment Variable to Your Job"),(0,o.kt)("p",null,"If you are using Spark's local or client mode, you could just export the env var in your shell:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"export lakesoul_home=/path/to/lakesoul.properties\n")),(0,o.kt)("p",null,"If you are using Spark's cluster mode, in which the driver would also be scheduled into Yarn or K8s cluster, you can setup the driver's env:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"For Hadoop Yarn, pass ",(0,o.kt)("inlineCode",{parentName:"li"},"--conf spark.yarn.appMasterEnv.lakesoul_home=lakesoul.properties --files /path/to/lakesoul.properties")," to ",(0,o.kt)("inlineCode",{parentName:"li"},"spark-submit")," command;"),(0,o.kt)("li",{parentName:"ul"},"For K8s, pass ",(0,o.kt)("inlineCode",{parentName:"li"},"--conf spark.kubernetes.driverEnv.lakesoul_home=lakesoul.properties --files /path/to/lakesoul.properties")," to ",(0,o.kt)("inlineCode",{parentName:"li"},"spark-submit")," command.")))}k.isMDXComponent=!0}}]);