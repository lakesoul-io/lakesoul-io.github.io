"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[205],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var o=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},r=Object.keys(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)a=r[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var p=o.createContext({}),i=function(e){var t=o.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},u=function(e){var t=i(e.components);return o.createElement(p.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},k=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,p=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),c=i(a),k=n,m=c["".concat(p,".").concat(k)]||c[k]||d[k]||r;return a?o.createElement(m,l(l({ref:t},u),{},{components:a})):o.createElement(m,l({ref:t},u))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,l=new Array(r);l[0]=k;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[c]="string"==typeof e?e:n,l[1]=s;for(var i=2;i<r;i++)l[i]=a[i];return o.createElement.apply(null,l)}return o.createElement.apply(null,a)}k.displayName="MDXCreateElement"},4213:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>i});var o=a(7462),n=(a(7294),a(3905));const r={},l="Setup a Local Environment",s={unversionedId:"Getting Started/setup-local-env",id:"Getting Started/setup-local-env",title:"Setup a Local Environment",description:"\x3c!--",source:"@site/docs/01-Getting Started/01-setup-local-env.md",sourceDirName:"01-Getting Started",slug:"/Getting Started/setup-local-env",permalink:"/docs/Getting Started/setup-local-env",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/01-setup-local-env.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LakeSoul General Concepts Introduction",permalink:"/docs/Getting Started/concepts"},next:{title:"Use Docker Compose",permalink:"/docs/Getting Started/docker-compose"}},p={},i=[{value:"Start A Local PostgreSQL DB",id:"start-a-local-postgresql-db",level:2},{value:"PG Database Initialization",id:"pg-database-initialization",level:2},{value:"Lakesoul PG Database Configuration Description:",id:"lakesoul-pg-database-configuration-description",level:2},{value:"Install an Apache Spark environment",id:"install-an-apache-spark-environment",level:2},{value:"Start spark-shell for testing LakeSoul",id:"start-spark-shell-for-testing-lakesoul",level:2},{value:"LakeSoul Spark Conf Parameters",id:"lakesoul-spark-conf-parameters",level:2}],u={toc:i},c="wrapper";function d(e){let{components:t,...a}=e;return(0,n.kt)(c,(0,o.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"setup-a-local-environment"},"Setup a Local Environment"),(0,n.kt)("h2",{id:"start-a-local-postgresql-db"},"Start A Local PostgreSQL DB"),(0,n.kt)("p",null,"The quickest way to start a pg DB is via docker container:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"docker run -d --name lakesoul-test-pg -p5432:5432 -e POSTGRES_USER=lakesoul_test -e POSTGRES_PASSWORD=lakesoul_test -e POSTGRES_DB=lakesoul_test -d postgres:14.5\n")),(0,n.kt)("h2",{id:"pg-database-initialization"},"PG Database Initialization"),(0,n.kt)("p",null,"Init PG database of LakeSoul using ",(0,n.kt)("inlineCode",{parentName:"p"},"script/meta_init.cql"),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"PGPASSWORD=lakesoul_test psql -h localhost -p 5432 -U lakesoul_test -f script/meta_init.sql\n")),(0,n.kt)("h2",{id:"lakesoul-pg-database-configuration-description"},"Lakesoul PG Database Configuration Description:"),(0,n.kt)("p",null,"By default, the PG database is connected to the local database. The configuration information is as follows,"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-txt"},"lakesoul.pg.driver=com.lakesoul.shaded.org.postgresql.Driver\nlakesoul.pg.url=jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\nlakesoul.pg.username=lakesoul_test\nlakesoul.pg.password=lakesoul_test\n")),(0,n.kt)("p",null,"LakeSoul supports to customize PG database configuration information. Add an environment variable ",(0,n.kt)("inlineCode",{parentName:"p"},"lakesoul_home")," before starting the Spark program to include the configuration file information."),(0,n.kt)("p",null,"For example, the PG database configuration information file path name is: ",(0,n.kt)("inlineCode",{parentName:"p"},"/opt/soft/pg.property"),", you need to add this environment variable before the program starts:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"export lakesoul_home=/opt/soft/pg.property\n")),(0,n.kt)("p",null,"You can put customized database configuration information in this file."),(0,n.kt)("h2",{id:"install-an-apache-spark-environment"},"Install an Apache Spark environment"),(0,n.kt)("p",null,"You could download spark distribution from ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/downloads.html"},"https://spark.apache.org/downloads.html"),", and please choose spark version 3.3.0 or above. Note that the official package from Apache Spark does not include hadoop-cloud component. We provide a Spark package with Hadoop cloud dependencies, download it from ",(0,n.kt)("a",{parentName:"p",href:"https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop3.tgz"},"https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop3.tgz"),"."),(0,n.kt)("p",null,"After unpacking spark package, you could find LakeSoul distribution jar from ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/lakesoul-io/LakeSoul/releases"},"https://github.com/lakesoul-io/LakeSoul/releases"),". Download the jar file put it into ",(0,n.kt)("inlineCode",{parentName:"p"},"jars")," directory of your spark environment."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop-3.tgz\ntar xf spark-3.3.2-bin-hadoop-3.tgz\nexport SPARK_HOME=${PWD}/spark-3.3.2-bin-hadoop3\nwget https://github.com/lakesoul-io/LakeSoul/releases/download/v2.5.0/lakesoul-spark-2.5.0-spark-3.3.jar -P $SPARK_HOME/jars\n")),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"For production deployment on Hadoop, it's recommended to use spark release without bundled hadoop:"),(0,n.kt)("p",{parentName:"admonition"},(0,n.kt)("a",{parentName:"p",href:"https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-without-hadoop.tgz"},"https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-without-hadoop.tgz")),(0,n.kt)("p",{parentName:"admonition"},"Refer to ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/hadoop-provided.html"},"https://spark.apache.org/docs/latest/hadoop-provided.html")," on how to setup hadoop classpath.")),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"Since 2.1.0, LakeSoul package all its dependencies into one single jar via maven shade plugin. Before that all jars were packaged into one tar.gz file.")),(0,n.kt)("h2",{id:"start-spark-shell-for-testing-lakesoul"},"Start spark-shell for testing LakeSoul"),(0,n.kt)("p",null,"cd into the spark installation directory, and start an interactive spark-shell:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul\n")),(0,n.kt)("h2",{id:"lakesoul-spark-conf-parameters"},"LakeSoul Spark Conf Parameters"),(0,n.kt)("p",null,"Before start to use Lakesoul, we should add some paramethers in ",(0,n.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf")," or ",(0,n.kt)("inlineCode",{parentName:"p"},"Spark Session Builder"),"\u3002"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Key"),(0,n.kt)("th",{parentName:"tr",align:null},"Value"),(0,n.kt)("th",{parentName:"tr",align:null},"Description"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"spark.sql.extensions"),(0,n.kt)("td",{parentName:"tr",align:null},"com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension"),(0,n.kt)("td",{parentName:"tr",align:null},"extention name for spark sql")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"spark.sql.catalog.lakesoul"),(0,n.kt)("td",{parentName:"tr",align:null},"org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog"),(0,n.kt)("td",{parentName:"tr",align:null},"plug in LakeSoul's catalog")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"spark.sql.defaultCatalog"),(0,n.kt)("td",{parentName:"tr",align:null},"lakesoul"),(0,n.kt)("td",{parentName:"tr",align:null},"set default catalog for spark")))))}d.isMDXComponent=!0}}]);