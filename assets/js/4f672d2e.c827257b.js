"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[526],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>f});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var c=n.createContext({}),p=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(c.Provider,{value:t},e.children)},s="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,c=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),s=p(a),m=o,f=s["".concat(c,".").concat(m)]||s[m]||u[m]||i;return a?n.createElement(f,l(l({ref:t},d),{},{components:a})):n.createElement(f,l({ref:t},d))}));function f(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,l=new Array(i);l[0]=m;var r={};for(var c in t)hasOwnProperty.call(t,c)&&(r[c]=t[c]);r.originalType=e,r[s]="string"==typeof e?e:o,l[1]=r;for(var p=2;p<i;p++)l[p]=a[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},9287:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>p});var n=a(7462),o=(a(7294),a(3905));const i={},l="LakeSoul Table Lifecycle Automatic Maintenance and Redundant Data Automatic Cleaning",r={unversionedId:"Usage Docs/clean-redundant-data",id:"Usage Docs/clean-redundant-data",title:"LakeSoul Table Lifecycle Automatic Maintenance and Redundant Data Automatic Cleaning",description:"This feature is available starting from version 2.4.0",source:"@site/docs/03-Usage Docs/09-clean-redundant-data.md",sourceDirName:"03-Usage Docs",slug:"/Usage Docs/clean-redundant-data",permalink:"/docs/Usage Docs/clean-redundant-data",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/03-Usage Docs/09-clean-redundant-data.md",tags:[],version:"current",sidebarPosition:9,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LakeSoul Global Automatic Compaction Service Usage",permalink:"/docs/Usage Docs/auto-compaction-task"},next:{title:"Use Presto to Query LakeSoul's Table",permalink:"/docs/Usage Docs/setup-presto"}},c={},p=[{value:"Manually Clean Up Old Compaction Data",id:"manually-clean-up-old-compaction-data",level:2},{value:"Automatically Clean Up Expired Data and Redundant Data",id:"automatically-clean-up-expired-data-and-redundant-data",level:2},{value:"Configure Partition Lifecycle via <code>partition.ttl</code> table property",id:"configure-partition-lifecycle-via-partitionttl-table-property",level:3},{value:"Configure Lifecycle of Redundant Data via <code>compaction.ttl</code> table property",id:"configure-lifecycle-of-redundant-data-via-compactionttl-table-property",level:3},{value:"Configuration Examples",id:"configuration-examples",level:3},{value:"Specify configuration when writing through data",id:"specify-configuration-when-writing-through-data",level:4},{value:"Add or modify the configuration through API",id:"add-or-modify-the-configuration-through-api",level:3},{value:"Execute a job that automatically cleans expired data in all tables",id:"execute-a-job-that-automatically-cleans-expired-data-in-all-tables",level:3}],d={toc:p},s="wrapper";function u(e){let{components:t,...a}=e;return(0,o.kt)(s,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"lakesoul-table-lifecycle-automatic-maintenance-and-redundant-data-automatic-cleaning"},"LakeSoul Table Lifecycle Automatic Maintenance and Redundant Data Automatic Cleaning"),(0,o.kt)("admonition",{type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"This feature is available starting from version 2.4.0")),(0,o.kt)("p",null,"In a data warehouse, it is usually necessary to set the life cycle of table data to save space and reduce costs."),(0,o.kt)("p",null,"On the other hand, for tables that are updated in real time, there will also be redundant data. Redundant data means that every time a compaction operation is performed, a new compaction file will be generated. The new compaction file contains all historical data. At this time, all historical compaction files can be regarded as redundant data."),(0,o.kt)("p",null,"At the same time, for a table data that is continuously updated and compaction operated. If the user only cares about the data changes at a certain time recently. At this time, users can choose to clean up all data before a certain compaction, which will retain a full copy of the data and support users' incremental and snapshot reads from a recent time."),(0,o.kt)("h2",{id:"manually-clean-up-old-compaction-data"},"Manually Clean Up Old Compaction Data"),(0,o.kt)("p",null,"When performing a compactition operation, users can turn on the switch cleanOldCompaction=true to clean up old compaction file data. Default is false."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"     LakeSoulTable.forPath(tablePath).compaction(true)\n")),(0,o.kt)("h2",{id:"automatically-clean-up-expired-data-and-redundant-data"},"Automatically Clean Up Expired Data and Redundant Data"),(0,o.kt)("p",null,"Users can configure the following two table properties:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("inlineCode",{parentName:"li"},"partition.ttl")," represents the partition expiration time, unit is days."),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("inlineCode",{parentName:"li"},"compaction.ttl")," represents the expiration time of redundant data, unit is days.")),(0,o.kt)("h3",{id:"configure-partition-lifecycle-via-partitionttl-table-property"},"Configure Partition Lifecycle via ",(0,o.kt)("inlineCode",{parentName:"h3"},"partition.ttl")," table property"),(0,o.kt)("p",null,"As for the expiration time of table partitions, assuming that the user configures it to 365 days, then in the table, if the latest commit record of a partition has expired, the partition data will be deleted. In particular, if all partitions of the table are expired, it is equivalent to executing the truncate function."),(0,o.kt)("h3",{id:"configure-lifecycle-of-redundant-data-via-compactionttl-table-property"},"Configure Lifecycle of Redundant Data via ",(0,o.kt)("inlineCode",{parentName:"h3"},"compaction.ttl")," table property"),(0,o.kt)("p",null,"Clean up redundant data in partitions. Assume that the redundant cleanup lasts for 3 days, then find the latest compaction 3 days ago and delete the data before it. The purpose is that snapshot reads and incremental reads within 3 days are all valid."),(0,o.kt)("h3",{id:"configuration-examples"},"Configuration Examples"),(0,o.kt)("p",null,"Users can set ",(0,o.kt)("inlineCode",{parentName:"p"},"partition.ttl")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"compaction.ttl")," in the following ways."),(0,o.kt)("h4",{id:"specify-configuration-when-writing-through-data"},"Specify configuration when writing through data"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'     val df = Seq(("2021-01-01", 1, "rice"), ("2021-01-01", 2, "bread")).toDF("date", "id", "name ")\n     val tablePath = "s3a://bucket-name/table/path/is/also/table/name"\n     df.write\n       .mode("append")\n       .format("lakesoul")\n       .option("rangePartitions", "date")\n       .option("hashPartitions", "id")\n       .option("partition.ttl",365)\n       .option("compaction.ttl",3)\n       .option("hashBucketNum", "2")\n       .save(tablePath)\n')),(0,o.kt)("p",null,"You can also add property configuration in ",(0,o.kt)("inlineCode",{parentName:"p"},"TBLPROPERTIES")," in the ",(0,o.kt)("inlineCode",{parentName:"p"},"CREATE TABLE")," SQL statement (in Flink, property configuration is added after ",(0,o.kt)("inlineCode",{parentName:"p"},"WITH"),"). Examples:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"-- For Spark\nCREATE TABLE table (id INT, data STRING) USING lakesoul\n    TBLPROPERTIES ('partition.ttl'='365', 'compaction.ttl'='7')\n\n-- For Flink\ncreate table `lakesoul`.`default`.test_table (`id` INT, data STRING,\n                                              PRIMARY KEY (`id`,`name`) NOT ENFORCED)\nWITH (\n    'connector'='lakesoul',\n    'hashBucketNum'='4',\n    'use_cdc'='true',\n    'partition.ttl'='365',\n    'compaction.ttl'='7',\n    'path'='file:///tmp/lakesoul/flink/sink/test');\n")),(0,o.kt)("h3",{id:"add-or-modify-the-configuration-through-api"},"Add or modify the configuration through API"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"     LakeSoulTable.forPath(tablePath).setPartitionTtl(128).setCompactionTtl(10)\n")),(0,o.kt)("p",null,"At the same time, the configuration can be canceled through tableAPI"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"     LakeSoulTable.forPath(tablePath).cancelPartitionTtl()\n     LakeSoulTable.forPath(tablePath).cancelCompactionTtl()\n")),(0,o.kt)("p",null,"Configure the LakeSoul metabase connection for Spark jobs. For detailed instructions, please refer to ",(0,o.kt)("a",{parentName:"p",href:"/docs/Usage%20Docs/setup-spark"},"LakeSoul Setup Spark Project/Job"),";"),(0,o.kt)("h3",{id:"execute-a-job-that-automatically-cleans-expired-data-in-all-tables"},"Execute a job that automatically cleans expired data in all tables"),(0,o.kt)("p",null,"LakeSoul provides a Spark job implementation for cleaning expired data. It will scan all expired partitions in the metadata and perform cleaning. Users can schedule this task regularly at the daily level to achieve the purpose of cleaning."),(0,o.kt)("p",null,"Start the Spark cleanup command locally:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/spark-submit \\\n     --name clean_redundant_data \\\n     --master yarn \\\n     --deploy-mode cluster \\\n     --executor-memory 3g \\\n     --executor-cores 1 \\\n     --num-executors 20 \\\n     --class com.dmetasoul.lakesoul.spark.clean.CleanExpiredData \\\n     jars/lakesoul-spark-2.4.0-spark-3.3.jar\n\n")),(0,o.kt)("admonition",{type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"The above cleaning tasks are effective for all LakeSoul tables.")))}u.isMDXComponent=!0}}]);