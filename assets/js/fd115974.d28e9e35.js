"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[707],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>h});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),c=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},p=function(e){var a=c(e.components);return n.createElement(s.Provider,{value:a},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),m=r,h=u["".concat(s,".").concat(m)]||u[m]||d[m]||o;return t?n.createElement(h,i(i({ref:a},p),{},{components:t})):n.createElement(h,i({ref:a},p))}));function h(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=m;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l[u]="string"==typeof e?e:r,i[1]=l;for(var c=2;c<o;c++)i[c]=t[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},1299:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var n=t(7462),r=(t(7294),t(3905));const o={},i="LakeSoul General Concepts Introduction",l={unversionedId:"Getting Started/concepts",id:"Getting Started/concepts",title:"LakeSoul General Concepts Introduction",description:"LakeSoul is an end-to-end real-time lake warehouse storage framework that uses an open architecture design to achieve high-performance reading and writing (Upsert and Merge on Read) through the NativeIO layer, and uniformly supports multiple computing engines, including batch processing (Spark) , stream processing (Flink), MPP (Presto), AI (PyTorch, Pandas, Ray), which can be deployed in Hadoop clusters and Kubernetes clusters. The overall architecture of LakeSoul is shown in the figure below:",source:"@site/docs/01-Getting Started/00-concepts.md",sourceDirName:"01-Getting Started",slug:"/Getting Started/concepts",permalink:"/docs/Getting Started/concepts",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/00-concepts.md",tags:[],version:"current",sidebarPosition:0,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LakeSoul Introduction",permalink:"/docs/intro"},next:{title:"Setup a Local Environment",permalink:"/docs/Getting Started/setup-local-env"}},s={},c=[{value:"LakeSoul Architecture",id:"lakesoul-architecture",level:2},{value:"Core Features of LakeSoul",id:"core-features-of-lakesoul",level:2},{value:"Core Concepts of LakeSoul",id:"core-concepts-of-lakesoul",level:2},{value:"No Primary Key Table",id:"no-primary-key-table",level:3},{value:"Primary key table",id:"primary-key-table",level:3},{value:"Snapshot read",id:"snapshot-read",level:3}],p={toc:c},u="wrapper";function d(e){let{components:a,...o}=e;return(0,r.kt)(u,(0,n.Z)({},p,o,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"lakesoul-general-concepts-introduction"},"LakeSoul General Concepts Introduction"),(0,r.kt)("p",null,"LakeSoul is an end-to-end real-time lake warehouse storage framework that uses an open architecture design to achieve high-performance reading and writing (Upsert and Merge on Read) through the NativeIO layer, and uniformly supports multiple computing engines, including batch processing (Spark) , stream processing (Flink), MPP (Presto), AI (PyTorch, Pandas, Ray), which can be deployed in Hadoop clusters and Kubernetes clusters. The overall architecture of LakeSoul is shown in the figure below:\n",(0,r.kt)("img",{alt:"arch",src:t(2694).Z,width:"2654",height:"1320"})),(0,r.kt)("h2",{id:"lakesoul-architecture"},"LakeSoul Architecture"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Metadata management layer. Centralized metadata management, write concurrency atomicity and consistency control, read version isolation (MVCC), etc. are implemented through PostgreSQL. Through the powerful transaction capabilities of PostgreSQL, automatic concurrent write conflict resolution (Write conflict reconsiliation) and end-to-end strictly once (Exactly Once) guarantee are realized. Through PostgreSQL's ",(0,r.kt)("a",{parentName:"p",href:"https://www.postgresql.org/docs/current/sql-notify.html"},"Trigger-Notify"),' event listening mechanism, functions such as automatic detached elastic compaction and expired data cleaning are realized, thereby achieving "autonomy" "style" management capabilities.'),(0,r.kt)("p",{parentName:"li"},"In addition, centralized metadata management can also easily realize unified management of multiple sets of storage in one lake warehouse instance. For example, data on multiple S3 buckets and multiple HDFS clusters can all be managed in one lake warehouse metadata. .")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},'NativeIO layer. Vectorized Parquet file reading and writing implemented in Rust. For real-time update scenarios, a "single-layer" LSM-Tree-like method is used. The primary key table is fragmented according to the primary key hash, sorted and written, and automatically merged during reading, thus realizing the Upsert function. The NativeIO layer has made a lot of performance optimizations for object storage and MOR scenarios, which has greatly improved the IO performance of Hucang. The NativeIO layer also implements column clipping, Filter pushdown and other functions.'),(0,r.kt)("p",{parentName:"li"},"The NativeIO layer uses the native IO library implemented by Rust, and uniformly encapsulates Java and Python interfaces, allowing LakeSoul to easily implement native connectors with various big data and AI frameworks, eliminating the need for conversion overhead in file formats or memory, and improving execution efficiency."),(0,r.kt)("p",{parentName:"li"},"The NativeIO layer supports common storage services such as HDFS, S3, OSS, OBS, and MinIO.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Engine docking layer. We are also continuously integrating Catalog and DataSource that implement frameworks such as Spark, Flink, Presto, PyTorch, Pandas, and Ray. Seamless connection between multiple engines, especially big data processing and AI computing. Therefore, LakeSoul is very suitable as the lake warehouse data base of the Data+AI integrated architecture."))),(0,r.kt)("h2",{id:"core-features-of-lakesoul"},"Core Features of LakeSoul"),(0,r.kt)("p",null,"LakeSoul's goal is to build an end-to-end lake warehouse platform covering data integration, real-time/batch data ETL calculations and AI calculations. The main core function points include:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/flink-cdc-sync"},"Real-time data integration"),". LakeSoul implements the entire database synchronization function based on Flink CDC, and currently supports MySQL, PostgreSQL, PolarDB, Oracle and other databases. For all data sources, full database synchronization, automatic new table discovery synchronization, and automatic Schema change synchronization (supporting column addition and column subtraction) are supported."),(0,r.kt)("li",{parentName:"ol"},"Streaming batch calculation. LakeSoul supports frameworks such as Spark and Flink for stream and batch ETL calculations. The primary key table supports ChangeLog (CDC) reading in Flink, thereby realizing ",(0,r.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/flink-lakesoul-connector"},"streaming incremental calculation"),"."),(0,r.kt)("li",{parentName:"ol"},"Data analysis and query. LakeSoul improves the performance of data analysis queries through a high-performance IO layer. It can also support various vectorized calculation engines. Currently, LakeSoul has implemented docking with Spark Gluten Engine to implement native vectorized calculations on Spark. LakeSoul is also further integrating with high-performance vectorized query engines such as Apache Doris, Clickhouse, and Presto Velox."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/machine-learning-support"},"AI Computing"),". LakeSoul can support distributed reading of various AI and data science frameworks such as PyTorch, Ray, and Pandas for training and inference of AI models."),(0,r.kt)("li",{parentName:"ol"},"Multi-tenant space and RBAC. LakeSoul has built-in ",(0,r.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/workspace-and-rbac"},"multi-space isolation and permission control"),". Multiple workspaces can be divided into the lake warehouse, and multiple users can be added to each workspace. Metadata and physical data in different spaces achieve isolation of access permissions. Space permission isolation is effective for SQL, Java/Scala, and Python jobs, including jobs submitted to the cluster for execution."),(0,r.kt)("li",{parentName:"ol"},"Autonomous management. LakeSoul provides ",(0,r.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/auto-compaction-task"},"automatically separated elastic compaction service"),", ",(0,r.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/clean-redundant-data"},"automatic data cleaning service, etc.")," to reduce the operation and maintenance workload. The detached elastic compaction service is automatically triggered by the metadata layer and executed in parallel without affecting the efficiency of writing tasks."),(0,r.kt)("li",{parentName:"ol"},"Snapshots and rollbacks. LakeSoul tables can support ",(0,r.kt)("a",{parentName:"li",href:"/docs/Tutorials/snapshot-manage"},"snapshot reading and version rollback")," based on timestamps."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/Usage%20Docs/export-to-databases"},"Export to other databases"),". LakeSoul provides utilities to export LakeSoul table to other databases in batch or streaming mode.")),(0,r.kt)("h2",{id:"core-concepts-of-lakesoul"},"Core Concepts of LakeSoul"),(0,r.kt)("p",null,"In LakeSoul, data is organized into tables. The table supports multi-level range partitioning and supports specifying primary keys."),(0,r.kt)("p",null,"The partition table adopts the same directory organization structure as Hive and supports dynamic partition writing. The primary key table is stored using hash bucket sorting, and the primary key can be composed of one or more columns."),(0,r.kt)("p",null,"When creating a table, you can use Spark Dataframe, Spark SQL, Flink SQL, etc. When creating a table, you can directly use the ",(0,r.kt)("inlineCode",{parentName:"p"},"partition by")," clause to specify the partition column, use the table attribute ",(0,r.kt)("inlineCode",{parentName:"p"},"hashPartitions")," to specify the primary key column, and use the table attribute ",(0,r.kt)("inlineCode",{parentName:"p"},"hashBucketNum")," to specify the number of hash shards."),(0,r.kt)("p",null,"All data types native to Spark and Flink can be used when creating tables. Data types are translated into Apache Arrow types and ultimately into Parquet file types. Types support multi-level nesting (Struct type)."),(0,r.kt)("h3",{id:"no-primary-key-table"},"No Primary Key Table"),(0,r.kt)("p",null,"No primary key tables can support multi-level partitioning, but do not have primary keys. Non-primary key tables can only be appended in Append mode. Suitable for scenarios where there is no clear primary key, such as log data, etc. Append writes to non-primary key tables can be executed concurrently, that is, multiple batch and streaming jobs can write concurrently to the table/partition."),(0,r.kt)("p",null,"In Spark and Flink, you can use the DataFrame and DataStream API to write, and you can use the SQL ",(0,r.kt)("inlineCode",{parentName:"p"},"insert into")," statement to write to non-primary key tables. Non-primary key tables can support streaming reading in Spark and Flink. Support batch reading in Presto and AI framework."),(0,r.kt)("h3",{id:"primary-key-table"},"Primary key table"),(0,r.kt)("p",null,"A primary key table can specify multiple levels of partitioning, as well as one or more primary key columns. Primary key columns cannot be null. For the same primary key, the latest value will be automatically merged when reading."),(0,r.kt)("p",null,"The primary key table can only support Upsert mode for updating. Updates can be made using the Spark API or the ",(0,r.kt)("inlineCode",{parentName:"p"},"MERGE INTO")," SQL statement. ",(0,r.kt)("inlineCode",{parentName:"p"},"INSERT INTO")," SQL in Flink will automatically be executed in the Upsert mode."),(0,r.kt)("p",null,"The primary key table can support concurrent write updates and partial column updates (Partial Update). During concurrent updates, the application layer needs to ensure that the writing order does not conflict. Generally speaking, if there are multiple streams updating different columns of the primary key table at the same time, you can use concurrent updates to write, so that data conflicts will not occur and the degree of concurrency will be improved."),(0,r.kt)("p",null,"The primary key table can also support ",(0,r.kt)("a",{parentName:"p",href:"/docs/Usage%20Docs/cdc-ingestion-table"},"CDC format"),". For tables synchronized from the database, CDC format needs to be enabled. At the same time, the CDC format will automatically support ChangeLog semantics during Flink stream reading, thus enabling streaming incremental calculations in Flink."),(0,r.kt)("p",null,"Primary key tables also support batch reading in Presto and AI frameworks."),(0,r.kt)("h3",{id:"snapshot-read"},"Snapshot read"),(0,r.kt)("p",null,"LakeSoul supports snapshot reading (Time Travel) and version rollback in Spark and Flink. Reference documents: ",(0,r.kt)("a",{parentName:"p",href:"https://lakesoul-io.github.io/docs/Tutorials/snapshot-manage"},"Spark snapshot function"),", ",(0,r.kt)("a",{parentName:"p",href:"https://lakesoul-io.github.io/docs/Usage%20Docs/flink-lakesoul-connector#42-snapshot-batch-read"},"Flink snapshot reading"),"."))}d.isMDXComponent=!0},2694:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/lakeSoulModel-2d7311a96b7f5c3728a4f7aa3ffa6aa0.png"}}]);