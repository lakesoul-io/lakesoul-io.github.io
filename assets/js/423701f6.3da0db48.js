"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[394],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>k});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function s(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?s(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},s=Object.keys(e);for(n=0;n<s.length;n++)t=s[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)t=s[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var i=n.createContext({}),c=function(e){var a=n.useContext(i),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},p=function(e){var a=c(e.components);return n.createElement(i.Provider,{value:a},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,s=e.originalType,i=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),u=c(t),m=r,k=u["".concat(i,".").concat(m)]||u[m]||d[m]||s;return t?n.createElement(k,l(l({ref:a},p),{},{components:t})):n.createElement(k,l({ref:a},p))}));function k(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var s=t.length,l=new Array(s);l[0]=m;var o={};for(var i in a)hasOwnProperty.call(a,i)&&(o[i]=a[i]);o.originalType=e,o[u]="string"==typeof e?e:r,l[1]=o;for(var c=2;c<s;c++)l[c]=t[c];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},3534:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>i,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var n=t(7462),r=(t(7294),t(3905));const s={},l="LakeSoul CDC Ingestion via Spark Streaming",o={unversionedId:"Tutorials/consume-cdc-via-spark-streaming",id:"Tutorials/consume-cdc-via-spark-streaming",title:"LakeSoul CDC Ingestion via Spark Streaming",description:"1. LakeSoul CDC Pipeline",source:"@site/docs/02-Tutorials/01-consume-cdc-via-spark-streaming.md",sourceDirName:"02-Tutorials",slug:"/Tutorials/consume-cdc-via-spark-streaming",permalink:"/docs/Tutorials/consume-cdc-via-spark-streaming",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/02-Tutorials/01-consume-cdc-via-spark-streaming.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Use Docker Compose",permalink:"/docs/Getting Started/docker-compose"},next:{title:"LakeSoul Flink CDC Whole Database Synchronization Tutorial",permalink:"/docs/Tutorials/flink-cdc-sink/"}},i={},c=[{value:"1. LakeSoul CDC Pipeline",id:"1-lakesoul-cdc-pipeline",level:2},{value:"2. Setup MySQL",id:"2-setup-mysql",level:2},{value:"2.1 Create database and table",id:"21-create-database-and-table",level:3},{value:"2.2 Use cdc benchmark generator:",id:"22-use-cdc-benchmark-generator",level:3},{value:"3. Setup Kafka (Ignore this step if you already have Kafka running)",id:"3-setup-kafka-ignore-this-step-if-you-already-have-kafka-running",level:2},{value:"3.1 Install Kafka via K8s (https://strimzi.io/docs/operators/latest/deploying.html#deploying-cluster-operator-str):",id:"31-install-kafka-via-k8s-httpsstrimziiodocsoperatorslatestdeployinghtmldeploying-cluster-operator-str",level:3},{value:"4. Setup Debezium (Ignore if you already have it)",id:"4-setup-debezium-ignore-if-you-already-have-it",level:2},{value:"4.1 Install Debezium",id:"41-install-debezium",level:3},{value:"4.2 Setup Debezium sync task",id:"42-setup-debezium-sync-task",level:3},{value:"5. Start Spark Streaming Sink to LakeSoul",id:"5-start-spark-streaming-sink-to-lakesoul",level:2},{value:"5.1 Setup",id:"51-setup",level:3},{value:"5.2 Start Spark Shell",id:"52-start-spark-shell",level:3},{value:"5.3 Create a LakeSoul Table",id:"53-create-a-lakesoul-table",level:3},{value:"5.4 Start spark streaming to sync Debezium CDC data into LakeSoul",id:"54-start-spark-streaming-to-sync-debezium-cdc-data-into-lakesoul",level:3},{value:"5.5 Read from LakeSoul to view synchronized data:",id:"55-read-from-lakesoul-to-view-synchronized-data",level:3}],p={toc:c},u="wrapper";function d(e){let{components:a,...t}=e;return(0,r.kt)(u,(0,n.Z)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"lakesoul-cdc-ingestion-via-spark-streaming"},"LakeSoul CDC Ingestion via Spark Streaming"),(0,r.kt)("h2",{id:"1-lakesoul-cdc-pipeline"},"1. LakeSoul CDC Pipeline"),(0,r.kt)("p",null,"LakeSoul supports ingesting any source of CDC by transforming CDC markups to LakeSoul's own field."),(0,r.kt)("p",null,"There are two ways of CDC ingestion for LakeSoul: 1) Write CDC stream into Kafka and use spark streaming to transform and write into LakeSoul (already supported); 2) Use Flink CDC to directly write into LakeSoul."),(0,r.kt)("p",null,"In this demo, we'll demonstrate the first way. We'll setup a MySQL instance, use scripts to generate DB modifications and use Debezium to sync them into Kafka, and then into LakeSoul via Spark Streaming."),(0,r.kt)("h2",{id:"2-setup-mysql"},"2. Setup MySQL"),(0,r.kt)("h3",{id:"21-create-database-and-table"},"2.1 Create database and table"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"Create database cdc;\nCREATE TABLE test(\n id int primary key,\n rangeid int,\n value varchar(100) \n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n")),(0,r.kt)("h3",{id:"22-use-cdc-benchmark-generator"},"2.2 Use cdc benchmark generator:"),(0,r.kt)("p",null,"We provide a mysql data generator for testing and benchmarking cdc sync. The generator is located under diretory ",(0,r.kt)("inlineCode",{parentName:"p"},"examples/cdc_ingestion_debezium/MysqlBenchmark"),"."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Modify mysqlcdc.conf as needed",(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-ini"}," user=user name of mysql\n passwd=password of mysql\n host=host of mysql\n port=port of mysql\n"))),(0,r.kt)("li",{parentName:"ol"},"Insert data into table",(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# Inside () are comments of parameters, remove them before execution\n bash MysqlCdcBenchmark.sh  insert  cdc(db name) test(table name) 10(lines to insert) 1(thread number)\n"))),(0,r.kt)("li",{parentName:"ol"},"Update data into table",(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"bash MysqlCdcBenchmark.sh  update  cdc test id(primary key) value(column to update) 10(lines to update) \n"))),(0,r.kt)("li",{parentName:"ol"},"Delete data from table",(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," bash  MysqlCdcBenchmark.sh  delete  cdc  test  10(lines to delete)\n")))),(0,r.kt)("h2",{id:"3-setup-kafka-ignore-this-step-if-you-already-have-kafka-running"},"3. Setup Kafka (Ignore this step if you already have Kafka running)"),(0,r.kt)("h3",{id:"31-install-kafka-via-k8s-httpsstrimziiodocsoperatorslatestdeployinghtmldeploying-cluster-operator-str"},"3.1 Install Kafka via K8s (",(0,r.kt)("a",{parentName:"h3",href:"https://strimzi.io/docs/operators/latest/deploying.html#deploying-cluster-operator-str"},"https://strimzi.io/docs/operators/latest/deploying.html#deploying-cluster-operator-str"),"):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create -f install/cluster-operator -n my-cluster-operator-namespace\nkubectl apply -f examples/kafka/kafka-persistent-single.yaml\n")),(0,r.kt)("h2",{id:"4-setup-debezium-ignore-if-you-already-have-it"},"4. Setup Debezium (Ignore if you already have it)"),(0,r.kt)("h3",{id:"41-install-debezium"},"4.1 Install Debezium"),(0,r.kt)("p",null,"To quickly setup a running container of Debezium on K8s:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: dbz-pod-claim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  # replace to actual StorageClass in your cluster\n  storageClassName: \n  resources:\n    requests:\n      storage: 10Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dbz-pod\n  namespace: dmetasoul\nspec:\n  restartPolicy: Never\n  containers:\n  - name: dbs\n    image: debezium/connect:latest\n    env:\n      - name: BOOTSTRAP_SERVERS\n        # replace to actual kafka host\n        value: ${kafka_host}:9092\n      - name: GROUP_ID\n        value: "1"\n      - name: CONFIG_STORAGE_TOPIC\n        value: my_connect_configs\n      - name: OFFSET_STORAGE_TOPIC\n        value: my_connect_offsets\n      - name: STATUS_STORAGE_TOPIC\n        value: my_connect_statuses\n    resources:\n      requests:\n        cpu: 500m\n        memory: 4Gi\n      limits:\n        cpu: 4\n        memory: 8Gi\n    volumeMounts:\n      - mountPath: "/kafka/data"\n        name: dbz-pv-storage\n\n  volumes:\n    - name: dbz-pv-storage\n      persistentVolumeClaim:\n        claimName: dbz-pod-claim\n')),(0,r.kt)("p",null,"Then apply this yaml file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f pod.yaml\n")),(0,r.kt)("h3",{id:"42-setup-debezium-sync-task"},"4.2 Setup Debezium sync task"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'# remember to replace {dbzhost} to actual dbz deployment ip address\n# replace database parameters accordingly\ncurl -X POST http://{dbzhost}:8083/connectors/ -H \'Cache-Control: no-cache\' -H \'Content-Type: application/json\' -d \'{\n    "name": "cdc",\n    "config": {\n        "connector.class": "io.debezium.connector.mysql.MySqlConnector",\n        "key.converter": "org.apache.kafka.connect.json.JsonConverter",\n        "key.converter.schemas.enable": "false",\n        "value.converter": "org.apache.kafka.connect.json.JsonConverter",\n        "value.converter.schemas.enable": "false",\n        "tasks.max": "1",\n        "database.hostname": "mysqlhost",\n        "database.port": "mysqlport",\n        "database.user": "mysqluser",\n        "database.password": "mysqlpassword",\n        "database.server.id": "1",\n        "database.server.name": "cdcserver",\n        "database.include.list": "cdc",\n        "database.history.kafka.bootstrap.servers": "kafkahost:9092",\n        "database.history.kafka.topic": "schema-changes.cdc",\n        "decimal.handling.mode": "double",\n        "table.include.list":"cdc.test" \n    }\n}\'\n')),(0,r.kt)("p",null,"Then check if sync task has been succcessfully created:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'curl -H "Accept:application/json" dbzhost:8083 -X GET http://dbzhost:8083/connectors/\n')),(0,r.kt)("p",null,"You could delete sync task after testing finished:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"curl -i  -X DELETE http://dbzhost:8083/connectors/cdc\n")),(0,r.kt)("h2",{id:"5-start-spark-streaming-sink-to-lakesoul"},"5. Start Spark Streaming Sink to LakeSoul"),(0,r.kt)("h3",{id:"51-setup"},"5.1 Setup"),(0,r.kt)("p",null,"Please refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/Getting%20Started/setup-local-env"},"Quick Start")," on how to setup LakeSoul and Spark environment."),(0,r.kt)("h3",{id:"52-start-spark-shell"},"5.2 Start Spark Shell"),(0,r.kt)("p",null,"Spark shell needs to be started with kafka dependencies:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2 --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog\n")),(0,r.kt)("p",null,"For other required settings such as meta database connection, please refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/Usage%20Docs/setup-meta-env"},"Setup Meta Env")," and ",(0,r.kt)("a",{parentName:"p",href:"/docs/Usage%20Docs/setup-spark"},"Setup Spark")),(0,r.kt)("h3",{id:"53-create-a-lakesoul-table"},"5.3 Create a LakeSoul Table"),(0,r.kt)("p",null,"We'll create a LakeSoul table called MysqlCdcTest, which will sync with the MySQL table we just setup. The LakeSoul table also has a primary key ",(0,r.kt)("inlineCode",{parentName:"p"},"id"),", and we need an extra field ",(0,r.kt)("inlineCode",{parentName:"p"},"op")," to represent CDC ops and add a table property ",(0,r.kt)("inlineCode",{parentName:"p"},"lakesoul_cdc_change_column")," with ",(0,r.kt)("inlineCode",{parentName:"p"},"op")," field."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nval path="/opt/spark/cdctest"\nval data=Seq((1L,1L,"hello world","insert")).toDF("id","rangeid","value","op")\nLakeSoulTable.createTable(data, path).shortTableName("cdc").hashPartitions("id").hashBucketNum(2).rangePartitions("rangeid").tableProperty("lakesoul_cdc_change_column" -> "op").create()\n')),(0,r.kt)("h3",{id:"54-start-spark-streaming-to-sync-debezium-cdc-data-into-lakesoul"},"5.4 Start spark streaming to sync Debezium CDC data into LakeSoul"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nval path="/opt/spark/cdctest"\nval lakeSoulTable = LakeSoulTable.forPath(path)\nvar strList = List.empty[String]\n\n//js1 is just a fake data to help generate the schema\nval js1 = """{\n          |  "before": {\n          |    "id": 2,\n          |    "rangeid": 2,\n          |    "value": "sms"\n          |  },\n          |  "after": {\n          |    "id": 2,\n          |    "rangeid": 2,\n          |    "value": "sms"\n          |  },\n          |  "source": {\n          |    "version": "1.8.0.Final",\n          |    "connector": "mysql",\n          |    "name": "cdcserver",\n          |    "ts_ms": 1644461444000,\n          |    "snapshot": "false",\n          |    "db": "cdc",\n          |    "sequence": null,\n          |    "table": "sms",\n          |    "server_id": 529210004,\n          |    "gtid": "de525a81-57f6-11ec-9b60-fa163e692542:1621099",\n          |    "file": "binlog.000033",\n          |    "pos": 54831329,\n          |    "row": 0,\n          |    "thread": null,\n          |    "query": null\n          |  },\n          |  "op": "c",\n          |  "ts_ms": 1644461444777,\n          |  "transaction": null\n          |}""".stripMargin\nstrList = strList :+ js1\nval rddData = spark.sparkContext.parallelize(strList)\nval resultDF = spark.read.json(rddData)\nval sche = resultDF.schema\n\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\n\n// Specify kafka settings\nval kfdf = spark.readStream\n  .format("kafka")\n  .option("kafka.bootstrap.servers", "kafkahost:9092")\n  .option("subscribe", "cdcserver.cdc.test")\n  .option("startingOffsets", "latest")\n  .load()\n\n// parse CDC json from debezium, and transform `op` field into one of \'insert\', \'update\', \'delete\' into LakeSoul\nval kfdfdata = kfdf\n  .selectExpr("CAST(value AS STRING) as value")\n  .withColumn("payload", from_json($"value", sche))\n  .filter("value is not null")\n  .drop("value")\n  .select("payload.after", "payload.before", "payload.op")\n  .withColumn(\n    "op",\n    when($"op" === "c", "insert")\n      .when($"op" === "u", "update")\n      .when($"op" === "d", "delete")\n      .otherwise("unknown")\n  )\n  .withColumn(\n    "data",\n    when($"op" === "insert" || $"op" === "update", $"after")\n      .when($"op" === "delete", $"before")\n  )\n  .drop($"after")\n  .drop($"before")\n  .select("data.*", "op")\n\n// upsert into LakeSoul with microbatch\nkfdfdata.writeStream\n  .foreachBatch { (batchDF: DataFrame, _: Long) =>\n    {\n      lakeSoulTable.upsert(batchDF)\n      batchDF.show\n    }\n  }\n  .start()\n  .awaitTermination()\n')),(0,r.kt)("h3",{id:"55-read-from-lakesoul-to-view-synchronized-data"},"5.5 Read from LakeSoul to view synchronized data:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nval path="/opt/spark/cdctest"\nval lakeSoulTable = LakeSoulTable.forPath(path)\nlakeSoulTable.toDF.select("*").show()\n')))}d.isMDXComponent=!0}}]);