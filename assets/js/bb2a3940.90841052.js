"use strict";(self.webpackChunklakesoul_website=self.webpackChunklakesoul_website||[]).push([[5912],{5235:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>r,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"Usage Docs/spark-api-docs","title":"Spark API Docs","description":"\x3c!--","source":"@site/docs/03-Usage Docs/03-spark-api-docs.md","sourceDirName":"03-Usage Docs","slug":"/Usage Docs/spark-api-docs","permalink":"/docs/Usage Docs/spark-api-docs","draft":false,"unlisted":false,"editUrl":"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/03-Usage Docs/03-spark-api-docs.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Setup Your Spark and Flink Project/Job","permalink":"/docs/Usage Docs/setup-spark"},"next":{"title":"Use LakeSoul CDC Table Format","permalink":"/docs/Usage Docs/cdc-ingestion-table"}}');var l=t(4848),s=t(8453);const o={},i="Spark API Docs",r={},d=[{value:"1. Create and Write LakeSoulTable",id:"1-create-and-write-lakesoultable",level:2},{value:"1.1 Table Name",id:"11-table-name",level:3},{value:"1.2 Metadata Management",id:"12-metadata-management",level:3},{value:"1.3 Partition",id:"13-partition",level:3},{value:"1.4 Code Examples",id:"14-code-examples",level:3},{value:"2. Read LakeSoulTable",id:"2-read-lakesoultable",level:2},{value:"2.1 Code Examples",id:"21-code-examples",level:3},{value:"3. Upsert LakeSoulTable",id:"3-upsert-lakesoultable",level:2},{value:"3.1 Batch",id:"31-batch",level:3},{value:"3.1.1 Code Examples",id:"311-code-examples",level:4},{value:"3.2 Streaming Support",id:"32-streaming-support",level:3},{value:"4. Update LakeSoulTable",id:"4-update-lakesoultable",level:2},{value:"4.1 Code Examples",id:"41-code-examples",level:3},{value:"5. Delete Data",id:"5-delete-data",level:2},{value:"5.1 Code Examples",id:"51-code-examples",level:3},{value:"6. Compaction",id:"6-compaction",level:2},{value:"6.1 Code Examples",id:"61-code-examples",level:3},{value:"6.2 Compaction And Load Partition to Hive",id:"62-compaction-and-load-partition-to-hive",level:3},{value:"7. Operate LakeSoulTable by Spark SQL",id:"7-operate-lakesoultable-by-spark-sql",level:2},{value:"7.1 Code Examples",id:"71-code-examples",level:3},{value:"7.1.1 DDL SQL",id:"711-ddl-sql",level:4},{value:"7.1.2 DML SQL",id:"712-dml-sql",level:4},{value:"8. Operator on Hash Primary Keys",id:"8-operator-on-hash-primary-keys",level:2},{value:"8.1 Join on Hash Keys",id:"81-join-on-hash-keys",level:3},{value:"8.2 Intersect/Except on Hash Keys",id:"82-intersectexcept-on-hash-keys",level:3},{value:"8.3 Code Examples",id:"83-code-examples",level:3},{value:"9. Schema Evolution",id:"9-schema-evolution",level:2},{value:"9.1 Merge Schema",id:"91-merge-schema",level:3},{value:"9.2 Code Examples",id:"92-code-examples",level:3},{value:"10. Drop Partition",id:"10-drop-partition",level:2},{value:"10.1 Code Examples",id:"101-code-examples",level:3},{value:"11. Drop Table",id:"11-drop-table",level:2},{value:"11.1 Code Examples",id:"111-code-examples",level:3}];function c(e){const a={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(a.header,{children:(0,l.jsx)(a.h1,{id:"spark-api-docs",children:"Spark API Docs"})}),"\n",(0,l.jsx)(a.h2,{id:"1-create-and-write-lakesoultable",children:"1. Create and Write LakeSoulTable"}),"\n",(0,l.jsx)(a.h3,{id:"11-table-name",children:"1.1 Table Name"}),"\n",(0,l.jsx)(a.p,{children:"The table name in LakeSoul can be a path, and the directory where the data is stored is the table name of LakeSoulTable. At the same time a table can have a table name to help remember, or to access in SQL, that is not a string in the form of a path."}),"\n",(0,l.jsxs)(a.p,{children:["When calling the Dataframe.write.save method to write data to LakeSoulTable, if the table does not exist, a new table will be automatically created using the storage path, but there is no table name by default, and can only be accessed through the path. You can add ",(0,l.jsx)(a.code,{children:'option("shortTableName" , "table_name")'})," option to set the table name."]}),"\n",(0,l.jsxs)(a.p,{children:["Through DataFrame.write.saveAsTable, a table will be created, which can be accessed by the table name. The default path is ",(0,l.jsx)(a.code,{children:"spark.sql.warehouse.dir"}),"/current_database/table_name, which can be accessed by the path or table name later. To customize the table path, you can add ",(0,l.jsx)(a.code,{children:'option("path", "s3://bucket/...")'})," option."]}),"\n",(0,l.jsxs)(a.p,{children:["When creating a table through SQL, the table name can be a path or a table name, and the path must be an absolute path. If it is a table name, the rules of the path are consistent with the above Dataframe.write.saveAsTable, which can be set through the LOCATION clause in ",(0,l.jsx)(a.code,{children:"CREATE TABLE"})," SQL. For how to create a primary key partition table in SQL, you can refer to ",(0,l.jsx)(a.a,{href:"#7-operate-lakesoultable-by-spark-sql",children:"7. Use Spark SQL to operate LakeSoul table"})]}),"\n",(0,l.jsx)(a.h3,{id:"12-metadata-management",children:"1.2 Metadata Management"}),"\n",(0,l.jsx)(a.p,{children:"LakeSoul manages metadata through external database, so it can process metadata efficiently, and the meta cluster can be easily scaled up in the cloud."}),"\n",(0,l.jsx)(a.h3,{id:"13-partition",children:"1.3 Partition"}),"\n",(0,l.jsx)(a.p,{children:"LakeSoulTable can be partitioned in two ways, range and hash, and they can be used at the same time."}),"\n",(0,l.jsxs)(a.ul,{children:["\n",(0,l.jsx)(a.li,{children:"Range partition is a common time-based table partition. Data files of different partitions are stored in different partition paths."}),"\n",(0,l.jsx)(a.li,{children:"To use a hash partition, you must specify both the hash primary key fields and the hash bucket num. The hash bucket num is used to hash the hash primary key fields."}),"\n",(0,l.jsx)(a.li,{children:"If you specify both range partition and hash partition, each range partition will have the same hash key written to file with the same bucket id."}),"\n",(0,l.jsx)(a.li,{children:"When partitioning is specified, data written to LakeSoulTable must contain partitioning fields."}),"\n"]}),"\n",(0,l.jsx)(a.p,{children:"Depending on the specific scenario, you can choose to use a range partition, a hash partition, or both. When a hash partition is specified, the data in LakeSoulTable will be unique by the primary key, which is the hash partition field + range partition field (if any)."}),"\n",(0,l.jsxs)(a.p,{children:["When a hash partition is specified, LakeSoulTable supports upsert operations, where writing to data in APPEND mode is disabled, and the ",(0,l.jsx)(a.code,{children:"lakeSoulTable.upsert()"})," method can be used instead."]}),"\n",(0,l.jsx)(a.h3,{id:"14-code-examples",children:"1.4 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\nimport spark.implicits._\n\nval df = Seq(("2021-01-01",1,"rice"),("2021-01-01",2,"bread")).toDF("date","id","name")\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\n\n//create table\n//spark batch\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id")\n  .option("hashBucketNum","2")\n  .save(tablePath)\n//spark streaming\nimport org.apache.spark.sql.streaming.Trigger\nval readStream = spark.readStream.parquet("inputPath")\nval writeStream = readStream.writeStream\n  .outputMode("append")\n  .trigger(Trigger.ProcessingTime("1 minutes"))\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id")\n  .option("hashBucketNum", "2")\n  .option("checkpointLocation", "s3a://bucket-name/checkpoint/path")\n  .start(tablePath)\nwriteStream.awaitTermination()\n\n//for existing table, it no longer need to specify partition information when writing data\n//equivalent to INSERT OVERWRITE PARTITION, if you do not specify option replaceWhere, the entire table will be overwritten\ndf.write\n  .mode("overwrite")\n  .format("lakesoul")\n  .option("replaceWhere","date=\'2021-01-01\'")\n  .save(tablePath)\n\n'})}),"\n",(0,l.jsx)(a.h2,{id:"2-read-lakesoultable",children:"2. Read LakeSoulTable"}),"\n",(0,l.jsxs)(a.p,{children:["You can read data by Spark API or building LakeSoulTable, Spark SQL is also supported, see ",(0,l.jsx)(a.a,{href:"#7-operate-lakeSoultable-by-spark-sql",children:"7. Operate LakeSoulTable by Spark SQL"})]}),"\n",(0,l.jsx)(a.h3,{id:"21-code-examples",children:"2.1 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\n\n//by spark api\nval df1 = spark.read.format("lakesoul").load(tablePath)\n\n//by LakeSoulTableRel\nval df2 = LakeSoulTable.forPath(tablePath).toDF\n\n'})}),"\n",(0,l.jsx)(a.h2,{id:"3-upsert-lakesoultable",children:"3. Upsert LakeSoulTable"}),"\n",(0,l.jsx)(a.h3,{id:"31-batch",children:"3.1 Batch"}),"\n",(0,l.jsx)(a.p,{children:"Upsert is supported when hash partitioning has been specified."}),"\n",(0,l.jsx)(a.p,{children:"MergeOnRead is used by default, upsert data is written as delta files. LakeSoul provides efficient upsert and merge scan performance."}),"\n",(0,l.jsxs)(a.p,{children:["Parameter ",(0,l.jsx)(a.code,{children:"spark.dmetasoul.lakesoul.deltaFile.enabled"})," can be set to ",(0,l.jsx)(a.code,{children:"false"})," to use CopyOnWrite mode, eventually merged data will be generated after upsert, but this mode is not recommended, because it has poor performance and low concurrent."]}),"\n",(0,l.jsx)(a.h4,{id:"311-code-examples",children:"3.1.1 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\nimport spark.implicits._\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\n\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\nval extraDF = Seq(("2021-01-01",3,"chicken")).toDF("date","id","name")\n\nlakeSoulTable.upsert(extraDF)\n'})}),"\n",(0,l.jsx)(a.h3,{id:"32-streaming-support",children:"3.2 Streaming Support"}),"\n",(0,l.jsx)(a.p,{children:"In streaming, when outputMode is complete, each write will overwrite all previous data."}),"\n",(0,l.jsxs)(a.p,{children:["When outputMode is append or update, if hash partition is specified, each write is treated as an upsert, if data with the same primary key exists at read time, the latest value of the same key overrides the previous one. Update mode is available only if hash partition is specified.",(0,l.jsx)(a.br,{}),"\n","Duplicate data is allowed if no hash partitioning is used."]}),"\n",(0,l.jsx)(a.h2,{id:"4-update-lakesoultable",children:"4. Update LakeSoulTable"}),"\n",(0,l.jsxs)(a.p,{children:["LakeSoul supports update operations, which are performed by specifying the condition and the field Expression that needs to be updated. There are several ways to perform update, see annotations in ",(0,l.jsx)(a.code,{children:"LakeSoulTable"}),"."]}),"\n",(0,l.jsx)(a.h3,{id:"41-code-examples",children:"4.1 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\nimport org.apache.spark.sql.functions._\n\n//update(condition, set)\nlakeSoulTable.update(col("date") > "2021-01-01", Map("date" -> lit("2021-01-02")))\n\n'})}),"\n",(0,l.jsx)(a.h2,{id:"5-delete-data",children:"5. Delete Data"}),"\n",(0,l.jsx)(a.p,{children:"LakeSoul supports delete operation to delete data that meet the conditions. Conditions can be any field, and if no condition is specified, all data in table will be deleted."}),"\n",(0,l.jsx)(a.h3,{id:"51-code-examples",children:"5.1 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\n\n//delete data that meet the condition\nlakeSoulTable.delete("date=\'2021-01-01\'")\n//delete full table data\nlakeSoulTable.delete()\n'})}),"\n",(0,l.jsx)(a.h2,{id:"6-compaction",children:"6. Compaction"}),"\n",(0,l.jsx)(a.p,{children:"Upsert will generates delta files, which can affect read efficiency when delta files num become too large, in this time, compaction can be performed to merge files."}),"\n",(0,l.jsx)(a.p,{children:"When compaction is performed to the full table, you can set conditions for compaction, only range partitions that meet the conditions will perform compaction."}),"\n",(0,l.jsx)(a.p,{children:"Conditions to trigger compaction:"}),"\n",(0,l.jsxs)(a.ol,{children:["\n",(0,l.jsxs)(a.li,{children:["The last modification time for a range partition is before ",(0,l.jsx)(a.code,{children:"spark.dmetasoul.lakesoul.compaction.interval"})," (ms), default is 12 hours"]}),"\n",(0,l.jsxs)(a.li,{children:["Delta file num exceeds ",(0,l.jsx)(a.code,{children:"spark.dmetasoul.lakesoul.deltaFile.max.num"}),", default is 5"]}),"\n"]}),"\n",(0,l.jsx)(a.h3,{id:"61-code-examples",children:"6.1 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\n\n//compaction on the specified partition\nlakeSoulTable.compaction("date=\'2021-01-01\'")\n//compAction on all partitions of the table\nlakeSoulTable.compaction()\n//compaction on all partitions, but only partitions meet the conditions will be performed\nlakeSoulTable.compaction(false)\n//spark sql\nspark.sql("call LakeSoulTable.compaction(condition=>map(\'date\',\'2021-01-01\'),tablePath=>\'"+tablePath+"\')")\nspark.sql("call LakeSoulTable.compaction(tableName=>\'lakesoul_table_name\')")\n\n'})}),"\n",(0,l.jsx)(a.h3,{id:"62-compaction-and-load-partition-to-hive",children:"6.2 Compaction And Load Partition to Hive"}),"\n",(0,l.jsx)(a.p,{children:"Since version 2.0, LakeSoul supports load partition into Hive after compaction."}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:"import com.dmetasoul.lakesoul.tables.LakeSoulTable\nval lakeSoulTable = LakeSoulTable.forName(\"lakesoul_test_table\")\nlakeSoulTable.compaction(\"date='2021-01-01'\", \"spark_catalog.default.hive_test_table\")\nspark.sql(\"call LakeSoulTable.compaction(tableName=>'lakesoul_table_name',hiveTableName=>'spark_catalog.default.hive_test_table',condition=>map('date','2021-01-01'))\")\n"})}),"\n",(0,l.jsxs)(a.p,{children:[(0,l.jsx)(a.strong,{children:"Note"})," If ",(0,l.jsx)(a.code,{children:"lakesoul"})," has been set as default catalog, Hive tables should be referenced with ",(0,l.jsx)(a.code,{children:"spark_catalog"})," prefix."]}),"\n",(0,l.jsx)(a.h2,{id:"7-operate-lakesoultable-by-spark-sql",children:"7. Operate LakeSoulTable by Spark SQL"}),"\n",(0,l.jsxs)(a.p,{children:["LakeSoul supports Spark SQL to read and write data. When using it, you need to set ",(0,l.jsx)(a.code,{children:"spark.sql.catalog.lakesoul"})," to ",(0,l.jsx)(a.code,{children:"org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog"}),". At the same time, you can also set LakeSoul as the default Catalog, that is, add the ",(0,l.jsx)(a.code,{children:"spark.sql.defaultCatalog=lakesoul"})," configuration item.\nWhat has to be aware of is:"]}),"\n",(0,l.jsxs)(a.ul,{children:["\n",(0,l.jsxs)(a.li,{children:["The ",(0,l.jsx)(a.code,{children:"insert into"})," function cannot be performed on a hash partitioned table, please use the ",(0,l.jsx)(a.code,{children:"MERGE INTO"})," SQL syntax;"]}),"\n"]}),"\n",(0,l.jsx)(a.h3,{id:"71-code-examples",children:"7.1 Code Examples"}),"\n",(0,l.jsx)(a.h4,{id:"711-ddl-sql",children:"7.1.1 DDL SQL"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-sql",children:"# To create a primary key table, you need to set the primary key name and the number of hash buckets through TBLPROPERTIES, if not set, it is a non-primary key table\n# To create a primary key CDC table, you need to add the table attribute `'lakesoul_cdc_change_column'='change_kind'`, please refer to [LakeSoul CDC table](../03-Usage%20Docs/04-cdc-ingestion-table.mdx)\nCREATE TABLE default.table_name (id string, date string, data string) USING lakesoul\n    PARTITIONED BY (date)\n    LOCATION 's3://bucket/table_path'\n    TBLPROPERTIES(\n      'hashPartitions'='id',\n      'hashBucketNum'='2')\n"})}),"\n",(0,l.jsx)(a.p,{children:"It also supports adding or deleting columns using ALTER TABLE. This part has the same syntax as Spark SQL and does not support changing the column type."}),"\n",(0,l.jsx)(a.h4,{id:"712-dml-sql",children:"7.1.2 DML SQL"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-sql",children:"# INSERT INTO\ninsert overwrite/into table default.table_name partition (date='2021-01-01') select id from tmpView\n\n# MERGE INTO\n# For primary key tables, Upsert can be implemented through the `Merge Into` statement\n# Currently does not support MATCHED/NOT MATCHED conditional statements in Merge Into\n# The ON clause can only contain expressions with equal primary keys. Non-primary key column joins are not supported, and non-equal expressions are not supported\nMERGE INTO default.`table_name` AS t USING source_table AS s\n    ON t.hash = s.hash\n    WHEN MATCHED THEN UPDATE SET *\n    WHEN NOT MATCHED THEN INSERT *\n"})}),"\n",(0,l.jsxs)(a.p,{children:[(0,l.jsx)(a.strong,{children:"Notice"}),":"]}),"\n",(0,l.jsxs)(a.ul,{children:["\n",(0,l.jsxs)(a.li,{children:["The database (namespace) name can be added before the table name, the default is the database name of the current ",(0,l.jsx)(a.code,{children:"USE"}),", and the ",(0,l.jsx)(a.code,{children:"default"})," if the ",(0,l.jsx)(a.code,{children:"USE database"})," has not been executed"]}),"\n",(0,l.jsxs)(a.li,{children:["The table path can be set using the LOCATION clause or the ",(0,l.jsx)(a.code,{children:"path"})," table property, if no path is set, it defaults to ",(0,l.jsx)(a.code,{children:"spark.sql.warehouse.dir"}),"/database_name/table_name/"]}),"\n",(0,l.jsxs)(a.li,{children:["You can use the table path to read and write a LakeSoul table. In SQL, the table name part needs to be written as lakesoul.default.",(0,l.jsx)(a.code,{children:"table_path"})]}),"\n"]}),"\n",(0,l.jsx)(a.h2,{id:"8-operator-on-hash-primary-keys",children:"8. Operator on Hash Primary Keys"}),"\n",(0,l.jsx)(a.p,{children:"When hash partition is specified, the data in each range partition is partitioned according to the hash primary key and the partitioned data is ordered. Therefore, there is no need to do shuffle and sort when some operators perform on hash primary key."}),"\n",(0,l.jsx)(a.p,{children:"LakeSoul currently supports optimization of join, intersect, and except, and more operators will be supported in the future."}),"\n",(0,l.jsx)(a.h3,{id:"81-join-on-hash-keys",children:"8.1 Join on Hash Keys"}),"\n",(0,l.jsx)(a.p,{children:"Scenarios:"}),"\n",(0,l.jsxs)(a.ul,{children:["\n",(0,l.jsx)(a.li,{children:"Shuffle and sort are not required when data from different partitions of the same table is joined on the hash keys"}),"\n",(0,l.jsx)(a.li,{children:"If two different tables have the same hash field type and number of fields, and the same hash bucket num, there is no need to shuffle and sort when they are joined  on the hash keys"}),"\n"]}),"\n",(0,l.jsx)(a.h3,{id:"82-intersectexcept-on-hash-keys",children:"8.2 Intersect/Except on Hash Keys"}),"\n",(0,l.jsx)(a.p,{children:"Scenarios:"}),"\n",(0,l.jsxs)(a.ul,{children:["\n",(0,l.jsx)(a.li,{children:"Intersect/Except on hash keys for different partitions of the same table does not require shuffle, sort, and distinct"}),"\n",(0,l.jsx)(a.li,{children:"Intersect/Except on hash keys for different tables that have the same type and number of hash keys, and the same hash bucket num, there is no need to shuffle, sort, and distinct"}),"\n"]}),"\n",(0,l.jsxs)(a.p,{children:["In a range partition, the hash primary keys are unique, so the results of intersect or except are not repeated, so the subsequent operations do not need to deduplicate again. For example, you can directly ",(0,l.jsx)(a.code,{children:"count"})," the number of data, without the need for ",(0,l.jsx)(a.code,{children:"count distinc"}),"."]}),"\n",(0,l.jsx)(a.h3,{id:"83-code-examples",children:"8.3 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .config("spark.sql.catalog.lakesoul", "org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog")\n  .config("spark.sql.defaultCatalog", "lakesoul")\n  .getOrCreate()\nimport spark.implicits._\n\n\nval df1 = Seq(("2021-01-01",1,1,"rice"),("2021-01-02",2,2,"bread")).toDF("date","id1","id2","name")\nval df2 = Seq(("2021-01-01",1,1,2.7),("2021-01-02",2,2,1.3)).toDF("date","id3","id4","price")\n\nval tablePath1 = "s3a://bucket-name/table/path/is/also/table/name/1"\nval tablePath2 = "s3a://bucket-name/table/path/is/also/table/name/2"\n\ndf1.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id1,id2")\n  .option("hashBucketNum","2")\n  .save(tablePath1)\ndf2.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id3,id4")\n  .option("hashBucketNum","2")\n  .save(tablePath2)\n\n\n//join on hash keys without shuffle and sort\n//different range partitions for the same table\nspark.sql(\n  s"""\n    |select t1.*,t2.* from\n    | (select * from lakesoul.`$tablePath1` where date=\'2021-01-01\') t1\n    | join \n    | (select * from lakesoul.`$tablePath1` where date=\'2021-01-02\') t2\n    | on t1.id1=t2.id1 and t1.id2=t2.id2\n  """.stripMargin)\n    .show()\n//different tables with the same hash setting\nspark.sql(\n  s"""\n    |select t1.*,t2.* from\n    | (select * from lakesoul.`$tablePath1` where date=\'2021-01-01\') t1\n    | join \n    | (select * from lakesoul.`$tablePath2` where date=\'2021-01-01\') t2\n    | on t1.id1=t2.id3 and t1.id2=t2.id4\n  """.stripMargin)\n  .show()\n\n//intersect/except on hash keys without shuffle,sort and distinct\n//different range partitions for the same table\nspark.sql(\n  s"""\n    |select count(1) from \n    | (select id1,id2 from lakesoul.`$tablePath1` where date=\'2021-01-01\'\n    |  intersect\n    | select id1,id2 from lakesoul.`$tablePath1` where date=\'2021-01-02\') t\n  """.stripMargin)\n  .show()\n//different tables with the same hash setting\nspark.sql(\n  s"""\n    |select count(1) from \n    | (select id1,id2 from lakesoul.`$tablePath1` where date=\'2021-01-01\'\n    |  intersect\n    | select id3,id4 from lakesoul.`$tablePath2` where date=\'2021-01-01\') t\n  """.stripMargin)\n  .show()\n\n'})}),"\n",(0,l.jsx)(a.h2,{id:"9-schema-evolution",children:"9. Schema Evolution"}),"\n",(0,l.jsx)(a.p,{children:"LakeSoul supports Schema Evolution, new columns allowed to be added (partitioning fields cannot be modified). When a new column is added and the existing data is read, the new column will be NULL. You can fill the new columns by upsert operation."}),"\n",(0,l.jsx)(a.h3,{id:"91-merge-schema",children:"9.1 Merge Schema"}),"\n",(0,l.jsxs)(a.p,{children:["Specify ",(0,l.jsx)(a.code,{children:"mergeSchema"})," to ",(0,l.jsx)(a.code,{children:"true"})," or enable ",(0,l.jsx)(a.code,{children:"autoMerge"})," to merge the schema when writing data. The new schema is the union of table schema and the current written data schema."]}),"\n",(0,l.jsx)(a.h3,{id:"92-code-examples",children:"9.2 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'df.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id")\n  .option("hashBucketNum","2")\n  //first way\n  .option("mergeSchema","true")\n  .save(tablePath)\n  \nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  //second way\n  .config("spark.dmetasoul.lakesoul.schema.autoMerge.enabled", "true")\n  .getOrCreate()\n'})}),"\n",(0,l.jsx)(a.h2,{id:"10-drop-partition",children:"10. Drop Partition"}),"\n",(0,l.jsx)(a.p,{children:"Drop a partition, also known as drop range partition, does not actually delete the data files. You can use the Cleanup operation to cleanup stale data."}),"\n",(0,l.jsx)(a.h3,{id:"101-code-examples",children:"10.1 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\n\n//drop the specified range partition\nlakeSoulTable.dropPartition("date=\'2021-01-01\'")\n\n'})}),"\n",(0,l.jsx)(a.h2,{id:"11-drop-table",children:"11. Drop Table"}),"\n",(0,l.jsx)(a.p,{children:"Drop table will directly deletes all the metadata and files."}),"\n",(0,l.jsx)(a.h3,{id:"111-code-examples",children:"11.1 Code Examples"}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-scala",children:'import com.dmetasoul.lakesoul.tables.LakeSoulTable\nimport org.apache.spark.sql._\nval spark = SparkSession.builder.master("local")\n  .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n  .getOrCreate()\n\nval tablePath = "s3a://bucket-name/table/path/is/also/table/name"\nval lakeSoulTable = LakeSoulTable.forPath(tablePath)\n\n//drop table\nlakeSoulTable.dropTable()\n\n'})})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,l.jsx)(a,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},8453:(e,a,t)=>{t.d(a,{R:()=>o,x:()=>i});var n=t(6540);const l={},s=n.createContext(l);function o(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),n.createElement(s.Provider,{value:a},e.children)}}}]);