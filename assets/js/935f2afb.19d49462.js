"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"LakeSoul Introduction","href":"/docs/intro","docId":"intro"},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup a Local Environment","href":"/docs/Getting Started/setup-local-env","docId":"Getting Started/setup-local-env"},{"type":"link","label":"Use Docker Compose","href":"/docs/Getting Started/docker-compose","docId":"Getting Started/docker-compose"}]},{"type":"category","label":"Tutorials","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LakeSoul CDC Ingestion via Spark Streaming","href":"/docs/Tutorials/consume-cdc-via-spark-streaming","docId":"Tutorials/consume-cdc-via-spark-streaming"},{"type":"link","label":"LakeSoul Flink CDC Whole Database Synchronization Tutorial","href":"/docs/Tutorials/flink-cdc-sink/","docId":"Tutorials/flink-cdc-sink/index"},{"type":"link","label":"Snapshot Related API Usage Tutorial","href":"/docs/Tutorials/snapshot-manage","docId":"Tutorials/snapshot-manage"},{"type":"link","label":"Mount LakeSoul Data to Hive Meta","href":"/docs/Tutorials/data-mount-to-hive","docId":"Tutorials/data-mount-to-hive"},{"type":"link","label":"Upsert Data and Merge UDF Tutorial","href":"/docs/Tutorials/upsert-and-merge-udf","docId":"Tutorials/upsert-and-merge-udf"},{"type":"link","label":"Multi Stream Merge to Build Wide Table Tutorial","href":"/docs/Tutorials/mutil-stream-merge","docId":"Tutorials/mutil-stream-merge"},{"type":"link","label":"Multiple Kafka Topics Data to LakeSoul Tutorial","href":"/docs/Tutorials/kafka-topics-data-to-lakesoul","docId":"Tutorials/kafka-topics-data-to-lakesoul"},{"type":"link","label":"Incremental Query Function Tutorial","href":"/docs/Tutorials/incremental-query","docId":"Tutorials/incremental-query"}]},{"type":"category","label":"Usage Docs","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup LakeSoul Meta DB","href":"/docs/Usage Docs/setup-meta-env","docId":"Usage Docs/setup-meta-env"},{"type":"link","label":"Setup Your Spark and Flink Project/Job","href":"/docs/Usage Docs/setup-spark","docId":"Usage Docs/setup-spark"},{"type":"link","label":"Spark API Docs","href":"/docs/Usage Docs/api-docs","docId":"Usage Docs/api-docs"},{"type":"link","label":"Use LakeSoul CDC Table Format","href":"/docs/Usage Docs/cdc-ingestion-table","docId":"Usage Docs/cdc-ingestion-table"},{"type":"link","label":"LakeSoul Flink CDC Synchronization of Entire MySQL Database","href":"/docs/Usage Docs/flink-cdc-sync","docId":"Usage Docs/flink-cdc-sync"},{"type":"link","label":"LakeSoul Flink Connector","href":"/docs/Usage Docs/flink-lakesoul-connector","docId":"Usage Docs/flink-lakesoul-connector"},{"type":"link","label":"LakeSoul Global Automatic Compaction Service Usage","href":"/docs/Usage Docs/auto-compaction-task","docId":"Usage Docs/auto-compaction-task"}]}]},"docs":{"Getting Started/docker-compose":{"id":"Getting Started/docker-compose","title":"Use Docker Compose","description":"Docker Compose Files","sidebar":"tutorialSidebar"},"Getting Started/setup-local-env":{"id":"Getting Started/setup-local-env","title":"Setup a Local Environment","description":"Start A Local PostgreSQL DB","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"LakeSoul Introduction","description":"LakeSoul is a cloud-native Lakehouse framework and supports scalable metadata management, ACID transactions, efficient and flexible upsert operation, schema evolution, and unified streaming & batch processing.","sidebar":"tutorialSidebar"},"Tutorials/consume-cdc-via-spark-streaming":{"id":"Tutorials/consume-cdc-via-spark-streaming","title":"LakeSoul CDC Ingestion via Spark Streaming","description":"1. LakeSoul CDC Pipeline","sidebar":"tutorialSidebar"},"Tutorials/data-mount-to-hive":{"id":"Tutorials/data-mount-to-hive","title":"Mount LakeSoul Data to Hive Meta","description":"Since version 2.0, LakeSoul supports two functions: attaching the directory path after Compaction to the specified Hive table, specifying the same partition name as LakeSoul, and customizing the partition name. This function can facilitate downstream systems that can only support Hive to read LakeSoul data. It is more recommended to support Hive JDBC through Kyuubi, so that you can directly use Hive JDBC to call the Spark engine to access the LakeSoul table, including Merge on Read.","sidebar":"tutorialSidebar"},"Tutorials/flink-cdc-sink/index":{"id":"Tutorials/flink-cdc-sink/index","title":"LakeSoul Flink CDC Whole Database Synchronization Tutorial","description":"LakeSoul Flink CDC Sink supports the entire database synchronization from MySQL to LakeSoul, and can support automatic table creation, automatic schema change, exactly once semantics, etc.","sidebar":"tutorialSidebar"},"Tutorials/incremental-query":{"id":"Tutorials/incremental-query","title":"Incremental Query Function Tutorial","description":"LakeSoul provides a timestamp-based incremental query API to facilitate users to obtain data streams added after a given timestamp. Users can query the incremental data within this time range by specifying the start timestamp and the end timestamp. If the end timestamp is not specified, the incremental data from the start time to the current latest time will be queried.","sidebar":"tutorialSidebar"},"Tutorials/kafka-topics-data-to-lakesoul":{"id":"Tutorials/kafka-topics-data-to-lakesoul","title":"Multiple Kafka Topics Data to LakeSoul Tutorial","description":"It is very convenient to synchronize the data in Kafka to LakeSoul by LakeSoul Kafka Stream.","sidebar":"tutorialSidebar"},"Tutorials/mutil-stream-merge":{"id":"Tutorials/mutil-stream-merge","title":"Multi Stream Merge to Build Wide Table Tutorial","description":"To build wide table, traditional data warehouse or ETL uses multi tables join according to the primary and foreign key. When there is a large amount of data or multiple joins are required, there will be problems such as low efficiency, large memory consumption, and even OOM. In addition, the Shuffle process takes up most of the data exchange time, and is inefficient. LakeSoul has supported upsert with merge operator, which can be used to implement multi stream merge in realtime, and avoid the above problems by eliminating join and shuffle. The following is a specific example of this scenario.","sidebar":"tutorialSidebar"},"Tutorials/snapshot-manage":{"id":"Tutorials/snapshot-manage","title":"Snapshot Related API Usage Tutorial","description":"LakeSoul uses snapshots to record each updated file set and generate a new version number in the metadata. If the historical snapshot version has not been cleaned up, it can also be read, rolled back and cleaned up through the LakeSoul API. Since the snapshot version is an internal mechanism, LakeSoul provides a timestamp-based snapshot management API for convenience.","sidebar":"tutorialSidebar"},"Tutorials/upsert-and-merge-udf":{"id":"Tutorials/upsert-and-merge-udf","title":"Upsert Data and Merge UDF Tutorial","description":"LakeSoul can support the function of updating some fields of the data that has entered the lake, without having to overwrite the entire data table, so as to avoid this heavy and resource wasting operation.","sidebar":"tutorialSidebar"},"Usage Docs/api-docs":{"id":"Usage Docs/api-docs","title":"Spark API Docs","description":"1. Create and Write LakeSoulTable","sidebar":"tutorialSidebar"},"Usage Docs/auto-compaction-task":{"id":"Usage Docs/auto-compaction-task","title":"LakeSoul Global Automatic Compaction Service Usage","description":"Since 2.3.0","sidebar":"tutorialSidebar"},"Usage Docs/cdc-ingestion-table":{"id":"Usage Docs/cdc-ingestion-table","title":"Use LakeSoul CDC Table Format","description":"CDC (Change Data Capture) is an important data source for Lakehouse. The goal of LakeSoul CDC table format is to sync the change of online OLTP database into LakeSoul in a very low latency, usually several minutes, manner so that the downstream analytics could get the newest results as soon as possible without the need of tranditional T+1 database dump. Compared to normal table, CDC table format supports delete row capability.","sidebar":"tutorialSidebar"},"Usage Docs/flink-cdc-sync":{"id":"Usage Docs/flink-cdc-sync","title":"LakeSoul Flink CDC Synchronization of Entire MySQL Database","description":"Since version 2.1.0, LakeSoul has implemented Flink CDC Sink, which can support Table API and SQL (single table), and Stream API (full database with multiple tables). The currently supported upstream data source is MySQL (5.6-8.0)","sidebar":"tutorialSidebar"},"Usage Docs/flink-lakesoul-connector":{"id":"Usage Docs/flink-lakesoul-connector","title":"LakeSoul Flink Connector","description":"Since 2.3.0","sidebar":"tutorialSidebar"},"Usage Docs/setup-meta-env":{"id":"Usage Docs/setup-meta-env","title":"Setup LakeSoul Meta DB","description":"LakeSoul use lakesoulhome (case insensitive) environment variable or lakesoulhome JVM property (case sensitive) to locate config file. The config file consists of required LakeSoul configs such as Postgres\'s connection info. An example property file is like the following:","sidebar":"tutorialSidebar"},"Usage Docs/setup-spark":{"id":"Usage Docs/setup-spark","title":"Setup Your Spark and Flink Project/Job","description":"Required Spark Version","sidebar":"tutorialSidebar"}}}')}}]);