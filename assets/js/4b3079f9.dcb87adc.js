"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[96],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>m});var l=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);a&&(l=l.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,l)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,l,n=function(e,a){if(null==e)return{};var t,l,n={},r=Object.keys(e);for(l=0;l<r.length;l++)t=r[l],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(l=0;l<r.length;l++)t=r[l],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var u=l.createContext({}),i=function(e){var a=l.useContext(u),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},p=function(e){var a=i(e.components);return l.createElement(u.Provider,{value:a},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return l.createElement(l.Fragment,{},a)}},k=l.forwardRef((function(e,a){var t=e.components,n=e.mdxType,r=e.originalType,u=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),c=i(t),k=n,m=c["".concat(u,".").concat(k)]||c[k]||d[k]||r;return t?l.createElement(m,s(s({ref:a},p),{},{components:t})):l.createElement(m,s({ref:a},p))}));function m(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var r=t.length,s=new Array(r);s[0]=k;var o={};for(var u in a)hasOwnProperty.call(a,u)&&(o[u]=a[u]);o.originalType=e,o[c]="string"==typeof e?e:n,s[1]=o;for(var i=2;i<r;i++)s[i]=t[i];return l.createElement.apply(null,s)}return l.createElement.apply(null,t)}k.displayName="MDXCreateElement"},5162:(e,a,t)=>{t.d(a,{Z:()=>s});var l=t(7294),n=t(6010);const r={tabItem:"tabItem_Ymn6"};function s(e){let{children:a,hidden:t,className:s}=e;return l.createElement("div",{role:"tabpanel",className:(0,n.Z)(r.tabItem,s),hidden:t},a)}},4866:(e,a,t)=>{t.d(a,{Z:()=>v});var l=t(7462),n=t(7294),r=t(6010),s=t(2466),o=t(6550),u=t(1980),i=t(7392),p=t(12);function c(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:a,label:t,attributes:l,default:n}}=e;return{value:a,label:t,attributes:l,default:n}}))}function d(e){const{values:a,children:t}=e;return(0,n.useMemo)((()=>{const e=a??c(t);return function(e){const a=(0,i.l)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,t])}function k(e){let{value:a,tabValues:t}=e;return t.some((e=>e.value===a))}function m(e){let{queryString:a=!1,groupId:t}=e;const l=(0,o.k6)(),r=function(e){let{queryString:a=!1,groupId:t}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:a,groupId:t});return[(0,u._X)(r),(0,n.useCallback)((e=>{if(!r)return;const a=new URLSearchParams(l.location.search);a.set(r,e),l.replace({...l.location,search:a.toString()})}),[r,l])]}function h(e){const{defaultValue:a,queryString:t=!1,groupId:l}=e,r=d(e),[s,o]=(0,n.useState)((()=>function(e){let{defaultValue:a,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!k({value:a,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const l=t.find((e=>e.default))??t[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:a,tabValues:r}))),[u,i]=m({queryString:t,groupId:l}),[c,h]=function(e){let{groupId:a}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(a),[l,r]=(0,p.Nk)(t);return[l,(0,n.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:l}),b=(()=>{const e=u??c;return k({value:e,tabValues:r})?e:null})();(0,n.useLayoutEffect)((()=>{b&&o(b)}),[b]);return{selectedValue:s,selectValue:(0,n.useCallback)((e=>{if(!k({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);o(e),i(e),h(e)}),[i,h,r]),tabValues:r}}var b=t(2389);const S={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function g(e){let{className:a,block:t,selectedValue:o,selectValue:u,tabValues:i}=e;const p=[],{blockElementScrollPositionUntilNextRender:c}=(0,s.o5)(),d=e=>{const a=e.currentTarget,t=p.indexOf(a),l=i[t].value;l!==o&&(c(a),u(l))},k=e=>{let a=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=p.indexOf(e.currentTarget)+1;a=p[t]??p[0];break}case"ArrowLeft":{const t=p.indexOf(e.currentTarget)-1;a=p[t]??p[p.length-1];break}}a?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":t},a)},i.map((e=>{let{value:a,label:t,attributes:s}=e;return n.createElement("li",(0,l.Z)({role:"tab",tabIndex:o===a?0:-1,"aria-selected":o===a,key:a,ref:e=>p.push(e),onKeyDown:k,onClick:d},s,{className:(0,r.Z)("tabs__item",S.tabItem,s?.className,{"tabs__item--active":o===a})}),t??a)})))}function f(e){let{lazy:a,children:t,selectedValue:l}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(a){const e=r.find((e=>e.props.value===l));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},r.map(((e,a)=>(0,n.cloneElement)(e,{key:a,hidden:e.props.value!==l}))))}function T(e){const a=h(e);return n.createElement("div",{className:(0,r.Z)("tabs-container",S.tabList)},n.createElement(g,(0,l.Z)({},e,a)),n.createElement(f,(0,l.Z)({},e,a)))}function v(e){const a=(0,b.Z)();return n.createElement(T,(0,l.Z)({key:String(a)},e))}},7472:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>u,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var l=t(7462),n=(t(7294),t(3905)),r=t(4866),s=t(5162);const o={},u="Spark Getting Started Guide",i={unversionedId:"Getting Started/spark-guide",id:"Getting Started/spark-guide",title:"Spark Getting Started Guide",description:"Setup",source:"@site/docs/01-Getting Started/02-spark-guide.mdx",sourceDirName:"01-Getting Started",slug:"/Getting Started/spark-guide",permalink:"/docs/Getting Started/spark-guide",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/02-spark-guide.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Quick Setup Environment",permalink:"/docs/Getting Started/setup-local-env"},next:{title:"Flink Getting Started Guide",permalink:"/docs/Getting Started/Flink-Guide"}},p={},c=[{value:"Setup",id:"setup",level:2},{value:"Setup",id:"setup-1",level:2},{value:"Spark 3 Support Matrix",id:"spark-3-support-matrix",level:3},{value:"Spark Shell/SQL",id:"spark-shellsql",level:3},{value:"Setup Maven Project",id:"setup-maven-project",level:3},{value:"Create Namespace",id:"create-namespace",level:2},{value:"Create Table",id:"create-table",level:2},{value:"Primary Key Table",id:"primary-key-table",level:3},{value:"CDC Table",id:"cdc-table",level:3},{value:"Insert/Merge Data",id:"insertmerge-data",level:2},{value:"Update Data",id:"update-data",level:2},{value:"Delete Data",id:"delete-data",level:2},{value:"Query Data",id:"query-data",level:2},{value:"Time Travel Query",id:"time-travel-query",level:2},{value:"Complete Query",id:"complete-query",level:3},{value:"Snapshot Query",id:"snapshot-query",level:3},{value:"Incremental Query",id:"incremental-query",level:3},{value:"Next steps",id:"next-steps",level:2}],d={toc:c},k="wrapper";function m(e){let{components:a,...t}=e;return(0,n.kt)(k,(0,l.Z)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"spark-getting-started-guide"},"Spark Getting Started Guide"),(0,n.kt)("h2",{id:"setup"},"Setup"),(0,n.kt)("h2",{id:"setup-1"},"Setup"),(0,n.kt)("p",null,"To use LakeSoul in Spark, first configure ",(0,n.kt)("a",{parentName:"p",href:"/docs/Getting%20Started/setup-local-env"},"Spark Catalog"),". LakeSoul uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Moreover, LakeSoul provides scala table API to extend the capability of LakeSoul table."),(0,n.kt)("h3",{id:"spark-3-support-matrix"},"Spark 3 Support Matrix"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"LakeSoul"),(0,n.kt)("th",{parentName:"tr",align:null},"Spark Version"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.2.x-2.4.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.3.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.0.x-2.1.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.1.x")))),(0,n.kt)("h3",{id:"spark-shellsql"},"Spark Shell/SQL"),(0,n.kt)("p",null,"Run spark-shell/spark-sql with the ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulSparkSessionExtension")," sql extension."),(0,n.kt)(r.Z,{defaultValue:"Spark SQL",values:[{label:"Spark SQL",value:"Spark SQL"},{label:"Scala",value:"Scala"}],mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-sql --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --jars lakesoul-spark-2.5.0-spark-3.3.jar\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --jars lakesoul-spark-2.5.0-spark-3.3.jar\n")))),(0,n.kt)("h3",{id:"setup-maven-project"},"Setup Maven Project"),(0,n.kt)("p",null,"Include maven dependencies in your project:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>com.dmetasoul</groupId>\n    <artifactId>lakesoul</artifactId>\n    <version>2.5.0-spark-3.3</version>\n</dependency>\n")),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nimport org.apache.spark.sql.SparkSession\nimport spark.implicits._\nimport com.dmetasoul.lakesoul.tables.LakeSoulTable\n\nval spark = SparkSession.builder()\n    .master("local")\n    .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog")\n    .config("spark.sql.defaultCatalog", "lakesoul")\n    .getOrCreate()\n    \n')))),(0,n.kt)("h2",{id:"create-namespace"},"Create Namespace"),(0,n.kt)("p",null,"First, create a namespace for LakeSoul table, default namespace of LakeSoul Catalog is ",(0,n.kt)("inlineCode",{parentName:"p"},"default"),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"# Spark SQL\nCREATE NAMESPACE lakesoul_namespace;\nUSE lakesoul_namespace;\n")),(0,n.kt)("h2",{id:"create-table"},"Create Table"),(0,n.kt)("p",null,"Create a partitioned LakeSoul table with the clause ",(0,n.kt)("inlineCode",{parentName:"p"},"USING lakesoul"),":"),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nCREATE TABLE lakesoul_table (id BIGINT, date STRING, data STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table'\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// scala\nval tablePath= "s3://lakesoul-test-bucket/test_table"\nval df = Seq(("2021-01-01",1,"rice"),("2021-01-01",2,"bread")).toDF("date","id","name")\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .save(tablePath)\n')))),(0,n.kt)("h3",{id:"primary-key-table"},"Primary Key Table"),(0,n.kt)("p",null,"In LakeSoul, a table with primary keys is defined as a hash-partitioned table. To create such a table, use the ",(0,n.kt)("inlineCode",{parentName:"p"},"USING lakesoul")," clause and specify the ",(0,n.kt)("inlineCode",{parentName:"p"},"TBLPROPERTIES")," setting, where ",(0,n.kt)("inlineCode",{parentName:"p"},"'hashPartitions'")," designates a comma-separated list of primary key column names, and ",(0,n.kt)("inlineCode",{parentName:"p"},"'hashBucketNum'")," determines the size or number of hash buckets."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nCREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, date STRING, name STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' \nTBLPROPERTIES ( \n  'hashPartitions'='id',\n  'hashBucketNum'='2')\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// scala\nval tablePath= "s3://lakesoul-test-bucket/test_table"\nval df = Seq(("2021-01-01",1,"rice"),("2021-01-01",2,"bread")).toDF("date","id","name")\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id")\n  .option("hashBucketNum","2")\n  .save(tablePath)\n')),(0,n.kt)("h3",{id:"cdc-table"},"CDC Table"),(0,n.kt)("p",null,"Optionally, a hash-partitioned LakeSoul table has the capability to record Change Data Capture (CDC) data, enabling the tracking of data modifications. To create a LakeSoul table with CDC support, one can utilize the DDL statement for a hash-partitioned LakeSoul table and include an additional ",(0,n.kt)("inlineCode",{parentName:"p"},"TBLPROPERTIES")," setting specifying the ",(0,n.kt)("inlineCode",{parentName:"p"},"'lakesoul_cdc_change_column'")," attribute. This attribute introduces an implicit column that assists the table in efficiently handling CDC information, ensuring precise tracking and management of data changes."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nCREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, date STRING, name STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' \nTBLPROPERTIES( \n  'hashPartitions'='id',\n  'hashBucketNum'='2',\n  'lakesoul_cdc_change_column' = 'op'\n)\n")),(0,n.kt)("h2",{id:"insertmerge-data"},"Insert/Merge Data"),(0,n.kt)("p",null,"To append new data to a non-hash-partitioned table using Spark SQL, use INSERT INTO."),(0,n.kt)("p",null,"To append new data to a table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"DataFrameWriterV2")," API. If this is the first write of the table, it will also auto-create the corresponding LakeSoul table. "),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nINSERT INTO TABLE lakesoul_table VALUES (1, '2024-01-01', 'Alice'), (2, '2024-01-01', 'Bob'), (1, \"2024-01-02\", \"Cathy\")\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval data: DataFrame = Seq((1, "2024-01-01", "Alice"), (2, "2024-01-01", "Bob"), (1, "2024-01-02", "Cathy"))\n              .toDF("id", "date", "name")\ndata.write.format("lakesoul").insertIno("lakesoul_table")\n')))),(0,n.kt)("p",null,"To append new data to a hash-partitioned table using Spark SQL, use Merge INTO."),(0,n.kt)("p",null,"To append new data to a hash-partitioned table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable")," upsert API."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\n// Create source_view\nCREATE OR REPLACE VIEW spark_catalog.default.source_view (id , date, data)\nAS SELECT (1 as `id`, '2024-01-01' as `date`, 'data' as `data`)\n\n// Merge source_view Into lakesoul_hash_table\n\nMERGE INTO lakesoul_hash_table AS t \nUSING spark_catalog.default.source_view AS s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_hash_table"\n\n// Init hash table with first dataframe\nval df = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4))\n        .toDF("range", "hash", "value")\nval writer = df.write.format("lakesoul").mode("overwrite")\n\nwriter\n    .option("rangePartitions", rangePartition.mkString(","))\n    .option("hashPartitions", hashPartition.mkString(","))\n    .option("hashBucketNum", hashBucketNum)\n    .save(tablePath)\n\n// merge the second dataframe into hash table using LakeSoulTable upsert API\nval dfUpsert = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4))\n        .toDF("range", "hash", "value")\nLakeSoulTable.forPath(tablePath).upsert(dfUpsert)\n\n')))),(0,n.kt)("h2",{id:"update-data"},"Update Data"),(0,n.kt)("p",null,"LakeSoul tables can be updated by a DataFrame or using a standard ",(0,n.kt)("inlineCode",{parentName:"p"},"UPDATE")," statement.\nTo update data to a table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable")," updateExpr API."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},'// Spark SQL\nUPDATE table_namespace.table_name SET name = "David" WHERE id = 2\n'))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).updateExpr("id = 2", Seq("name"->"David").toMap)\n')))),(0,n.kt)("h2",{id:"delete-data"},"Delete Data"),(0,n.kt)("p",null,"LakeSoul tables can be removes the records by a DataFrame or using a standard ",(0,n.kt)("inlineCode",{parentName:"p"},"DELETE")," statement.\nTo delete data to a table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable")," ",(0,n.kt)("inlineCode",{parentName:"p"},"delete")," API."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nDELETE FROM lakesoul.lakesoul_namespace.tbl  WHERE key =1\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).delete("id = 1 or id =2")\n')))),(0,n.kt)("h2",{id:"query-data"},"Query Data"),(0,n.kt)("p",null,"LakeSoul tables can be queried using a DataFrame or Spark SQL."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"// Spark SQL\nSELECT * FROM lakesoul_table\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\n\n// query data with DataFrameReader API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nspark.read.format("lakesoul").load(tablePath)\n\n// query data with LakeSoulTable API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).toDF\n\nval tableName = "lakesoul_table"\nLakeSoulTable.forName(tableName).toDF\n')))),(0,n.kt)("h2",{id:"time-travel-query"},"Time Travel Query"),(0,n.kt)("p",null,"LakeSoul supports time travel query to query the table at any point-in-time in history or the changed data between two commit time. "),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_cdc_table"\nSeq(("range1", "hash1", "insert"), ("range2", "hash2", "insert"), ("range3", "hash2", "insert"), ("range4", "hash2", "insert"), ("range4", "hash4", "insert"), ("range3", "hash3", "insert"))\n    .toDF("range", "hash", "op")\n    .write\n    .mode("append")\n    .format("lakesoul")\n    .option("rangePartitions", "range")\n    .option("hashPartitions", "hash")\n    .option("hashBucketNum", "2")\n    .option("shortTableName", "lakesoul_cdc_table")\n    .option("lakesoul_cdc_change_column", "op")\n    .save(tablePath)\n// record the version of 1st commit \nval versionA: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n\nval lakeTable = LakeSoulTable.forPath(tablePath)\nlakeTable.upsert(Seq(("range1", "hash1-1", "delete"), ("range2", "hash2-10", "delete"))\n.toDF("range", "hash", "op"))\n// record the version of 2nd commit \nval versionB: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\nlakeTable.upsert(Seq(("range1", "hash1-13", "insert"), ("range2", "hash2-13", "update"))\n.toDF("range", "hash", "op"))\nlakeTable.upsert(Seq(("range1", "hash1-15", "insert"), ("range2", "hash2-15", "update"))\n.toDF("range", "hash", "op"))\n// record the version of 3rd,4th commits \nval versionC: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n')),(0,n.kt)("h3",{id:"complete-query"},"Complete Query"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.sql("SELECT * FROM lakesoul_cdc_table")\n')),(0,n.kt)("h3",{id:"snapshot-query"},"Snapshot Query"),(0,n.kt)("p",null,"LakeSoul supports snapshot query for query the table at a point-in-time in history."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range2")\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, ReadType.SNAPSHOT_READ)\n    .load(tablePath)\n')),(0,n.kt)("h3",{id:"incremental-query"},"Incremental Query"),(0,n.kt)("p",null,"LakeSoul supports incremental query to obtain a set of records that changed between a start and end time."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range1")\n    .option(LakeSoulOptions.READ_START_TIME, versionA)\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, ReadType.INCREMENTAL_READ)\n    .load(tablePath)\n')),(0,n.kt)("h2",{id:"next-steps"},"Next steps"),(0,n.kt)("p",null,"Next, you can learn more usage cases about LakeSoul tables in Spark at ",(0,n.kt)("a",{parentName:"p",href:"/docs/Usage%20Docs/spark-api-docs"},"Spark API docs"),"."))}m.isMDXComponent=!0}}]);