"use strict";(self.webpackChunklakesoul_website=self.webpackChunklakesoul_website||[]).push([[9205],{72:(e,a,o)=>{o.r(a),o.d(a,{assets:()=>i,contentTitle:()=>l,default:()=>p,frontMatter:()=>t,metadata:()=>r,toc:()=>d});var n=o(5893),s=o(1151);const t={},l="Quick Setup Environment",r={id:"Getting Started/setup-local-env",title:"Quick Setup Environment",description:"\x3c!--",source:"@site/docs/01-Getting Started/01-setup-local-env.md",sourceDirName:"01-Getting Started",slug:"/Getting Started/setup-local-env",permalink:"/docs/Getting Started/setup-local-env",draft:!1,unlisted:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/01-setup-local-env.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LakeSoul General Concepts Introduction",permalink:"/docs/Getting Started/concepts"},next:{title:"Spark Getting Started Guide",permalink:"/docs/Getting Started/spark-guide"}},i={},d=[{value:"1. Set up a test environment in the Linux local file system",id:"1-set-up-a-test-environment-in-the-linux-local-file-system",level:2},{value:"1.1 Start A Local PostgreSQL DB",id:"11-start-a-local-postgresql-db",level:3},{value:"1.2 PG Database Initialization",id:"12-pg-database-initialization",level:3},{value:"1.3 Lakesoul PG Database Configuration Description:",id:"13-lakesoul-pg-database-configuration-description",level:3},{value:"1.4 Install an Apache Spark environment",id:"14-install-an-apache-spark-environment",level:3},{value:"1.4.1 Start spark-shell for testing LakeSoul",id:"141-start-spark-shell-for-testing-lakesoul",level:4},{value:"1.4.2 Write data to object storage service",id:"142-write-data-to-object-storage-service",level:4},{value:"LakeSoul Spark Conf Parameters",id:"lakesoul-spark-conf-parameters",level:4},{value:"1.5 Setup Flink environment",id:"15-setup-flink-environment",level:3},{value:"1.5.1 Start Flink SQL shell",id:"151-start-flink-sql-shell",level:4},{value:"1.5.2 Write data to object storage service",id:"152-write-data-to-object-storage-service",level:4},{value:"2. Start on Hadoop, Spark and Flink cluster environments",id:"2-start-on-hadoop-spark-and-flink-cluster-environments",level:2},{value:"2.1 Add the following information to the Spark configuration file spark-defaults.conf",id:"21-add-the-following-information-to-the-spark-configuration-file-spark-defaultsconf",level:3},{value:"2.2 Add the following information to the Flink configuration file flink-conf.yaml",id:"22-add-the-following-information-to-the-flink-configuration-file-flink-confyaml",level:3},{value:"2.3 Configuration Hadoop Environment",id:"23-configuration-hadoop-environment",level:3},{value:"3. Use Docker Compose to Start a Local Cluster",id:"3-use-docker-compose-to-start-a-local-cluster",level:2},{value:"3.1 Docker Compose Files",id:"31-docker-compose-files",level:3},{value:"3.2 Install Docker Compose",id:"32-install-docker-compose",level:3},{value:"3.3 Start docker compose",id:"33-start-docker-compose",level:3},{value:"3.4 Run LakeSoul Tests in Docker Compose Env",id:"34-run-lakesoul-tests-in-docker-compose-env",level:3},{value:"3.4.1 Prepare LakeSoul Properties File",id:"341-prepare-lakesoul-properties-file",level:4},{value:"3.4.2 Prepare Spark Image",id:"342-prepare-spark-image",level:4},{value:"3.4.3 Start Spark Shell",id:"343-start-spark-shell",level:4},{value:"3.4.4 Execute LakeSoul Scala APIs",id:"344-execute-lakesoul-scala-apis",level:4},{value:"3.4.5 Verify Data Written Successfully",id:"345-verify-data-written-successfully",level:4},{value:"3.5 Cleanup Meta Tables and MinIO Bucket",id:"35-cleanup-meta-tables-and-minio-bucket",level:3},{value:"3.6 Shutdown Docker Compose Env",id:"36-shutdown-docker-compose-env",level:3}];function c(e){const a={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h1,{id:"quick-setup-environment",children:"Quick Setup Environment"}),"\n",(0,n.jsx)(a.h2,{id:"1-set-up-a-test-environment-in-the-linux-local-file-system",children:"1. Set up a test environment in the Linux local file system"}),"\n",(0,n.jsx)(a.p,{children:"To store data on local disk, only a PostgreSQL database is required."}),"\n",(0,n.jsx)(a.h3,{id:"11-start-a-local-postgresql-db",children:"1.1 Start A Local PostgreSQL DB"}),"\n",(0,n.jsx)(a.p,{children:"The quickest way to start a pg DB is via docker container:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"docker run -d --name lakesoul-test-pg -p5432:5432 -e POSTGRES_USER=lakesoul_test -e POSTGRES_PASSWORD=lakesoul_test -e POSTGRES_DB=lakesoul_test -d postgres:14.5\n"})}),"\n",(0,n.jsx)(a.h3,{id:"12-pg-database-initialization",children:"1.2 PG Database Initialization"}),"\n",(0,n.jsxs)(a.p,{children:["Init PG database of LakeSoul using ",(0,n.jsx)(a.code,{children:"script/meta_init.cql"}),".\nExecute code blow in the LakeSoul base directory:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:'docker cp script/meta_init.sql lakesoul-test-pg:/\n\ndocker exec -i lakesoul-test-pg sh -c "PGPASSWORD=lakesoul_test psql -h localhost -p 5432 -U lakesoul_test -f meta_init.sql"\n'})}),"\n",(0,n.jsx)(a.h3,{id:"13-lakesoul-pg-database-configuration-description",children:"1.3 Lakesoul PG Database Configuration Description:"}),"\n",(0,n.jsx)(a.p,{children:"By default, the PG database is connected to the local database. The configuration information is as follows,"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-txt",children:"lakesoul.pg.driver=com.lakesoul.shaded.org.postgresql.Driver\nlakesoul.pg.url=jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\nlakesoul.pg.username=lakesoul_test\nlakesoul.pg.password=lakesoul_test\n"})}),"\n",(0,n.jsxs)(a.p,{children:["LakeSoul supports to customize PG database configuration information. Add an environment variable ",(0,n.jsx)(a.code,{children:"lakesoul_home"})," before starting the Spark program to include the configuration file information."]}),"\n",(0,n.jsxs)(a.p,{children:["For example, the PG database configuration information file path name is: ",(0,n.jsx)(a.code,{children:"/opt/soft/pg.property"}),", you need to add this environment variable before the program starts:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"export lakesoul_home=/opt/soft/pg.property\n"})}),"\n",(0,n.jsx)(a.p,{children:"You can put customized database configuration information in this file."}),"\n",(0,n.jsx)(a.h3,{id:"14-install-an-apache-spark-environment",children:"1.4 Install an Apache Spark environment"}),"\n",(0,n.jsxs)(a.p,{children:["You could download spark distribution from ",(0,n.jsx)(a.a,{href:"https://spark.apache.org/downloads.html",children:"https://spark.apache.org/downloads.html"}),", and please choose spark version 3.3.0 or above. Note that the official package from Apache Spark does not include hadoop-cloud component. We provide a Spark package with Hadoop cloud dependencies, download it from ",(0,n.jsx)(a.a,{href:"https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop3.tgz",children:"https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop3.tgz"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["After unpacking spark package, you could find LakeSoul distribution jar from ",(0,n.jsx)(a.a,{href:"https://github.com/lakesoul-io/LakeSoul/releases",children:"https://github.com/lakesoul-io/LakeSoul/releases"}),". Download the jar file put it into ",(0,n.jsx)(a.code,{children:"jars"})," directory of your spark environment."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"wget https://dmetasoul-bucket.obs.cn-southwest-2.myhuaweicloud.com/releases/spark/spark-3.3.2-bin-hadoop-3.tgz\ntar xf spark-3.3.2-bin-hadoop-3.tgz\nexport SPARK_HOME=${PWD}/spark-3.3.2-bin-hadoop3\nwget https://github.com/lakesoul-io/LakeSoul/releases/download/v2.5.4/lakesoul-spark-2.5.4-spark-3.3.jar -P $SPARK_HOME/jars\n"})}),"\n",(0,n.jsxs)(a.admonition,{type:"tip",children:[(0,n.jsx)(a.p,{children:"For production deployment on Hadoop, it's recommended to use spark release without bundled hadoop:"}),(0,n.jsx)(a.p,{children:(0,n.jsx)(a.a,{href:"https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-without-hadoop.tgz",children:"https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-without-hadoop.tgz"})}),(0,n.jsxs)(a.p,{children:["Refer to ",(0,n.jsx)(a.a,{href:"https://spark.apache.org/docs/latest/hadoop-provided.html",children:"https://spark.apache.org/docs/latest/hadoop-provided.html"})," on how to setup hadoop classpath."]})]}),"\n",(0,n.jsx)(a.admonition,{type:"tip",children:(0,n.jsx)(a.p,{children:"Since 2.1.0, LakeSoul package all its dependencies into one single jar via maven shade plugin. Before that all jars were packaged into one tar.gz file."})}),"\n",(0,n.jsx)(a.h4,{id:"141-start-spark-shell-for-testing-lakesoul",children:"1.4.1 Start spark-shell for testing LakeSoul"}),"\n",(0,n.jsx)(a.p,{children:"cd into the spark installation directory, and start an interactive spark-shell:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"./bin/spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul\n"})}),"\n",(0,n.jsx)(a.h4,{id:"142-write-data-to-object-storage-service",children:"1.4.2 Write data to object storage service"}),"\n",(0,n.jsx)(a.p,{children:"It is necessary to add information such as object storage access key, secret key and endpoint."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"./bin/spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --conf spark.hadoop.fs.s3a.access.key=XXXXXX --conf spark.hadoop.fs.s3a.secret.key=XXXXXX --conf spark.hadoop.fs.s3a.endpoint=XXXXXX --conf spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n"})}),"\n",(0,n.jsxs)(a.p,{children:["If it is a storage service compatible with S3 such as Minio, you also need to add ",(0,n.jsx)(a.code,{children:"--conf spark.hadoop.fs.s3a.path.style.access=true"}),"."]}),"\n",(0,n.jsx)(a.h4,{id:"lakesoul-spark-conf-parameters",children:"LakeSoul Spark Conf Parameters"}),"\n",(0,n.jsxs)(a.p,{children:["Before start to use Lakesoul, we should add some paramethers in ",(0,n.jsx)(a.code,{children:"spark-defaults.conf"})," or ",(0,n.jsx)(a.code,{children:"Spark Session Builder"}),"\u3002"]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Key"}),(0,n.jsx)(a.th,{children:"Value"}),(0,n.jsx)(a.th,{children:"Description"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"spark.sql.extensions"}),(0,n.jsx)(a.td,{children:"com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension"}),(0,n.jsx)(a.td,{children:"extention name for spark sql"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"spark.sql.catalog.lakesoul"}),(0,n.jsx)(a.td,{children:"org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog"}),(0,n.jsx)(a.td,{children:"plug in LakeSoul's catalog"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"spark.sql.defaultCatalog"}),(0,n.jsx)(a.td,{children:"lakesoul"}),(0,n.jsx)(a.td,{children:"set default catalog for spark"})]})]})]}),"\n",(0,n.jsx)(a.h3,{id:"15-setup-flink-environment",children:"1.5 Setup Flink environment"}),"\n",(0,n.jsxs)(a.p,{children:["Download LakeSoul Flink jar: ",(0,n.jsx)(a.a,{href:"https://github.com/lakesoul-io/LakeSoul/releases/download/v2.5.4/lakesoul-flink-1.17-2.5.4.jar",children:"https://github.com/lakesoul-io/LakeSoul/releases/download/v2.5.4/lakesoul-flink-1.17-2.5.4.jar"})]}),"\n",(0,n.jsxs)(a.p,{children:["Download Flink: ",(0,n.jsx)(a.a,{href:"https://dlcdn.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz",children:"https://dlcdn.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz"})]}),"\n",(0,n.jsx)(a.h4,{id:"151-start-flink-sql-shell",children:"1.5.1 Start Flink SQL shell"}),"\n",(0,n.jsxs)(a.p,{children:["After creating the pg database and ",(0,n.jsx)(a.code,{children:"lakesoul_home"})," configuration file, place the LakeSoul Flink jars in the FLink directory.\nEnter the Flink installation directory and execute the following command:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"export lakesoul_home=/opt/soft/pg.property && ./bin/start-cluster.sh\n\nexport lakesoul_home=/opt/soft/pg.property && ./bin/sql-client.sh embedded -j lakesoul-flink-1.17-2.5.4.jar\n"})}),"\n",(0,n.jsx)(a.h4,{id:"152-write-data-to-object-storage-service",children:"1.5.2 Write data to object storage service"}),"\n",(0,n.jsx)(a.p,{children:"Access key, Secret key and Endpoint information need to be added to the Flink configuration file flink-conf.yaml"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yaml",children:"s3.access-key: XXXXXX\ns3.secret-key: XXXXXX\ns3.endpoint: XXXXXX\n"})}),"\n",(0,n.jsx)(a.p,{children:"If it is a storage service compatible with S3 such as Minio, you also need to add:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yaml",children:"s3.path.style.access: true\n"})}),"\n",(0,n.jsxs)(a.p,{children:["And place flink-s3-fs-hadoop.jar and flink-shaded-hadoop-2-uber-2.6.5-10.0.jar under Flink/lib\nDownload flink-s3-fs-hadoop.jar: ",(0,n.jsx)(a.a,{href:"https://repo1.maven.org/maven2/org/apache/flink/flink-s3-fs-hadoop/1.17.2/flink-s3-fs-hadoop-1.17.2.jar",children:"https://repo1.maven.org/maven2/org/apache/flink/flink-s3-fs-hadoop/1.17.2/flink-s3-fs-hadoop-1.17.2.jar"}),"\nDownload flink-shaded-hadoop-2-uber-2.6.5-10.0.jar: ",(0,n.jsx)(a.a,{href:"https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-10.0/flink-shaded-hadoop-2-uber-2.6.5-10.0.jar",children:"https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-10.0/flink-shaded-hadoop-2-uber-2.6.5-10.0.jar"})]}),"\n",(0,n.jsx)(a.h2,{id:"2-start-on-hadoop-spark-and-flink-cluster-environments",children:"2. Start on Hadoop, Spark and Flink cluster environments"}),"\n",(0,n.jsxs)(a.p,{children:["To deploy LakeSoul in a Hadoop cluster, you only need to add the relevant configuration information to the environment variables and Spark and FLink cluster configurations. For the Spark environment, please refer to ",(0,n.jsx)(a.a,{href:"#13-Installation-spark-environment",children:"1.3"})," for installation and deployment. For the Flink environment, please refer to [1.4](#14-flink-Local Environment Construction) for installation and deployment. It is recommended that the environments of Spark and Flink do not include Hadoop dependencies. Use the ",(0,n.jsx)(a.code,{children:"SPARK_DIST_CLASSPATH"})," and ",(0,n.jsx)(a.code,{children:"HADOOP_CLASSPATH"})," environment variables to introduce the Hadoop environment to avoid dependence on the Hadoop version."]}),"\n",(0,n.jsx)(a.p,{children:"The detailed configurations are as follows:"}),"\n",(0,n.jsx)(a.h3,{id:"21-add-the-following-information-to-the-spark-configuration-file-spark-defaultsconf",children:"2.1 Add the following information to the Spark configuration file spark-defaults.conf"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension\nspark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog\nspark.sql.defaultCatalog=lakesoul\n\nspark.yarn.appMasterEnv.LAKESOUL_PG_DRIVER=com.lakesoul.shaded.org.postgresql.Driver\nspark.yarn.appMasterEnv.LAKESOUL_PG_URL=jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\nspark.yarn.appMasterEnv.LAKESOUL_PG_USERNAME=lakesoul_test\nspark.yarn.appMasterEnv.LAKESOUL_PG_PASSWORD=lakesoul_test\n"})}),"\n",(0,n.jsx)(a.h3,{id:"22-add-the-following-information-to-the-flink-configuration-file-flink-confyaml",children:"2.2 Add the following information to the Flink configuration file flink-conf.yaml"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yaml",children:"containerized.master.env.LAKESOUL_PG_DRIVER: com.lakesoul.shaded.org.postgresql.Driver\ncontainerized.master.env.LAKESOUL_PG_USERNAME: postgres\ncontainerized.master.env.LAKESOUL_PG_PASSWORD: postgres123\ncontainerized.master.env.LAKESOUL_PG_URL: jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\ncontainerized.taskmanager.env.LAKESOUL_PG_DRIVER: com.lakesoul.shaded.org.postgresql.Driver\ncontainerized.taskmanager.env.LAKESOUL_PG_USERNAME: lakesoul_test\ncontainerized.taskmanager.env.LAKESOUL_PG_PASSWORD: lakesoul_test\ncontainerized.taskmanager.env.LAKESOUL_PG_URL: jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\n"})}),"\n",(0,n.jsx)(a.p,{children:"In the above configurations, LakeSoul's PG URL connection address, user name, and password need to be modified accordingly according to the specific deployment of PostgreSQL."}),"\n",(0,n.jsx)(a.h3,{id:"23-configuration-hadoop-environment",children:"2.3 Configuration Hadoop Environment"}),"\n",(0,n.jsx)(a.p,{children:"Configure global environment variable information on the client machine. Here you need to write the variable information into an env.sh file.\nHere the Hadoop version is 3.1.4.0-315, the Spark version is spark-3.3.2, and the Flink version is flink-1.17.2. Change Hadoop environment variables according to your Hadoop deployment. If your environment has been pre-configured with Hadoop, you can omit those Hadoop related envs."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nexport HADOOP_HOME="/usr/hdp/3.1.4.0-315/hadoop"\nexport HADOOP_HDFS_HOME="/usr/hdp/3.1.4.0-315/hadoop-hdfs"\nexport HADOOP_MAPRED_HOME="/usr/hdp/3.1.4.0-315/hadoop-mapreduce"\nexport HADOOP_YARN_HOME="/usr/hdp/3.1.4.0-315/hadoop-yarn"\nexport HADOOP_LIBEXEC_DIR="/usr/hdp/3.1.4.0-315/hadoop/libexec"\nexport HADOOP_CONF_DIR="/usr/hdp/3.1.4.0-315/hadoop/conf"\n\nexport SPARK_HOME=/usr/hdp/spark-3.3.2-bin-without-hadoop-ddf\nexport SPARK_CONF_DIR=/home/lakesoul/lakesoul_hadoop_ci/LakeSoul-main/LakeSoul/script/benchmark/hadoop/spark-conf\n\nexport FLINK_HOME=/opt/flink-1.17.2\nexport FLINK_CONF_DIR=/opt/flink-1.17.2/conf\nexport PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$FLINK_HOME/bin:$JAVA_HOME/bin:$PATH\nexport HADOOP_CLASSPATH=$(hadoop classpath)\nexport SPARK_DIST_CLASSPATH=$HADOOP_CLASSPATH\nexport LAKESOUL_PG_DRIVER=com.lakesoul.shaded.org.postgresql.Driver\nexport LAKESOUL_PG_URL=jdbc:postgresql://127.0.0.1:5432/lakesoul_test?stringtype=unspecified\nexport LAKESOUL_PG_USERNAME=lakesoul_test\nexport LAKESOUL_PG_PASSWORD=lakesoul_test\n'})}),"\n",(0,n.jsx)(a.p,{children:"After configuring the above information, execute the following command, and then you can submit the LakeSoul task to the yarn cluster for running on the client"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"source env.sh\n"})}),"\n",(0,n.jsx)(a.h2,{id:"3-use-docker-compose-to-start-a-local-cluster",children:"3. Use Docker Compose to Start a Local Cluster"}),"\n",(0,n.jsx)(a.h3,{id:"31-docker-compose-files",children:"3.1 Docker Compose Files"}),"\n",(0,n.jsxs)(a.p,{children:["We provide a docker compose env to quickly start a local PostgreSQL service and a MinIO S3 Storage service. The docker compose env is located under ",(0,n.jsx)(a.a,{href:"https://github.com/lakesoul-io/LakeSoul/tree/main/docker/lakesoul-docker-compose-env",children:"lakesoul-docker-compose-env"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"32-install-docker-compose",children:"3.2 Install Docker Compose"}),"\n",(0,n.jsxs)(a.p,{children:["To install docker compose, please refer to ",(0,n.jsx)(a.a,{href:"https://docs.docker.com/engine/install/",children:"Install Docker Engine"})]}),"\n",(0,n.jsx)(a.h3,{id:"33-start-docker-compose",children:"3.3 Start docker compose"}),"\n",(0,n.jsx)(a.p,{children:"To start the docker compose env, cd into the docker compose env dir, and execute the command:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"cd docker/lakesoul-docker-compose-env/\ndocker compose up -d\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Then use ",(0,n.jsx)(a.code,{children:"docker compose ps"})," to check both services' statuses are ",(0,n.jsx)(a.code,{children:"running(healthy)"}),". The PostgreSQL service would automatically setup the database and tables required by LakeSoul Meta. And the MinIO service would setup a public bucket. You can change the user, password, database name and MinIO bucket name accordingly in the ",(0,n.jsx)(a.code,{children:"docker-compose.yml"})," file."]}),"\n",(0,n.jsx)(a.h3,{id:"34-run-lakesoul-tests-in-docker-compose-env",children:"3.4 Run LakeSoul Tests in Docker Compose Env"}),"\n",(0,n.jsx)(a.h4,{id:"341-prepare-lakesoul-properties-file",children:"3.4.1 Prepare LakeSoul Properties File"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-ini",metastring:'title="lakesoul.properties"',children:"lakesoul.pg.driver=com.lakesoul.shaded.org.postgresql.Driver\nlakesoul.pg.url=jdbc:postgresql://lakesoul-docker-compose-env-lakesoul-meta-db-1:5432/lakesoul_test?stringtype=unspecified\nlakesoul.pg.username=lakesoul_test\nlakesoul.pg.password=lakesoul_test\n"})}),"\n",(0,n.jsx)(a.h4,{id:"342-prepare-spark-image",children:"3.4.2 Prepare Spark Image"}),"\n",(0,n.jsx)(a.p,{children:"You could use bitnami's Spark 3.3 docker image with packaged hadoop denendencies:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"docker pull bitnami/spark:3.3.1\n"})}),"\n",(0,n.jsx)(a.h4,{id:"343-start-spark-shell",children:"3.4.3 Start Spark Shell"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"docker run --net lakesoul-docker-compose-env_default --rm -ti \\\n    -v $(pwd)/lakesoul.properties:/opt/spark/work-dir/lakesoul.properties \\\n    --env lakesoul_home=/opt/spark/work-dir/lakesoul.properties bitnami/spark:3.3.1 \\\n    spark-shell \\\n    --packages com.dmetasoul:lakesoul-spark:spark-3.3-2.5.4 \\\n    --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension \\\n    --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog \\\n    --conf spark.sql.defaultCatalog=lakesoul \\\n    --conf spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n    --conf spark.hadoop.fs.s3a.buffer.dir=/opt/spark/work-dir/s3a \\\n    --conf spark.hadoop.fs.s3a.path.style.access=true \\\n    --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \\\n    --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\n"})}),"\n",(0,n.jsx)(a.h4,{id:"344-execute-lakesoul-scala-apis",children:"3.4.4 Execute LakeSoul Scala APIs"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'val tablePath= "s3://lakesoul-test-bucket/test_table"\nval df = Seq(("2021-01-01",1,"rice"),("2021-01-01",2,"bread")).toDF("date","id","name")\ndf.write\n  .mode("append")\n  .format("lakesoul")\n  .option("rangePartitions","date")\n  .option("hashPartitions","id")\n  .option("hashBucketNum","2")\n  .save(tablePath)\n'})}),"\n",(0,n.jsx)(a.h4,{id:"345-verify-data-written-successfully",children:"3.4.5 Verify Data Written Successfully"}),"\n",(0,n.jsxs)(a.p,{children:["Open link ",(0,n.jsx)(a.a,{href:"http://127.0.0.1:9001/buckets/lakesoul-test-bucket/browse/",children:"http://127.0.0.1:9001/buckets/lakesoul-test-bucket/browse/"})," in your browser to verify that LakeSoul table has been written to MinIO successfully.\nUse minioadmin1",":minioadmin1"," to login into MinIO's console."]}),"\n",(0,n.jsx)(a.h3,{id:"35-cleanup-meta-tables-and-minio-bucket",children:"3.5 Cleanup Meta Tables and MinIO Bucket"}),"\n",(0,n.jsx)(a.p,{children:"To cleanup all contents in LakeSoul meta tables, execute:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"docker exec -ti lakesoul-docker-compose-env-lakesoul-meta-db-1 psql -h localhost -U lakesoul_test -d lakesoul_test -f /meta_cleanup.sql\n"})}),"\n",(0,n.jsx)(a.p,{children:"To cleanup all contents in MinIO bucket, execute:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"docker run --net lakesoul-docker-compose-env_default --rm -t bitnami/spark:3.3.1 aws --no-sign-request --endpoint-url http://minio:9000 s3 rm --recursive s3://lakesoul-test-bucket/\n"})}),"\n",(0,n.jsx)(a.h3,{id:"36-shutdown-docker-compose-env",children:"3.6 Shutdown Docker Compose Env"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"cd docker/lakesoul-docker-compose-env/\ndocker compose stop\ndocker compose down\n"})})]})}function p(e={}){const{wrapper:a}={...(0,s.a)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},1151:(e,a,o)=>{o.d(a,{Z:()=>r,a:()=>l});var n=o(7294);const s={},t=n.createContext(s);function l(e){const a=n.useContext(t);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),n.createElement(t.Provider,{value:a},e.children)}}}]);