"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[287],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>k});var n=a(7294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=d(a),p=l,k=c["".concat(s,".").concat(p)]||c[p]||m[p]||r;return a?n.createElement(k,o(o({ref:t},u),{},{components:a})):n.createElement(k,o({ref:t},u))}));function k(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=a.length,o=new Array(r);o[0]=p;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[c]="string"==typeof e?e:l,o[1]=i;for(var d=2;d<r;d++)o[d]=a[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},6522:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var n=a(7462),l=(a(7294),a(3905));const r={},o="Flink lakesoul datasource",i={unversionedId:"Usage Docs/flink-lakesoul-datasource",id:"Usage Docs/flink-lakesoul-datasource",title:"Flink lakesoul datasource",description:"LakeSoul supports Flink DataSource, using Flink's DataStream API and Table API to perform LakeSoul batch and streaming read.",source:"@site/docs/03-Usage Docs/06-flink-lakesoul-datasource.md",sourceDirName:"03-Usage Docs",slug:"/Usage Docs/flink-lakesoul-datasource",permalink:"/docs/Usage Docs/flink-lakesoul-datasource",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/03-Usage Docs/06-flink-lakesoul-datasource.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LakeSoul Flink CDC Synchronization of Entire MySQL Database",permalink:"/docs/Usage Docs/flink-cdc-sync"}},s={},d=[{value:"1. Get started",id:"1-get-started",level:2},{value:"1.1 Create catalog",id:"11-create-catalog",level:3},{value:"1.2 Create table",id:"12-create-table",level:3},{value:"1.3 Write data",id:"13-write-data",level:3},{value:"1.4 Alter/Drop table",id:"14-alterdrop-table",level:3},{value:"2. Flink query data",id:"2-flink-query-data",level:2},{value:"2.1 Full read",id:"21-full-read",level:3},{value:"2.2 Snapshot read",id:"22-snapshot-read",level:3},{value:"2.3 Incremental read",id:"23-incremental-read",level:3},{value:"2.4 Streaming read",id:"24-streaming-read",level:3},{value:"2.5 Cdc read",id:"25-cdc-read",level:3}],u={toc:d},c="wrapper";function m(e){let{components:t,...a}=e;return(0,l.kt)(c,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"flink-lakesoul-datasource"},"Flink lakesoul datasource"),(0,l.kt)("p",null,"LakeSoul supports Flink DataSource, using Flink's DataStream API and Table API to perform LakeSoul batch and streaming read."),(0,l.kt)("h2",{id:"1-get-started"},"1. Get started"),(0,l.kt)("p",null,"The method of add Lakesoul dependency in Flink: Package and compile the lakesoul-link folder to obtain lakesoul-flink-2.2.0-flink-1.14-SNAPSHOT.jar, and place this jar in the Flink folder.\nTo create a lakesoul table using Flink, it is recommended to use the Flink SQL Client, which supports direct operation of the lakesoul table using the Flink SQL command\uff0cThe Flink SQL format in this document is to directly input commands on the Flink SQL Client interface; The Table API format needs to be written and used in the code.\nSwitch to the Flink folder and execute the command to start the SQLclient client."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"# Import PostgreSql database configuration and start Flink SQL Client\nexport lakesoul_home=pg.properties && bin/sql-client.sh embedded -j lakesoul-flink-2.2.0-flink-1.14-SNAPSHOT.jar\n")),(0,l.kt)("h3",{id:"11-create-catalog"},"1.1 Create catalog"),(0,l.kt)("p",null,"Create a catalog of lakesoul type and specify the catalog type as lakesoul. Specify the database of lakesoul, which defaults to ",(0,l.kt)("inlineCode",{parentName:"p"},"default"),", to create a lakesoul table through Flink."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Flink SQL format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"create catalog lakesoul with('type'='lakesoul');\nuse catalog lakesoul;\nshow databases;\nuse `default`;\nshow tables;\n")),(0,l.kt)("ol",{start:2},(0,l.kt)("li",{parentName:"ol"},"Table API format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Java"},'TableEnvironment createTableEnv = TableEnvironment.create(EnvironmentSettings.inBatchMode());\nCatalog lakesoulCatalog = new LakeSoulCatalog();\ncreateTableEnv.registerCatalog("lakeSoul", lakesoulCatalog);\ncreateTableEnv.useCatalog("lakeSoul");\ncreateTableEnv.useDatabase("default");\n')),(0,l.kt)("h3",{id:"12-create-table"},"1.2 Create table"),(0,l.kt)("p",null,"LakeSoul supports creating multiple types of tables through Flink, including non primary keys, hash partition, range partition, and both range and hash partitions.\nThe parameter of creating table"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-text"},"1. Partition field `Partitioned BY`, used to specify the range partition field of the table. If there is no range partition field, it is omitted\n2. Data source `connector`, used to specify the data source type\n3. Storage `format`, used to specify the storage format of the table\n4. Storage `path`, used to specify the storage path for the table\n")),(0,l.kt)("p",null,"The lakesoul table supports all common data types of Flink and corresponds one-to-one with Spark SQL data types, enabling the lakesoul table to support both Flink and Spark reads and writes."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Flink SQL format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- Create test_table, with id and name as joint primary keys, region and date as two-level range partitions, catalog as `lakesoul`, and database as `default`\ncreate table `lakesoul`.`default`.test_table (\n            `id` INT,\n            name STRING,\n            score INT,\n            `date` DATE,\n            region STRING,\n        PRIMARY KEY (`id`,`name`) NOT ENFORCED\n        ) PARTITIONED BY (`region`,`date`)\n        WITH (\n            'connector'='lakeSoul',\n            'format'='lakesoul',\n            'path'='file:///tmp/lakesoul/flink/sink/test');          \n")),(0,l.kt)("ol",{start:2},(0,l.kt)("li",{parentName:"ol"},"Table API format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Java"},'// Reference 1.1 create catalog\nTableEnvironment tEnv = ...\nString createUserSql = "create table user_info (" +\n        "    order_id INT," +\n        "    name STRING PRIMARY KEY NOT ENFORCED," +\n        "    score INT" +\n        ") WITH (" +\n        "    \'format\'=\'lakesoul\'," +\n        "    \'path\'=\'/tmp/lakesoul/flink/user\' )";\ntEnv.executeSql(createUserSql);\n')),(0,l.kt)("h3",{id:"13-write-data"},"1.3 Write data"),(0,l.kt)("p",null,"Supports Flink to write data to the lakesoul table in batch and streaming mode. There are two methods for setting the time zone when writing data."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"# Set when starting Flink SQLClient\n/bin/sql-client.sh embedded -z Asia/Shanghai\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- Set in SQLClient command-line interface\nSET 'table.local-time-zone' = 'Asia/Shanghai';\n")),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Flink SQL format")),(0,l.kt)("p",null,"Batch: Insert data directly"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"insert into `lakesoul`.`default`.test_table values (1,'AAA','98',TO_DATE('2023-05-10'),'China');\n")),(0,l.kt)("p",null,"Streaming: Build a streaming task to synchronize data from existing table and write it to a new table"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- Set checkpoint interval\nset 'execution.checkpointing.interval' = '2min';\n\n-- Insert all data from `lakesoul`.`cdcsink`.soure_table into lakesoul`.`default`.test_table\ninsert into `lakesoul`.`default`.test_table select * from `lakesoul`.`cdcsink`.soure_table;\n")),(0,l.kt)("ol",{start:2},(0,l.kt)("li",{parentName:"ol"},"Table API format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-java"},"// Reference 1.1 create catalog\nTableEnvironment tEnv = ...\ntEnvs.executeSql(\"INSERT INTO user_info VALUES (1, 'Bob', 90), (2, 'Alice', 80), (3, 'Jack', 75), (3, 'Amy', 95),(5, 'Tom', 75), (4, 'Mike', 70)\").await();\n")),(0,l.kt)("h3",{id:"14-alterdrop-table"},"1.4 Alter/Drop table"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Flink SQL format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"ALTER TABLE test_table RENAME TO new_table;\nDROP TABLE if exists test_table; \n")),(0,l.kt)("ol",{start:2},(0,l.kt)("li",{parentName:"ol"},"Table API format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-java"},'// Reference 1.1 create catalog\nTableEnvironment tEnv = ...\ntEnvs.executeSql("ALTER TABLE test_table RENAME TO new_table")\ntEnvs.executeSql("DROP TABLE if exists test_table");\n')),(0,l.kt)("h2",{id:"2-flink-query-data"},"2. Flink query data"),(0,l.kt)("p",null,"Support Flink to read lakesoul tables in batch and streaming mode, execute commands on the Flink SQLClient client, and switch between streaming and batch execution modes."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- Execute Flink tasks in streaming mode\nSET execution.runtime-mode = streaming;\n-- Execute Flink tasks in batch mode\nSET execution.runtime-mode = batch;\n")),(0,l.kt)("p",null,"Use Flink SQL\uff0cformat of conditional queries: SELECT ",(0,l.kt)("em",{parentName:"p"}," FROM test_table /"),"+ OPTIONS('key'='value')*/"),(0,l.kt)("p",null,"read-option parameter"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"parameter"),(0,l.kt)("th",{parentName:"tr",align:null},"explanation"),(0,l.kt)("th",{parentName:"tr",align:null},"format"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"readtype"),(0,l.kt)("td",{parentName:"tr",align:null},"Read type. User can specify incremental read, snapshot read, and no default full read"),(0,l.kt)("td",{parentName:"tr",align:null},"'readtype'='incremental'")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"partitiondesc"),(0,l.kt)("td",{parentName:"tr",align:null},"Partition information. If no partition information is specified, it defaults to reading for all partitions"),(0,l.kt)("td",{parentName:"tr",align:null},"'partitiondesc'='region=China'")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"readstarttime"),(0,l.kt)("td",{parentName:"tr",align:null},"Start reading timestamp. If no start timestamp is specified, it defaults to reading from the start version number"),(0,l.kt)("td",{parentName:"tr",align:null},"'readstarttime'='2023-05-01 15:15:15'")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"readendtime"),(0,l.kt)("td",{parentName:"tr",align:null},"End reading timestamp. If no end timestamp is specified, the current latest version number will be read by default"),(0,l.kt)("td",{parentName:"tr",align:null},"'readendtime'='2023-05-01 15:20:15'")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"timezone"),(0,l.kt)("td",{parentName:"tr",align:null},"The time zone of the timestamp. If the time zone of timestamp is not specified, it defaults to processing according to local time zone"),(0,l.kt)("td",{parentName:"tr",align:null},"'timezone'='Asia/Sahanghai'")))),(0,l.kt)("h3",{id:"21-full-read"},"2.1 Full read"),(0,l.kt)("p",null,"Supports batch and streaming read of full data from lakesoul tables, supports common Flink SQL operators such as distinct, where, group by, having, join, order by, limit and so on.\nBatch is the process of reading the latest version of data at the current time. Streaming is the process of reading the latest version of data at the current time, and once the data is updated, it can be automatically recognized and read."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Flink SQL format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- Set batch mode and read test_table\nSET execution.runtime-mode = batch;\nSELECT * FROM `lakesoul`.`default`.test_table order by id\n\n-- Set streaming mode and read test_table\nSET execution.runtime-mode = stream;\nSELECT * FROM `lakesoul`.`default`.test_table where id > 3 \n")),(0,l.kt)("ol",{start:2},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Table API format"),(0,l.kt)("p",{parentName:"li"},"Execute SQL format"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-java"},'// Creating a Batch Execution Environment\nStreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration());\nenv.setRuntimeMode(RuntimeExecutionMode.BATCH);\nStreamTableEnvironment tEnvs = StreamTableEnvironment.create(env);\n// Set the catalog of lakesoul type\nCatalog lakesoulCatalog = new LakeSoulCatalog();\ntEnvs.registerCatalog("lakeSoul", lakesoulCatalog);\ntEnvs.useCatalog("lakeSoul");\ntEnvs.useDatabase("default");\n\ntEnvs.executeSql("SELECT * FROM test_table order by id").print();\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-java"},'// Creating a Streaming Execution Environment\nStreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration());\nenv.setRuntimeMode(RuntimeExecutionMode.STREAMING);\nenv.enableCheckpointing(2000, CheckpointingMode.EXACTLY_ONCE);\nenv.getCheckpointConfig().setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\nStreamTableEnvironment tEnvs = StreamTableEnvironment.create(env);\n// Set the catalog of lakesoul type\nCatalog lakesoulCatalog = new LakeSoulCatalog();\ntEnvs.registerCatalog("lakeSoul", lakesoulCatalog);\ntEnvs.useCatalog("lakeSoul");\ntEnvs.useDatabase("default");\n\ntEnvs.executeSql("SELECT * FROM test_table where id > 3").print();\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Execute API format\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-java"},'// Reference 1.1 create catalog\nTableEnvironment tEnv = ...\nTable userInfo = createTableEnv.from("user_info");\nTable filter = userInfo.filter("order_id=3").select($("name"), $("score"));\nfilter.execute().print();\n\n+--------------------------------+-------------+\n|                           name |       score |\n+--------------------------------+-------------+\n|                           Jack |          75 |\n|                            Amy |          95 |\n+--------------------------------+-------------+\n')),(0,l.kt)("h3",{id:"22-snapshot-read"},"2.2 Snapshot read"),(0,l.kt)("p",null,"Supports Flink to perform snapshot read on lakesoul table. Users can query all data before the end timestamp by specifying partition information and end timestamp."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Flink SQL format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- Perform snapshot read on the test_table partition in region=China, with end timestamp of 2023-05-01 15:20:15 and a time zone of Asia/Sahanghai\nSELECT * FROM `lakesoul`.`default`.test_table /*+ OPTIONS('readtype'='snapshot','partitiondesc'='region=China','readendtime'='2023-05-01 15:20:15','timezone'='Asia/Sahanghai')*/ \n")),(0,l.kt)("ol",{start:2},(0,l.kt)("li",{parentName:"ol"},"Table API format")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-java"},"StreamTableEnvironment tEnvs = StreamTableEnvironment.create(env);\ntEnvs.executeSql(\"SELECT * FROM `lakesoul`.`default`.test_table /*+ OPTIONS('readtype'='snapshot','partitiondesc'='region=China','readendtime'='2023-05-01 15:20:15','timezone'='Asia/Sahanghai')*/\").print();\n")),(0,l.kt)("h3",{id:"23-incremental-read"},"2.3 Incremental read"),(0,l.kt)("p",null,"Supports Flink to perform incremental read on lakesoul table. Users can query incremental data within this time range by specifying partition information, start timestamp, and end timestamp.\nFlink SQL format"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- According to the streaming read test_table, the data in the region=China partition reads with timestamp range 2023-05-01 15:15:15 to 2023-05-01 15:20:15, and the time zone is Asia/Sahanghai\nSELECT * FROM `lakesoul`.`default`.test_table /*+ OPTIONS('readtype'='incremental','partitiondesc'='region=China','readstarttime'='2023-05-01 15:15:15','readendtime'='2023-05-01 15:20:15','timezone'='Asia/Sahanghai')*/\n")),(0,l.kt)("h3",{id:"24-streaming-read"},"2.4 Streaming read"),(0,l.kt)("p",null,"Supports Flink to perform streaming read on lakesoul table. Streaming read is based on incremental read, and users can query incremental data since the start timestamp by specifying the start timestamp and partition information.\nFlink SQL format"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- According to the streaming read test_table, the data in the region=China partition starts with a timestamp of 2023-05-01 15:15:15, and the time zone is Asia/Sahanghai\nSELECT * FROM test_table /*+ OPTIONS('partitiondesc'='region=China','readstarttime'='2023-05-01 15:15:15','timezone'='Asia/Sahanghai')*/\n")),(0,l.kt)("h3",{id:"25-cdc-read"},"2.5 Cdc read"),(0,l.kt)("p",null,"Supports Flink to read the lakesoul table in cdc mode. When creating the lakesoul table, set ",(0,l.kt)("inlineCode",{parentName:"p"},"use_cdc")," to true."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"-- Create a lakesoul cdc table, where op represents the type of operation\ncreate table `lakesoul`.`default`.cdc_sink (\n            `id` INT PRIMARY KEY NOT ENFORCED,\n            name STRING,\n            score INT,\n            op STRING\n        ) WITH (\n            'connector'='lakeSoul',\n            'format'='lakesoul',\n            'use_cdc'='true',\n            'path'='file:///tmp/lakesoul/flink/sink/cdc_test'); \n")),(0,l.kt)("p",null,"Synchronize data from the source table cdc_info and retain the operation information for each record in the cdc_sink table."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"insert into `lakesoul`.`default`.cdc_sink select * from `lakesoul`.`cdcsink`.cdc_info;\n")))}m.isMDXComponent=!0}}]);