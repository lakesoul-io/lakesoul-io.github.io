"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[96],{3905:(e,a,t)=>{t.d(a,{Zo:()=>i,kt:()=>m});var l=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);a&&(l=l.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,l)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,l,n=function(e,a){if(null==e)return{};var t,l,n={},r=Object.keys(e);for(l=0;l<r.length;l++)t=r[l],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(l=0;l<r.length;l++)t=r[l],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var u=l.createContext({}),p=function(e){var a=l.useContext(u),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},i=function(e){var a=p(e.components);return l.createElement(u.Provider,{value:a},e.children)},k="mdxType",c={inlineCode:"code",wrapper:function(e){var a=e.children;return l.createElement(l.Fragment,{},a)}},d=l.forwardRef((function(e,a){var t=e.components,n=e.mdxType,r=e.originalType,u=e.parentName,i=o(e,["components","mdxType","originalType","parentName"]),k=p(t),d=n,m=k["".concat(u,".").concat(d)]||k[d]||c[d]||r;return t?l.createElement(m,s(s({ref:a},i),{},{components:t})):l.createElement(m,s({ref:a},i))}));function m(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var r=t.length,s=new Array(r);s[0]=d;var o={};for(var u in a)hasOwnProperty.call(a,u)&&(o[u]=a[u]);o.originalType=e,o[k]="string"==typeof e?e:n,s[1]=o;for(var p=2;p<r;p++)s[p]=t[p];return l.createElement.apply(null,s)}return l.createElement.apply(null,t)}d.displayName="MDXCreateElement"},5162:(e,a,t)=>{t.d(a,{Z:()=>s});var l=t(7294),n=t(6010);const r={tabItem:"tabItem_Ymn6"};function s(e){let{children:a,hidden:t,className:s}=e;return l.createElement("div",{role:"tabpanel",className:(0,n.Z)(r.tabItem,s),hidden:t},a)}},4866:(e,a,t)=>{t.d(a,{Z:()=>N});var l=t(7462),n=t(7294),r=t(6010),s=t(2466),o=t(6550),u=t(1980),p=t(7392),i=t(12);function k(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:a,label:t,attributes:l,default:n}}=e;return{value:a,label:t,attributes:l,default:n}}))}function c(e){const{values:a,children:t}=e;return(0,n.useMemo)((()=>{const e=a??k(t);return function(e){const a=(0,p.l)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,t])}function d(e){let{value:a,tabValues:t}=e;return t.some((e=>e.value===a))}function m(e){let{queryString:a=!1,groupId:t}=e;const l=(0,o.k6)(),r=function(e){let{queryString:a=!1,groupId:t}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:a,groupId:t});return[(0,u._X)(r),(0,n.useCallback)((e=>{if(!r)return;const a=new URLSearchParams(l.location.search);a.set(r,e),l.replace({...l.location,search:a.toString()})}),[r,l])]}function S(e){const{defaultValue:a,queryString:t=!1,groupId:l}=e,r=c(e),[s,o]=(0,n.useState)((()=>function(e){let{defaultValue:a,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!d({value:a,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const l=t.find((e=>e.default))??t[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:a,tabValues:r}))),[u,p]=m({queryString:t,groupId:l}),[k,S]=function(e){let{groupId:a}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(a),[l,r]=(0,i.Nk)(t);return[l,(0,n.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:l}),h=(()=>{const e=u??k;return d({value:e,tabValues:r})?e:null})();(0,n.useLayoutEffect)((()=>{h&&o(h)}),[h]);return{selectedValue:s,selectValue:(0,n.useCallback)((e=>{if(!d({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);o(e),p(e),S(e)}),[p,S,r]),tabValues:r}}var h=t(2389);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function T(e){let{className:a,block:t,selectedValue:o,selectValue:u,tabValues:p}=e;const i=[],{blockElementScrollPositionUntilNextRender:k}=(0,s.o5)(),c=e=>{const a=e.currentTarget,t=i.indexOf(a),l=p[t].value;l!==o&&(k(a),u(l))},d=e=>{let a=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;a=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;a=i[t]??i[i.length-1];break}}a?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":t},a)},p.map((e=>{let{value:a,label:t,attributes:s}=e;return n.createElement("li",(0,l.Z)({role:"tab",tabIndex:o===a?0:-1,"aria-selected":o===a,key:a,ref:e=>i.push(e),onKeyDown:d,onClick:c},s,{className:(0,r.Z)("tabs__item",b.tabItem,s?.className,{"tabs__item--active":o===a})}),t??a)})))}function g(e){let{lazy:a,children:t,selectedValue:l}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(a){const e=r.find((e=>e.props.value===l));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},r.map(((e,a)=>(0,n.cloneElement)(e,{key:a,hidden:e.props.value!==l}))))}function y(e){const a=S(e);return n.createElement("div",{className:(0,r.Z)("tabs-container",b.tabList)},n.createElement(T,(0,l.Z)({},e,a)),n.createElement(g,(0,l.Z)({},e,a)))}function N(e){const a=(0,h.Z)();return n.createElement(y,(0,l.Z)({key:String(a)},e))}},7472:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>i,contentTitle:()=>u,default:()=>m,frontMatter:()=>o,metadata:()=>p,toc:()=>k});var l=t(7462),n=(t(7294),t(3905)),r=t(4866),s=t(5162);const o={},u="Spark Getting Started Guide",p={unversionedId:"Getting Started/spark-guide",id:"Getting Started/spark-guide",title:"Spark Getting Started Guide",description:"Setup",source:"@site/docs/01-Getting Started/02-spark-guide.mdx",sourceDirName:"01-Getting Started",slug:"/Getting Started/spark-guide",permalink:"/docs/Getting Started/spark-guide",draft:!1,editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/docs/01-Getting Started/02-spark-guide.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Quick Setup Environment",permalink:"/docs/Getting Started/setup-local-env"},next:{title:"Flink Getting Started Guide",permalink:"/docs/Getting Started/Flink-Guide"}},i={},k=[{value:"Setup",id:"setup",level:2},{value:"Setup",id:"setup-1",level:2},{value:"Spark 3 Support Matrix",id:"spark-3-support-matrix",level:3},{value:"Spark Shell/SQL/PySpark",id:"spark-shellsqlpyspark",level:3},{value:"Setup Maven Project",id:"setup-maven-project",level:3},{value:"Create Namespace",id:"create-namespace",level:2},{value:"Create Table",id:"create-table",level:2},{value:"Primary Key Table",id:"primary-key-table",level:3},{value:"CDC Table",id:"cdc-table",level:3},{value:"Insert/Merge Data",id:"insertmerge-data",level:2},{value:"Update Data",id:"update-data",level:2},{value:"Delete Data",id:"delete-data",level:2},{value:"Query Data",id:"query-data",level:2},{value:"Time Travel Query",id:"time-travel-query",level:2},{value:"Complete Query",id:"complete-query",level:3},{value:"Snapshot Query",id:"snapshot-query",level:3},{value:"Incremental Query",id:"incremental-query",level:3},{value:"Next steps",id:"next-steps",level:2}],c={toc:k},d="wrapper";function m(e){let{components:a,...t}=e;return(0,n.kt)(d,(0,l.Z)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"spark-getting-started-guide"},"Spark Getting Started Guide"),(0,n.kt)("h2",{id:"setup"},"Setup"),(0,n.kt)("h2",{id:"setup-1"},"Setup"),(0,n.kt)("p",null,"To use LakeSoul in Spark, first configure ",(0,n.kt)("a",{parentName:"p",href:"/docs/Getting%20Started/setup-local-env"},"Spark Catalog"),". LakeSoul uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Moreover, LakeSoul provides scala table API to extend the capability of LakeSoul table."),(0,n.kt)("h3",{id:"spark-3-support-matrix"},"Spark 3 Support Matrix"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"LakeSoul"),(0,n.kt)("th",{parentName:"tr",align:null},"Spark Version"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.2.x-2.4.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.3.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"2.0.x-2.1.x"),(0,n.kt)("td",{parentName:"tr",align:null},"3.1.x")))),(0,n.kt)("h3",{id:"spark-shellsqlpyspark"},"Spark Shell/SQL/PySpark"),(0,n.kt)("p",null,"Run spark-shell/spark-sql/pyspark with the ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulSparkSessionExtension")," sql extension."),(0,n.kt)(r.Z,{defaultValue:"Spark SQL",values:[{label:"Spark SQL",value:"Spark SQL"},{label:"Scala",value:"Scala"},{label:"PySpark",value:"PySpark"}],mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-sql --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --jars lakesoul-spark-2.5.0-spark-3.3.jar\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"spark-shell --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --jars lakesoul-spark-2.5.0-spark-3.3.jar\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://github.com/lakesoul-io/LakeSoul/tree/main/python/lakesoul/spark/tables.py\npyspark --conf spark.sql.extensions=com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension --conf spark.sql.catalog.lakesoul=org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog --conf spark.sql.defaultCatalog=lakesoul --jars lakesoul-spark-2.5.0-spark-3.3.jar --py-files tables.py\n")))),(0,n.kt)("h3",{id:"setup-maven-project"},"Setup Maven Project"),(0,n.kt)("p",null,"Include maven dependencies in your project:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>com.dmetasoul</groupId>\n    <artifactId>lakesoul</artifactId>\n    <version>2.5.0-spark-3.3</version>\n</dependency>\n")),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.lakesoul.LakeSoulOptions\nimport spark.implicits._\nimport com.dmetasoul.lakesoul.tables.LakeSoulTable\n\n\nval builder = SparkSession.builder()\n    .master("local")\n    .config("spark.sql.extensions", "com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension")\n    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.lakesoul.catalog.LakeSoulCatalog")\n    .config("spark.sql.defaultCatalog", "lakesoul")\nval spark = builder.getOrCreate()\n\n')))),(0,n.kt)("h2",{id:"create-namespace"},"Create Namespace"),(0,n.kt)("p",null,"First, create a namespace for LakeSoul table, default namespace of LakeSoul Catalog is ",(0,n.kt)("inlineCode",{parentName:"p"},"default"),"."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE NAMESPACE IF NOT EXISTS lakesoul_namespace;\nUSE lakesoul_namespace;\nSHOW TABLES;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.sql("CREATE NAMESPACE IF NOT EXISTS lakesoul_namespace")\nspark.sql("USE lakesoul_namespace")\nspark.sql("SHOW TABLES")\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'// python\nspark.sql("CREATE NAMESPACE IF NOT EXISTS lakesoul_namespace")\nspark.sql("USE lakesoul_namespace")\nspark.sql("SHOW TABLES")\n')))),(0,n.kt)("h2",{id:"create-table"},"Create Table"),(0,n.kt)("p",null,"Create a partitioned LakeSoul table using SQL with the clause ",(0,n.kt)("inlineCode",{parentName:"p"},"USING lakesoul"),", or using ",(0,n.kt)("inlineCode",{parentName:"p"},"DataFrameWriterV2")," API at the first ",(0,n.kt)("inlineCode",{parentName:"p"},"save"),"."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE lakesoul_table (id BIGINT, name STRING, `date` STRING) \nUSING lakesoul \nPARTITIONED BY (`date`) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table';\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"// Scala\nspark.sql(\"CREATE TABLE lakesoul_table (id BIGINT, name STRING, `date` STRING) USING lakesoul PARTITIONED BY (`date`) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table'\")\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"// python\nspark.sql(\"CREATE TABLE lakesoul_table (id BIGINT, name STRING, `date` STRING) USING lakesoul PARTITIONED BY (`date`) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_table'\")\n")))),(0,n.kt)("h3",{id:"primary-key-table"},"Primary Key Table"),(0,n.kt)("p",null,"In LakeSoul, a table with primary keys is defined as a hash-partitioned table. To create such a table, use the ",(0,n.kt)("inlineCode",{parentName:"p"},"USING lakesoul")," clause and specify the ",(0,n.kt)("inlineCode",{parentName:"p"},"TBLPROPERTIES")," setting, where ",(0,n.kt)("inlineCode",{parentName:"p"},"'hashPartitions'")," designates a comma-separated list of primary key column names, and ",(0,n.kt)("inlineCode",{parentName:"p"},"'hashBucketNum'")," determines the size or number of hash buckets."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, name STRING, date STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' \nTBLPROPERTIES ( 'hashPartitions'='id', 'hashBucketNum'='2');\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"// Scala\nspark.sql(\"CREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' TBLPROPERTIES ( 'hashPartitions'='id', 'hashBucketNum'='2')\")\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"// python\nspark.sql(\"CREATE TABLE lakesoul_hash_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_hash_table' TBLPROPERTIES ( 'hashPartitions'='id', 'hashBucketNum'='2')\")\n")))),(0,n.kt)("h3",{id:"cdc-table"},"CDC Table"),(0,n.kt)("p",null,"Optionally, a hash-partitioned LakeSoul table has the capability to record Change Data Capture (CDC) data, enabling the tracking of data modifications. To create a LakeSoul table with CDC support, one can utilize the DDL statement for a hash-partitioned LakeSoul table and include an additional ",(0,n.kt)("inlineCode",{parentName:"p"},"TBLPROPERTIES")," setting specifying the ",(0,n.kt)("inlineCode",{parentName:"p"},"'lakesoul_cdc_change_column'")," attribute. This attribute introduces an implicit column that assists the table in efficiently handling CDC information, ensuring precise tracking and management of data changes."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, name STRING, date STRING) \nUSING lakesoul \nPARTITIONED BY (date) \nLOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' \nTBLPROPERTIES('hashPartitions'='id', 'hashBucketNum'='2', 'lakesoul_cdc_change_column' = 'op');\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"// Scala\nspark.sql(\"CREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' TBLPROPERTIES('hashPartitions'='id', 'hashBucketNum'='2', 'lakesoul_cdc_change_column' = 'op')\")\n"))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"// python\nspark.sql(\"CREATE TABLE lakesoul_cdc_table (id BIGINT NOT NULL, name STRING, date STRING) USING lakesoul PARTITIONED BY (date) LOCATION 'file:/tmp/lakesoul_namespace/lakesoul_cdc_table' TBLPROPERTIES('hashPartitions'='id', 'hashBucketNum'='2', 'lakesoul_cdc_change_column' = 'op')\")\n")))),(0,n.kt)("h2",{id:"insertmerge-data"},"Insert/Merge Data"),(0,n.kt)("p",null,"To append new data to a non-hash-partitioned table using Spark SQL, use INSERT INTO."),(0,n.kt)("p",null,"To append new data to a table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"DataFrameWriterV2")," API. If this is the first write of the table, it will also auto-create the corresponding LakeSoul table. "),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO TABLE lakesoul_table VALUES (1, 'Alice', '2024-01-01'), (2, 'Bob', '2024-01-01'), (1, 'Cathy', '2024-01-02');\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval data = Seq(Row(1L, "Alice", "2024-01-01"), Row(2L, "Bob", "2024-01-01"), Row(1L, "Cathy", "2024-01-02"))\nval schema = StructType(Seq(StructField("id", LongType, false), StructField("name", StringType, true), StructField("date", StringType, false)))\nval df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n\ndf.write.format("lakesoul").insertInto("lakesoul_table")\n\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom pyspark.sql.types import *\ndata = [(1,"Cathy","2024-01-02")]\nschema = StructType([StructField("id", LongType(), False), StructField("name", StringType(), True), StructField("date", StringType(), False)])\ndf = spark.createDataFrame(data,schema=schema)\ndf.write.format("lakesoul").insertInto("lakesoul_table")\n')))),(0,n.kt)("p",null,"To append new data to a hash-partitioned table using Spark SQL, use Merge INTO."),(0,n.kt)("p",null,"To append new data to a hash-partitioned table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable")," upsert API."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE OR REPLACE VIEW spark_catalog.default.source_view (id , name, date)\nAS SELECT 1L as `id`, 'George' as `name`, '2024-01-01' as `date`;\n\n\nMERGE INTO lakesoul_hash_table AS t \nUSING spark_catalog.default.source_view AS s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\n\n// Init hash table with first dataframe\nval df = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4)).toDF("range", "hash", "value")\nval writer = df.write.format("lakesoul").mode("overwrite")\n\nwriter\n    .option("rangePartitions", "range")\n    .option("hashPartitions", "hash")\n    .option("hashBucketNum", 2)\n    .save(tablePath)\n\n// merge the second dataframe into hash table using LakeSoulTable upsert API\nval dfUpsert = Seq((20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4)).toDF("range", "hash", "value")\nLakeSoulTable.forPath(tablePath).upsert(dfUpsert)\n\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom pyspark.sql.types import *\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\ndf = spark.createDataFrame([(20201101, 1, 1), (20201101, 2, 2), (20201101, 3, 3), (20201102, 4, 4)],schema=\'range string,hash string,value string\')\ndf.write.format("lakesoul").mode("overwrite").option("rangePartitions", "range").option("hashPartitions", "hash").option("hashBucketNum", 2).save(tablePath)\ndfUpsert = spark.createDataFrame([(20201111, 1, 1), (20201111, 2, 2), (20201111, 3, 3), (20201112, 4, 4)],schema=\'range string,hash string,value string\')\nLakeSoulTable.forPath(spark,tablePath).upsert(dfUpsert)\n')))),(0,n.kt)("h2",{id:"update-data"},"Update Data"),(0,n.kt)("p",null,"LakeSoul tables can be updated by a DataFrame or using a standard ",(0,n.kt)("inlineCode",{parentName:"p"},"UPDATE")," statement.\nTo update data to a table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable")," updateExpr API."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"UPDATE lakesoul_table SET name = 'David' WHERE id = 2;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).updateExpr("id = 2", Seq(("name"->"\'David\'")).toMap)\n\n\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\nLakeSoulTable.forPath(spark,tablePath).update("hash = 4", { "value":"5"})\n')))),(0,n.kt)("h2",{id:"delete-data"},"Delete Data"),(0,n.kt)("p",null,"LakeSoul tables can be removes the records by a DataFrame or using a standard ",(0,n.kt)("inlineCode",{parentName:"p"},"DELETE")," statement.\nTo delete data to a table using DataFrame, use ",(0,n.kt)("inlineCode",{parentName:"p"},"LakeSoulTable")," ",(0,n.kt)("inlineCode",{parentName:"p"},"delete")," API."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"DELETE FROM lakesoul_table WHERE id =1;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).delete("id = 1 or id =2")\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\nLakeSoulTable.forPath(spark,tablePath).delete("hash = 4")\n')))),(0,n.kt)("h2",{id:"query-data"},"Query Data"),(0,n.kt)("p",null,"LakeSoul tables can be queried using a DataFrame or Spark SQL."),(0,n.kt)(r.Z,{mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"Spark SQL",label:"Spark SQL",default:!0,mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM lakesoul_table;\n"))),(0,n.kt)(s.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\n\n// query data with DataFrameReader API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nspark.read.format("lakesoul").load(tablePath)\n\n// query data with LakeSoulTable API\nval tablePath = "file:/tmp/lakesoul_namespace/lakesoul_table"\nLakeSoulTable.forPath(tablePath).toDF\n\nval tableName = "lakesoul_table"\nLakeSoulTable.forName(tableName).toDF\n'))),(0,n.kt)(s.Z,{value:"PySpark",label:"PySpark",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'  // python\nfrom tables import LakeSoulTable\ntablePath = "file:/tmp/lakesoul_namespace/lakesoul_upsert_table"\n\n// query data with LakeSoulTable API\nLakeSoulTable.forPath(spark,tablePath).toDF().show()\n\n// query data with DataFrameReader API\nspark.read.format("lakesoul").load(tablePath).show()\n')))),(0,n.kt)("h2",{id:"time-travel-query"},"Time Travel Query"),(0,n.kt)("p",null,"LakeSoul supports time travel query to query the table at any point-in-time in history or the changed data between two commit time. "),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nval tablePath = "file:/tmp/lakesoul_namespace/cdc_table"\nSeq(("range1", "hash1", "insert"), ("range2", "hash2", "insert"), ("range3", "hash2", "insert"), ("range4", "hash2", "insert"), ("range4", "hash4", "insert"), ("range3", "hash3", "insert"))\n    .toDF("range", "hash", "op")\n    .write\n    .mode("append")\n    .format("lakesoul")\n    .option("rangePartitions", "range")\n    .option("hashPartitions", "hash")\n    .option("hashBucketNum", "2")\n    .option("shortTableName", "cdc_table")\n    .option("lakesoul_cdc_change_column", "op")\n    .save(tablePath)\n// record the version of 1st commit \nimport java.text.SimpleDateFormat\n\nval versionA: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n\nval lakeTable = LakeSoulTable.forPath(tablePath)\nlakeTable.upsert(Seq(("range1", "hash1-1", "delete"), ("range2", "hash2-10", "delete"))\n.toDF("range", "hash", "op"))\n// record the version of 2nd commit \nval versionB: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\nlakeTable.upsert(Seq(("range1", "hash1-13", "insert"), ("range2", "hash2-13", "update"))\n.toDF("range", "hash", "op"))\nlakeTable.upsert(Seq(("range1", "hash1-15", "insert"), ("range2", "hash2-15", "update"))\n.toDF("range", "hash", "op"))\n// record the version of 3rd,4th commits \nval versionC: String = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(System.currentTimeMillis)\n\n\n')),(0,n.kt)("h3",{id:"complete-query"},"Complete Query"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.sql("SELECT * FROM cdc_table")\n')),(0,n.kt)("h3",{id:"snapshot-query"},"Snapshot Query"),(0,n.kt)("p",null,"LakeSoul supports snapshot query for query the table at a point-in-time in history."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range2")\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, LakeSoulOptions.ReadType.SNAPSHOT_READ)\n    .load(tablePath)\n')),(0,n.kt)("h3",{id:"incremental-query"},"Incremental Query"),(0,n.kt)("p",null,"LakeSoul supports incremental query to obtain a set of records that changed between a start and end time."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},'// Scala\nspark.read.format("lakesoul")\n    .option(LakeSoulOptions.PARTITION_DESC, "range=range1")\n    .option(LakeSoulOptions.READ_START_TIME, versionA)\n    .option(LakeSoulOptions.READ_END_TIME, versionB)\n    .option(LakeSoulOptions.READ_TYPE, LakeSoulOptions.ReadType.INCREMENTAL_READ)\n    .load(tablePath)\n')),(0,n.kt)("h2",{id:"next-steps"},"Next steps"),(0,n.kt)("p",null,"Next, you can learn more usage cases about LakeSoul tables in Spark at ",(0,n.kt)("a",{parentName:"p",href:"/docs/Usage%20Docs/spark-api-docs"},"Spark API docs"),"."))}m.isMDXComponent=!0}}]);