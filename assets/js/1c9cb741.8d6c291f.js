"use strict";(self.webpackChunklakesoul_website=self.webpackChunklakesoul_website||[]).push([[7542],{8693:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var o=t(5893),n=t(1151);const i={},s="LakeSoul releases version 2.3.0, with Fully Support of CDC Incremental Computing and Other Important Features",r={permalink:"/blog/2023/07/17/lakesoul-2.3.0-release",editUrl:"https://github.com/lakesoul-io/LakeSoul/tree/main/website/blog/2023-07-17-lakesoul-2.3.0-release/index.md",source:"@site/blog/2023-07-17-lakesoul-2.3.0-release/index.md",title:"LakeSoul releases version 2.3.0, with Fully Support of CDC Incremental Computing and Other Important Features",description:"\x3c!--",date:"2023-07-17T00:00:00.000Z",formattedDate:"July 17, 2023",tags:[],readingTime:3.905,hasTruncateMarker:!1,authors:[],frontMatter:{},unlisted:!1,prevItem:{title:"LakeSoul Opensource Project Introduction",permalink:"/blog/2023/12/01/lakesoul-introduction"},nextItem:{title:"What's new in version 2.2.0",permalink:"/blog/2023/04/21/lakesoul-2.2.0-release"}},l={authorsImageUrls:[]},c=[{value:"Flink SQL/Table API",id:"flink-sqltable-api",level:2},{value:"Flink Multi-Source Ingestion Stream API",id:"flink-multi-source-ingestion-stream-api",level:2},{value:"Global Automatic Small File Compaction Service",id:"global-automatic-small-file-compaction-service",level:2},{value:"Summary",id:"summary",level:2}];function u(e){const a={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",...(0,n.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(a.p,{children:"Recently, LakeSoul, the lakehouse framework released version 2.3.0. This new release is the first release of LakeSoul after it entered the incubation of the Linux Foundation AI & Data as a sandbox project. This new version adds Flink SQL/Table API, which supports stream and batch read and write. The Flink DataStream API for multi-table real-time CDC stream ingestion has been refactored to better support data ingestionfrom multiple data sources to the lakehouse. A new global automatic small file compaction service has been added."}),"\n",(0,o.jsx)(a.h2,{id:"flink-sqltable-api",children:"Flink SQL/Table API"}),"\n",(0,o.jsx)(a.p,{children:"In version 2.3.0, LakeSoul fully supports the Flink SQL/Table API, and supports both streaming and batch methods to read or write LakeSoul tables. When reading or writing streams, LakeSoul fully supports the semantics of Flink Changelog Stream."}),"\n",(0,o.jsx)(a.p,{children:"When writing in stream mode, it can be connected to a variety of stream sources and also CDC collecting tools, including Debezium and Flink CDC Connector. LakeSoul supports row-level upsert and delete. LakeSoul supports stream read for tables into Changelog Stream format to facilitate incremental streaming SQL calculation in Flink. At the same time, LakeSoul also supports Flink batch mode, which can support batch upsert, full read, snapshot read and other functions."}),"\n",(0,o.jsxs)(a.p,{children:["Using LakeSoul + Flink SQL, you can easily build a large-scale, low-cost, high-performance real-time data warehouse on the data lake. For specific usage methods, please refer to ",(0,o.jsx)(a.a,{href:"https://lakesoul-io.github.io/docs/Usage%20Docs/flink-lakesoul-connector",children:"Flink SQL Documentation"}),"."]}),"\n",(0,o.jsx)(a.h2,{id:"flink-multi-source-ingestion-stream-api",children:"Flink Multi-Source Ingestion Stream API"}),"\n",(0,o.jsxs)(a.p,{children:["LakeSoul can support the synchronization of the entire database from version 2.1, and provides [MySQL entire database automatic synchronization tool] (",(0,o.jsx)(a.a,{href:"https://lakesoul-io.github.io/docs/Usage%20Docs/flink-cdc-sync",children:"https://lakesoul-io.github.io/docs/Usage%20Docs/flink-cdc-sync"}),")."]}),"\n",(0,o.jsx)(a.p,{children:"In this version 2.3 update, we refactored the DDL parsing logic when the entire database containing multiple tables is synchronized in one Flink job. Specifically, LakeSoul no longer needs to parse DDL events from the upstream datasources, or go to the source database to obtain information such as the schema of the table when synchronizing the entire database, but directly parses from the DML events to determine whether there is a new table or the schema of an existing table has changed. When a new table or schema change is encountered, the table creation or schema change will be automatically executed in the LakeSoul side."}),"\n",(0,o.jsxs)(a.p,{children:["This change allows LakeSoul to support any type of data source ingestion, such as MySQL, Oracle CDC collection, or consumption of CDC events from Kafka. Developers only need to parse the CDC message into ",(0,o.jsx)(a.a,{href:"https://github.com/lakesoul-io/LakeSoul/blob/main/lakesoul-flink/src/main/java/org/apache/flink/lakesoul/types/BinarySourceRecord.java",children:"BinarySourceRecord"})," object, and create ",(0,o.jsx)(a.code,{children:"DataStream<BinarySourceRecord>"}),", then the whole datasource can be synchronized into LakeSoul. LakeSoul has implemented the conversion from Debezium DML message format to ",(0,o.jsx)(a.code,{children:"BinarySourceRecord"})," object. To accommodate other CDC formats developers can refer to that implementation."]}),"\n",(0,o.jsx)(a.h2,{id:"global-automatic-small-file-compaction-service",children:"Global Automatic Small File Compaction Service"}),"\n",(0,o.jsx)(a.p,{children:"LakeSoul supports streaming and concurrent Upsert or Append operations. Each Upsert/Append operation will write several files, which are automatically merged when read (Merge on Read)."}),"\n",(0,o.jsxs)(a.p,{children:["LakeSoul's MOR performance is already relatively efficient (refer to ",(0,o.jsx)(a.a,{href:"https://lakesoul-io.github.io/blog/2023/04/21/lakesoul-2.2.0-release",children:"Previous Performance Comparison"}),"), It is measured that the MOR performance drops by about 15% after 100 upserts. However, in order to have higher read performance, LakeSoul also provides the function of small file compaction. The compaction functionality is a Spark API that needs to be called independently for each table, which is inconvenient to use."]}),"\n",(0,o.jsxs)(a.p,{children:["In this version 2.3 update, LakeSoul provides ",(0,o.jsx)(a.a,{href:"https://lakesoul-io.github.io/docs/Usage%20Docs/auto-compaction-task",children:"Global Automatic Small File Consolidation Service"}),". This service is actually a Spark job, which automatically triggers the merge operation of eligible tables by listening to the write events of the LakeSoul PG metadata database. This compaction service has several advantages:"]}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsx)(a.li,{children:"Global Compaction Service. The compaction service only needs to be started once in the cluster, and it will automatically compact all the tables (it also supports dividing into multiple databases), and it does not need to be configured in the write job of each table, which is easy to use."}),"\n",(0,o.jsx)(a.li,{children:"Separate Compaction Service. Since LakeSoul can support concurrent writing, the writing of the compaction service does not affect other writing jobs and can be executed concurrently."}),"\n",(0,o.jsxs)(a.li,{children:["Elastic Resource Scaling. The global compaction service is implemented using Spark, and automatic scaling can be achieved by enabling Spark's ",(0,o.jsx)(a.a,{href:"https://spark.apache.org/docs/3.3.1/job-scheduling.html#dynamic-resource-allocation",children:"Dynamic Allocation"}),"."]}),"\n"]}),"\n",(0,o.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(a.p,{children:"The LakeSoul 2.3 version update can better support the construction of large-scale real-time lakehouses, and provides core functionalities such as high-performance IO, incremental streaming computing, and convenient and fast multi-source data ingestion. It is easy to use and reduces the maintenance cost of the data lake."}),"\n",(0,o.jsx)(a.p,{children:"In the next version, LakeSoul will provide more functions such as built-in RBAC and native Python reader. LakeSoul is currently a sandbox incubation project of the Linux Foundation AI & Data, and developers and users are welcome to participate in the community to build a faster and more usable lakehouse framework."})]})}function h(e={}){const{wrapper:a}={...(0,n.a)(),...e.components};return a?(0,o.jsx)(a,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},1151:(e,a,t)=>{t.d(a,{Z:()=>r,a:()=>s});var o=t(7294);const n={},i=o.createContext(n);function s(e){const a=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),o.createElement(i.Provider,{value:a},e.children)}}}]);